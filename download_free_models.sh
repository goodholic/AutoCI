#!/bin/bash

# AutoCI v3.0 - ë¬´ë£Œ ê³ ì„±ëŠ¥ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ì¸ì¦ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ ëª¨ë¸ë“¤

echo "ğŸš€ AutoCI v3.0 - ë¬´ë£Œ ê³ ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"
echo "============================================"
echo ""
echo "âœ… ì¥ì : ì¸ì¦ ë¶ˆí•„ìš”, ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥"
echo "ğŸ“Š ì„±ëŠ¥: ìƒìš© ëª¨ë¸ ìˆ˜ì¤€ì˜ ê³ í’ˆì§ˆ"
echo "ğŸ’¾ í¬ê¸°: 7B-13B (ì ë‹¹í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©)"
echo ""

# ê°€ìƒí™˜ê²½ í™•ì¸
if [ -z "$VIRTUAL_ENV" ] && [ -d "autoci_env" ]; then
    echo "ğŸ”„ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
    source autoci_env/bin/activate
fi

# Python í™˜ê²½ í™•ì¸
if ! command -v python &> /dev/null; then
    echo "âŒ Pythonì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    exit 1
fi

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘..."
python -m pip install huggingface-hub transformers torch --quiet

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p models
cd models

echo ""
echo "ì‚¬ìš© ê°€ëŠ¥í•œ ë¬´ë£Œ ê³ ì„±ëŠ¥ ëª¨ë¸:"
echo "1. Code Llama 7B (ê¸°ë³¸) - ì½”ë”© ì „ë¬¸, ë¹ ë¥¸ ì†ë„"
echo "2. Code Llama 13B - ë” ë†’ì€ ì„±ëŠ¥, ì¤‘ê°„ ì†ë„"
echo "3. Mistral 7B Instruct - ë²”ìš© ê³ ì„±ëŠ¥, í•œêµ­ì–´ ì§€ì›"
echo "4. OpenCodeInterpreter 6.7B - ì½”ë”© ì „ë¬¸, ìµœì‹  ëª¨ë¸"
echo "5. ëª¨ë“  ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)"
echo ""
read -p "ì„ íƒí•˜ì„¸ìš” (1-5): " choice

case $choice in
    1)
        echo "ğŸ“¦ Code Llama 7B ë‹¤ìš´ë¡œë“œ..."
        download_codelllama7b=true
        ;;
    2)
        echo "ğŸ“¦ Code Llama 13B ë‹¤ìš´ë¡œë“œ..."
        download_codelllama13b=true
        ;;
    3)
        echo "ğŸ“¦ Mistral 7B ë‹¤ìš´ë¡œë“œ..."
        download_mistral=true
        ;;
    4)
        echo "ğŸ“¦ OpenCodeInterpreter ë‹¤ìš´ë¡œë“œ..."
        download_opencodeinterpreter=true
        ;;
    5)
        echo "ğŸ“¦ ëª¨ë“  ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ..."
        download_codelllama7b=true
        download_codelllama13b=true
        download_mistral=true
        download_opencodeinterpreter=true
        ;;
    *)
        echo "âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤."
        exit 1
        ;;
esac

echo ""
echo "ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
echo ""

# Python ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > download_free_models.py << 'EOF'
#!/usr/bin/env python3
"""
AutoCI ë¬´ë£Œ ê³ ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def download_model(repo_id, local_dir, description):
    """ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤"""
    print(f"ğŸ“¥ {description} ë‹¤ìš´ë¡œë“œ ì¤‘...")
    print(f"   ì €ì¥ì†Œ: {repo_id}")
    print(f"   ì €ì¥ ìœ„ì¹˜: {local_dir}")
    
    try:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=repo_id,
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            cache_dir="./cache"
        )
        print(f"âœ… {description} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return True
    except Exception as e:
        print(f"âŒ {description} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """ë©”ì¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    import sys
    
    models_to_download = []
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì–´ë–¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí• ì§€ ê²°ì •
    if len(sys.argv) > 1:
        if "codelllama7b" in sys.argv:
            models_to_download.append({
                "repo_id": "codellama/CodeLlama-7b-Instruct-hf",
                "local_dir": "../CodeLlama-7b-Instruct-hf",
                "description": "Code Llama 7B Instruct"
            })
        
        if "codelllama13b" in sys.argv:
            models_to_download.append({
                "repo_id": "codellama/CodeLlama-13b-Instruct-hf",
                "local_dir": "./CodeLlama-13b-Instruct-hf",
                "description": "Code Llama 13B Instruct"
            })
        
        if "mistral" in sys.argv:
            models_to_download.append({
                "repo_id": "mistralai/Mistral-7B-Instruct-v0.3",
                "local_dir": "./Mistral-7B-Instruct-v0.3",
                "description": "Mistral 7B Instruct v0.3"
            })
            
        if "opencodeinterpreter" in sys.argv:
            models_to_download.append({
                "repo_id": "m-a-p/OpenCodeInterpreter-DS-6.7B",
                "local_dir": "./OpenCodeInterpreter-DS-6.7B",
                "description": "OpenCodeInterpreter DS 6.7B"
            })
    
    if not models_to_download:
        print("âŒ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ì´ {len(models_to_download)}ê°œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    print("")
    
    success_count = 0
    for i, model in enumerate(models_to_download, 1):
        print(f"[{i}/{len(models_to_download)}] {model['description']}")
        if download_model(model["repo_id"], model["local_dir"], model["description"]):
            success_count += 1
        print("")
    
    print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(models_to_download)} ì„±ê³µ")
    
    # ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±
    create_model_info_file(models_to_download, success_count)

def create_model_info_file(downloaded_models, success_count):
    """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±"""
    info_content = f"""# AutoCI ë¬´ë£Œ ê³ ì„±ëŠ¥ ëª¨ë¸ ì •ë³´

## ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(downloaded_models)}

### ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:
"""
    
    for model in downloaded_models:
        if os.path.exists(model["local_dir"]):
            info_content += f"- âœ… {model['description']} - {model['local_dir']}\n"
        else:
            info_content += f"- âŒ {model['description']} - ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n"
    
    info_content += f"""

### ëª¨ë¸ íŠ¹ì§•:
- **Code Llama 7B**: ë¹ ë¥¸ ì†ë„, ì½”ë”© ì „ë¬¸, 16GB RAM ê¶Œì¥
- **Code Llama 13B**: ë†’ì€ ì„±ëŠ¥, ì½”ë”© ì „ë¬¸, 24GB RAM ê¶Œì¥  
- **Mistral 7B**: ë²”ìš© ê³ ì„±ëŠ¥, í•œêµ­ì–´ ì§€ì›, 16GB RAM ê¶Œì¥
- **OpenCodeInterpreter**: ìµœì‹  ì½”ë”© ëª¨ë¸, ì°½ì˜ì  ì½”ë”©, 16GB RAM ê¶Œì¥

### ì‚¬ìš©ë²•:
```python
# ëª¨ë¸ ë¡œë“œ ì˜ˆì‹œ
from transformers import AutoModelForCausalLM, AutoTokenizer

# Code Llama 7B (ì˜ˆì‹œ)
tokenizer = AutoTokenizer.from_pretrained("./CodeLlama-7b-Instruct-hf")
model = AutoModelForCausalLM.from_pretrained(
    "./CodeLlama-7b-Instruct-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:
- **7B ëª¨ë¸**: 8-16GB RAM
- **13B ëª¨ë¸**: 16-24GB RAM
- GPU ë©”ëª¨ë¦¬: 6GB+ ê¶Œì¥ (ì„ íƒì‚¬í•­)

### ì„±ëŠ¥ ìˆœìœ„ (ì¶”ì •):
1. Code Llama 13B - ìµœê³  ì½”ë”© ì„±ëŠ¥
2. Mistral 7B - ìµœê³  ë²”ìš© ì„±ëŠ¥
3. OpenCodeInterpreter - ìµœì‹  ì½”ë”© ëª¨ë¸
4. Code Llama 7B - ë¹ ë¥¸ ì†ë„

### AutoCI ì‹œì‘:
```bash
source autoci_env/bin/activate
python start_autoci_agent.py
```
"""
    
    with open("../free_models_info.md", "w", encoding="utf-8") as f:
        f.write(info_content)
    
    print("ğŸ“„ ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±: free_models_info.md")

if __name__ == "__main__":
    main()
EOF

# ì„ íƒì— ë”°ë¼ Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
args=""
if [ "$download_codelllama7b" = true ]; then
    args="$args codelllama7b"
fi
if [ "$download_codelllama13b" = true ]; then
    args="$args codelllama13b"
fi
if [ "$download_mistral" = true ]; then
    args="$args mistral"
fi
if [ "$download_opencodeinterpreter" = true ]; then
    args="$args opencodeinterpreter"
fi

# Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python download_free_models.py $args

# ì •ë¦¬
rm -f download_free_models.py

echo ""
echo "ğŸ‰ ë¬´ë£Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸ“‹ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™•ì¸:"
if [ -d "../CodeLlama-7b-Instruct-hf" ]; then
    echo "  âœ… Code Llama 7B (ê¸°ë³¸)"
fi
if [ -d "CodeLlama-13b-Instruct-hf" ]; then
    echo "  âœ… Code Llama 13B (ê³ ì„±ëŠ¥)"
fi
if [ -d "Mistral-7B-Instruct-v0.3" ]; then
    echo "  âœ… Mistral 7B (ë²”ìš© ê³ ì„±ëŠ¥)"
fi
if [ -d "OpenCodeInterpreter-DS-6.7B" ]; then
    echo "  âœ… OpenCodeInterpreter (ìµœì‹  ì½”ë”©)"
fi

echo ""
echo "ğŸ“– ìì„¸í•œ ì •ë³´: free_models_info.md íŒŒì¼ í™•ì¸"
echo "ğŸš€ AutoCI ì‹œì‘: python start_autoci_agent.py"
echo ""
echo "ğŸ’¡ Tip: ì´ ëª¨ë¸ë“¤ì€ ì¸ì¦ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!"
echo "" 