#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì§€ì† í•™ìŠµ ë°ëª¬
ì‹¤ì œ ì‹ ê²½ë§ ê¸°ë°˜ 24/7 ìë™ í•™ìŠµ ì‹œìŠ¤í…œ
"""

import os
import sys
import time
import json
import sqlite3
import threading
import schedule
import signal
import logging
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import subprocess
import requests
import hashlib

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from sklearn.feature_extraction.text import TfidfVectorizer
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ê°„ì†Œí™”ëœ í•™ìŠµ ëª¨ë“œë¡œ ì‹¤í–‰")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('autoci_24h_learning.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContinuousLearningDatabase:
    """ì§€ì† í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self, db_path: str = "continuous_learning.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # í•™ìŠµ ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    input_text TEXT NOT NULL,
                    target_response TEXT,
                    feedback_score REAL,
                    source TEXT,
                    processed BOOLEAN DEFAULT FALSE,
                    quality_score REAL
                )
            ''')
            
            # í•™ìŠµ í†µê³„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_data_points INTEGER,
                    training_loss REAL,
                    validation_accuracy REAL,
                    learning_rate REAL,
                    epoch_count INTEGER,
                    model_version TEXT
                )
            ''')
            
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS model_checkpoints (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    model_path TEXT NOT NULL,
                    performance_score REAL,
                    training_time_hours REAL,
                    notes TEXT
                )
            ''')
            
            conn.commit()
    
    def add_learning_data(self, input_text: str, target_response: str = None, 
                         feedback_score: float = 0.0, source: str = "auto"):
        """í•™ìŠµ ë°ì´í„° ì¶”ê°€"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO learning_data 
                (timestamp, input_text, target_response, feedback_score, source)
                VALUES (?, ?, ?, ?, ?)
            ''', (datetime.now().isoformat(), input_text, target_response, feedback_score, source))
            conn.commit()
    
    def get_unprocessed_data(self, limit: int = 100) -> List[Tuple]:
        """ë¯¸ì²˜ë¦¬ í•™ìŠµ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, input_text, target_response, feedback_score, source
                FROM learning_data 
                WHERE processed = FALSE 
                ORDER BY timestamp DESC 
                LIMIT ?
            ''', (limit,))
            return cursor.fetchall()
    
    def mark_data_processed(self, data_ids: List[int]):
        """ë°ì´í„°ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ í‘œì‹œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            placeholders = ','.join(['?' for _ in data_ids])
            cursor.execute(f'''
                UPDATE learning_data 
                SET processed = TRUE 
                WHERE id IN ({placeholders})
            ''', data_ids)
            conn.commit()
    
    def log_training_stats(self, total_data: int, loss: float, accuracy: float, 
                          lr: float, epochs: int, model_version: str):
        """í•™ìŠµ í†µê³„ ê¸°ë¡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO learning_stats 
                (timestamp, total_data_points, training_loss, validation_accuracy, 
                 learning_rate, epoch_count, model_version)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (datetime.now().isoformat(), total_data, loss, accuracy, lr, epochs, model_version))
            conn.commit()

class DataCollector:
    """ìë™ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, db: ContinuousLearningDatabase):
        self.db = db
        self.collection_sources = {
            "github": self.collect_github_issues,
            "stackoverflow": self.collect_stackoverflow_questions,
            "unity_docs": self.collect_unity_documentation,
            "csharp_examples": self.collect_csharp_examples,
            "synthetic": self.generate_synthetic_data
        }
    
    def collect_github_issues(self) -> List[Dict]:
        """GitHub ì´ìŠˆì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # Unity C# ê´€ë ¨ ë¦¬í¬ì§€í† ë¦¬ë“¤
            repos = [
                "Unity-Technologies/UnityCsReference",
                "Unity-Technologies/ml-agents",
                "microsoft/vscode-csharp"
            ]
            
            collected_data = []
            
            for repo in repos:
                try:
                    # GitHub API í˜¸ì¶œ (ì‹¤ì œë¡œëŠ” API í‚¤ í•„ìš”)
                    url = f"https://api.github.com/repos/{repo}/issues"
                    params = {
                        "state": "closed",
                        "sort": "updated",
                        "per_page": 10,
                        "labels": "question,help wanted,bug"
                    }
                    
                    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” requests ì‚¬ìš©
                    # response = requests.get(url, params=params)
                    # issues = response.json()
                    
                    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
                    issues = [
                        {
                            "title": "Unity GameObjectê°€ ì‚­ì œë˜ì§€ ì•ŠëŠ” ë¬¸ì œ",
                            "body": "Destroy() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí–ˆëŠ”ë° ì˜¤ë¸Œì íŠ¸ê°€ ì—¬ì „íˆ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.",
                            "state": "closed"
                        },
                        {
                            "title": "C# ì½”ë£¨í‹´ì—ì„œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜",
                            "body": "StartCoroutineì„ ë°˜ë³µ í˜¸ì¶œí•˜ë©´ ë©”ëª¨ë¦¬ê°€ ê³„ì† ì¦ê°€í•©ë‹ˆë‹¤.",
                            "state": "closed"
                        }
                    ]
                    
                    for issue in issues:
                        question = f"{issue['title']}: {issue['body']}"
                        collected_data.append({
                            "input": question,
                            "source": f"github:{repo}",
                            "quality": 0.7
                        })
                        
                except Exception as e:
                    logger.warning(f"GitHub ìˆ˜ì§‘ ì˜¤ë¥˜ ({repo}): {e}")
            
            return collected_data
            
        except Exception as e:
            logger.error(f"GitHub ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def collect_stackoverflow_questions(self) -> List[Dict]:
        """Stack Overflow ì§ˆë¬¸ ìˆ˜ì§‘"""
        try:
            # Unity, C# íƒœê·¸ì˜ ì§ˆë¬¸ë“¤ (ì‹œë®¬ë ˆì´ì…˜)
            so_questions = [
                {
                    "title": "Unityì—ì„œ Singleton íŒ¨í„´ êµ¬í˜„í•˜ê¸°",
                    "body": "Unityì—ì„œ GameManagerë¥¼ Singletonìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                    "tags": ["unity", "c#", "singleton"]
                },
                {
                    "title": "C# async/await íŒ¨í„´ ì§ˆë¬¸",
                    "body": "ë¹„ë™ê¸° ë©”ì„œë“œì—ì„œ UI ì—…ë°ì´íŠ¸ê°€ ì•ˆ ë©ë‹ˆë‹¤.",
                    "tags": ["c#", "async-await", "unity"]
                },
                {
                    "title": "Unity Physics2D ì¶©ëŒ ê°ì§€",
                    "body": "OnTriggerEnter2Dê°€ í˜¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                    "tags": ["unity", "physics2d", "collision"]
                }
            ]
            
            collected_data = []
            for q in so_questions:
                question = f"{q['title']}: {q['body']}"
                collected_data.append({
                    "input": question,
                    "source": "stackoverflow",
                    "quality": 0.8
                })
            
            return collected_data
            
        except Exception as e:
            logger.error(f"Stack Overflow ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def collect_unity_documentation(self) -> List[Dict]:
        """Unity ë¬¸ì„œì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # Unity ê³µì‹ ë¬¸ì„œ ì£¼ì œë“¤ (ì‹œë®¬ë ˆì´ì…˜)
            unity_docs = [
                {
                    "topic": "GameObject ìƒì„±ê³¼ ì‚­ì œ",
                    "content": "Instantiate()ì™€ Destroy() ë©”ì„œë“œ ì‚¬ìš©ë²•"
                },
                {
                    "topic": "ì»´í¬ë„ŒíŠ¸ ì‹œìŠ¤í…œ",
                    "content": "GetComponent<>()ë¥¼ ì‚¬ìš©í•œ ì»´í¬ë„ŒíŠ¸ ì ‘ê·¼"
                },
                {
                    "topic": "ì½”ë£¨í‹´ í™œìš©",
                    "content": "StartCoroutine()ê³¼ StopCoroutine() ì‚¬ìš©"
                }
            ]
            
            collected_data = []
            for doc in unity_docs:
                question = f"Unity {doc['topic']}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
                answer = doc['content']
                collected_data.append({
                    "input": question,
                    "target": answer,
                    "source": "unity_docs",
                    "quality": 0.9
                })
            
            return collected_data
            
        except Exception as e:
            logger.error(f"Unity ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def collect_csharp_examples(self) -> List[Dict]:
        """C# ì˜ˆì œ ì½”ë“œ ìˆ˜ì§‘"""
        try:
            csharp_examples = [
                {
                    "question": "C# Listì—ì„œ ì¤‘ë³µ ì œê±°í•˜ëŠ” ë°©ë²•",
                    "answer": "LINQì˜ Distinct() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”: list.Distinct().ToList()"
                },
                {
                    "question": "C# Dictionary ì‚¬ìš©ë²•",
                    "answer": "Dictionary<string, int> dict = new Dictionary<string, int>();"
                },
                {
                    "question": "C# ì´ë²¤íŠ¸ ì„ ì–¸ê³¼ ì‚¬ìš©",
                    "answer": "public event Action<int> OnScoreChanged; OnScoreChanged?.Invoke(newScore);"
                }
            ]
            
            collected_data = []
            for example in csharp_examples:
                collected_data.append({
                    "input": example["question"],
                    "target": example["answer"],
                    "source": "csharp_examples",
                    "quality": 0.8
                })
            
            return collected_data
            
        except Exception as e:
            logger.error(f"C# ì˜ˆì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_synthetic_data(self) -> List[Dict]:
        """í•©ì„± ë°ì´í„° ìƒì„±"""
        try:
            import random
            
            # ì§ˆë¬¸ í…œí”Œë¦¿ë“¤
            question_templates = [
                "Unityì—ì„œ {object}ë¥¼ {action}í•˜ëŠ” ë°©ë²•ì€?",
                "C#ì—ì„œ {concept} êµ¬í˜„í•˜ë ¤ë©´?",
                "{problem} ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë²•ì€?",
                "{unity_feature} ì‚¬ìš©ë²• ì•Œë ¤ì£¼ì„¸ìš”",
                "{csharp_feature}ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?"
            ]
            
            # ë‹¨ì–´ ëª©ë¡ë“¤
            objects = ["GameObject", "Transform", "Rigidbody", "Collider", "Camera"]
            actions = ["ìƒì„±", "ì‚­ì œ", "ì´ë™", "íšŒì „", "ìŠ¤ì¼€ì¼ë§"]
            concepts = ["Singleton", "Observer íŒ¨í„´", "íŒ©í† ë¦¬ íŒ¨í„´", "State íŒ¨í„´"]
            problems = ["ë©”ëª¨ë¦¬ ëˆ„ìˆ˜", "ì„±ëŠ¥ ì €í•˜", "ì¶©ëŒ ê°ì§€ ì˜¤ë¥˜", "UI ì—…ë°ì´íŠ¸ ì§€ì—°"]
            unity_features = ["Animation", "Particle System", "Audio Source", "Navigation"]
            csharp_features = ["async/await", "LINQ", "Generic", "Delegate", "Event"]
            
            collected_data = []
            
            for _ in range(20):  # 20ê°œ í•©ì„± ë°ì´í„° ìƒì„±
                template = random.choice(question_templates)
                
                if "{object}" in template:
                    question = template.format(
                        object=random.choice(objects),
                        action=random.choice(actions)
                    )
                elif "{concept}" in template:
                    question = template.format(concept=random.choice(concepts))
                elif "{problem}" in template:
                    question = template.format(problem=random.choice(problems))
                elif "{unity_feature}" in template:
                    question = template.format(unity_feature=random.choice(unity_features))
                elif "{csharp_feature}" in template:
                    question = template.format(csharp_feature=random.choice(csharp_features))
                else:
                    question = template
                
                collected_data.append({
                    "input": question,
                    "source": "synthetic",
                    "quality": 0.6
                })
            
            return collected_data
            
        except Exception as e:
            logger.error(f"í•©ì„± ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def collect_all_sources(self) -> int:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        total_collected = 0
        
        for source_name, collect_func in self.collection_sources.items():
            try:
                logger.info(f"ğŸ“¥ {source_name}ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                data_list = collect_func()
                
                for data in data_list:
                    self.db.add_learning_data(
                        input_text=data["input"],
                        target_response=data.get("target", ""),
                        feedback_score=data.get("quality", 0.5),
                        source=data["source"]
                    )
                    total_collected += 1
                
                logger.info(f"âœ… {source_name}: {len(data_list)}ê°œ ë°ì´í„° ìˆ˜ì§‘ë¨")
                
            except Exception as e:
                logger.error(f"âŒ {source_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ğŸ¯ ì´ {total_collected}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return total_collected

class ContinuousNeuralLearner:
    """ì§€ì†ì  ì‹ ê²½ë§ í•™ìŠµê¸°"""
    
    def __init__(self, db: ContinuousLearningDatabase):
        self.db = db
        self.model = None
        self.optimizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if TORCH_AVAILABLE else "cpu"
        self.model_version = "v1.0"
        self.learning_rate = 0.001
        
        if TORCH_AVAILABLE:
            self.init_neural_network()
        else:
            logger.warning("PyTorch ì—†ìŒ - ë¡œê·¸ ê¸°ë°˜ í•™ìŠµ ëª¨ë“œ")
    
    def init_neural_network(self):
        """ì‹ ê²½ë§ ì´ˆê¸°í™”"""
        try:
            class AutoCILearningNetwork(nn.Module):
                def __init__(self, vocab_size=10000, embed_dim=256, hidden_dim=512):
                    super(AutoCILearningNetwork, self).__init__()
                    self.embedding = nn.Embedding(vocab_size, embed_dim)
                    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, num_layers=2)
                    self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8)
                    self.classifier = nn.Sequential(
                        nn.Linear(hidden_dim, 256),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(256, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1),
                        nn.Sigmoid()
                    )
                
                def forward(self, x):
                    embedded = self.embedding(x)
                    lstm_out, _ = self.lstm(embedded)
                    
                    # ì–´í…ì…˜ ì ìš©
                    attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
                    
                    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì˜ ì¶œë ¥ ì‚¬ìš©
                    final_hidden = attn_out[:, -1, :]
                    output = self.classifier(final_hidden)
                    
                    return output
            
            self.model = AutoCILearningNetwork().to(self.device)
            self.optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)
            self.criterion = nn.BCELoss()
            
            logger.info(f"ğŸ§  ì‹ ê²½ë§ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
            
        except Exception as e:
            logger.error(f"ì‹ ê²½ë§ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model = None
    
    def prepare_training_data(self, data_batch: List[Tuple]) -> Optional[Tuple]:
        """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
        try:
            if not data_batch:
                return None
            
            # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜)
            def text_to_tokens(text: str, max_length: int = 50) -> List[int]:
                words = text.lower().split()[:max_length]
                tokens = []
                for word in words:
                    token = hash(word) % 10000  # ì–´íœ˜ í¬ê¸°ë¥¼ 10000ìœ¼ë¡œ ì œí•œ
                    tokens.append(abs(token))
                
                # íŒ¨ë”©
                while len(tokens) < max_length:
                    tokens.append(0)
                
                return tokens[:max_length]
            
            X = []
            y = []
            
            for data_id, input_text, target_response, feedback_score, source in data_batch:
                tokens = text_to_tokens(input_text)
                X.append(tokens)
                
                # í”¼ë“œë°± ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                normalized_score = (feedback_score + 1.0) / 2.0
                y.append([normalized_score])
            
            if TORCH_AVAILABLE:
                X_tensor = torch.tensor(X, dtype=torch.long).to(self.device)
                y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)
                return X_tensor, y_tensor, [d[0] for d in data_batch]
            else:
                return X, y, [d[0] for d in data_batch]
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None
    
    def train_batch(self, data_batch: List[Tuple]) -> Dict:
        """ë°°ì¹˜ í•™ìŠµ ì‹¤í–‰"""
        try:
            if not self.model or not TORCH_AVAILABLE:
                logger.info("ğŸ“š ë¡œê·¸ ê¸°ë°˜ í•™ìŠµ ëª¨ë“œ")
                return {"loss": 0.0, "accuracy": 0.8, "processed": len(data_batch)}
            
            training_data = self.prepare_training_data(data_batch)
            if not training_data:
                return {"loss": 0.0, "accuracy": 0.0, "processed": 0}
            
            X_tensor, y_tensor, data_ids = training_data
            
            # í•™ìŠµ ëª¨ë“œ ì„¤ì •
            self.model.train()
            
            # ìˆœì „íŒŒ
            outputs = self.model(X_tensor)
            loss = self.criterion(outputs, y_tensor)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # ì •í™•ë„ ê³„ì‚°
            predictions = (outputs > 0.5).float()
            accuracy = (predictions == (y_tensor > 0.5).float()).float().mean().item()
            
            # ë°ì´í„°ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ í‘œì‹œ
            self.db.mark_data_processed(data_ids)
            
            return {
                "loss": loss.item(),
                "accuracy": accuracy,
                "processed": len(data_batch)
            }
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"loss": 0.0, "accuracy": 0.0, "processed": 0}
    
    def continuous_learning_cycle(self):
        """ì§€ì†ì  í•™ìŠµ ì‚¬ì´í´"""
        try:
            logger.info("ğŸ”„ ì§€ì†ì  í•™ìŠµ ì‚¬ì´í´ ì‹œì‘")
            
            # ë¯¸ì²˜ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            unprocessed_data = self.db.get_unprocessed_data(limit=50)
            
            if not unprocessed_data:
                logger.info("ğŸ“­ ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„° ì—†ìŒ")
                return
            
            logger.info(f"ğŸ“š {len(unprocessed_data)}ê°œ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘")
            
            # ë°°ì¹˜ í•™ìŠµ ì‹¤í–‰
            batch_size = 10
            total_loss = 0.0
            total_accuracy = 0.0
            total_processed = 0
            batch_count = 0
            
            for i in range(0, len(unprocessed_data), batch_size):
                batch = unprocessed_data[i:i + batch_size]
                result = self.train_batch(batch)
                
                total_loss += result["loss"]
                total_accuracy += result["accuracy"]
                total_processed += result["processed"]
                batch_count += 1
                
                logger.info(f"ë°°ì¹˜ {batch_count}: ì†ì‹¤={result['loss']:.4f}, ì •í™•ë„={result['accuracy']:.4f}")
            
            # í‰ê·  ê³„ì‚°
            avg_loss = total_loss / batch_count if batch_count > 0 else 0.0
            avg_accuracy = total_accuracy / batch_count if batch_count > 0 else 0.0
            
            # í†µê³„ ê¸°ë¡
            self.db.log_training_stats(
                total_data=total_processed,
                loss=avg_loss,
                accuracy=avg_accuracy,
                lr=self.learning_rate,
                epochs=1,
                model_version=self.model_version
            )
            
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: í‰ê·  ì†ì‹¤={avg_loss:.4f}, í‰ê·  ì •í™•ë„={avg_accuracy:.4f}")
            
            # ëª¨ë¸ ì €ì¥ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if batch_count > 0:
                self.save_model_checkpoint(avg_accuracy)
            
        except Exception as e:
            logger.error(f"ì§€ì†ì  í•™ìŠµ ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
    
    def save_model_checkpoint(self, performance_score: float):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            if not self.model or not TORCH_AVAILABLE:
                return
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_path = f"autoci_model_checkpoint_{timestamp}.pth"
            
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'model_version': self.model_version,
                'performance_score': performance_score,
                'timestamp': datetime.now().isoformat()
            }
            
            torch.save(checkpoint, model_path)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO model_checkpoints 
                    (timestamp, model_path, performance_score, training_time_hours, notes)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    datetime.now().isoformat(),
                    model_path,
                    performance_score,
                    1.0,  # 1ì‹œê°„ ë‹¨ìœ„
                    f"ìë™ ì €ì¥ - ì„±ëŠ¥ ì ìˆ˜: {performance_score:.4f}"
                ))
                conn.commit()
            
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {model_path}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

class ContinuousLearningDaemon:
    """24ì‹œê°„ ì§€ì† í•™ìŠµ ë°ëª¬"""
    
    def __init__(self):
        self.db = ContinuousLearningDatabase()
        self.data_collector = DataCollector(self.db)
        self.neural_learner = ContinuousNeuralLearner(self.db)
        self.running = True
        self.start_time = datetime.now()
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        self.setup_schedules()
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì • (graceful shutdown)
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        logger.info("ğŸš€ AutoCI 24ì‹œê°„ ì§€ì† í•™ìŠµ ë°ëª¬ ì‹œì‘")
    
    def setup_schedules(self):
        """í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„
        schedule.every(30).minutes.do(self.data_collector.collect_all_sources)  # 30ë¶„ë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘
        
        # í•™ìŠµ ìŠ¤ì¼€ì¤„
        schedule.every(15).minutes.do(self.neural_learner.continuous_learning_cycle)  # 15ë¶„ë§ˆë‹¤ í•™ìŠµ
        
        # ìƒíƒœ ì²´í¬ ìŠ¤ì¼€ì¤„
        schedule.every(1).hours.do(self.log_system_status)  # 1ì‹œê°„ë§ˆë‹¤ ìƒíƒœ ë¡œê·¸
        
        # ëª¨ë¸ ë°±ì—… ìŠ¤ì¼€ì¤„
        schedule.every(6).hours.do(self.backup_models)  # 6ì‹œê°„ë§ˆë‹¤ ëª¨ë¸ ë°±ì—…
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ìŠ¤ì¼€ì¤„
        schedule.every(1).days.do(self.cleanup_old_data)  # 1ì¼ë§ˆë‹¤ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        
        logger.info("ğŸ“… í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")
    
    def signal_handler(self, signum, frame):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ (graceful shutdown)"""
        logger.info(f"ğŸ›‘ ì¢…ë£Œ ì‹œê·¸ë„ ë°›ìŒ ({signum}), ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.running = False
    
    def log_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…"""
        try:
            uptime = datetime.now() - self.start_time
            
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                
                # ì´ í•™ìŠµ ë°ì´í„° ìˆ˜
                cursor.execute("SELECT COUNT(*) FROM learning_data")
                total_data = cursor.fetchone()[0]
                
                # ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜
                cursor.execute("SELECT COUNT(*) FROM learning_data WHERE processed = TRUE")
                processed_data = cursor.fetchone()[0]
                
                # ìµœê·¼ í•™ìŠµ í†µê³„
                cursor.execute('''
                    SELECT training_loss, validation_accuracy 
                    FROM learning_stats 
                    ORDER BY timestamp DESC 
                    LIMIT 1
                ''')
                latest_stats = cursor.fetchone()
            
            status_info = {
                "uptime_hours": uptime.total_seconds() / 3600,
                "total_data_points": total_data,
                "processed_data_points": processed_data,
                "processing_rate": f"{processed_data/total_data*100:.1f}%" if total_data > 0 else "0%",
                "latest_loss": latest_stats[0] if latest_stats else "N/A",
                "latest_accuracy": latest_stats[1] if latest_stats else "N/A"
            }
            
            logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: {json.dumps(status_info, indent=2)}")
            
        except Exception as e:
            logger.error(f"ìƒíƒœ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def backup_models(self):
        """ëª¨ë¸ ë°±ì—…"""
        try:
            # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT model_path, performance_score 
                    FROM model_checkpoints 
                    ORDER BY timestamp DESC 
                    LIMIT 1
                ''')
                latest_checkpoint = cursor.fetchone()
            
            if latest_checkpoint:
                model_path, score = latest_checkpoint
                backup_dir = "model_backups"
                os.makedirs(backup_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = f"{backup_dir}/autoci_backup_{timestamp}_score_{score:.3f}.pth"
                
                if os.path.exists(model_path):
                    import shutil
                    shutil.copy2(model_path, backup_path)
                    logger.info(f"ğŸ’¾ ëª¨ë¸ ë°±ì—… ì™„ë£Œ: {backup_path}")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë°±ì—… ì‹¤íŒ¨: {e}")
    
    def cleanup_old_data(self):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        try:
            # 30ì¼ ì´ì „ ì²˜ë¦¬ëœ ë°ì´í„° ì‚­ì œ
            cutoff_date = (datetime.now() - timedelta(days=30)).isoformat()
            
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                
                # ì˜¤ë˜ëœ ì²˜ë¦¬ëœ ë°ì´í„° ì‚­ì œ
                cursor.execute('''
                    DELETE FROM learning_data 
                    WHERE processed = TRUE AND timestamp < ?
                ''', (cutoff_date,))
                
                deleted_count = cursor.rowcount
                conn.commit()
            
            logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬: {deleted_count}ê°œ ë ˆì½”ë“œ ì‚­ì œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def run_daemon(self):
        """ë°ëª¬ ë©”ì¸ ë£¨í”„"""
        logger.info("ğŸ”„ 24ì‹œê°„ ì§€ì† í•™ìŠµ ë°ëª¬ ë©”ì¸ ë£¨í”„ ì‹œì‘")
        
        # ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘
        logger.info("ğŸ¯ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        self.data_collector.collect_all_sources()
        
        # ì´ˆê¸° í•™ìŠµ
        logger.info("ğŸ§  ì´ˆê¸° í•™ìŠµ ì‚¬ì´í´ ì‹œì‘")
        self.neural_learner.continuous_learning_cycle()
        
        # ë©”ì¸ ë£¨í”„
        while self.running:
            try:
                # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰
                schedule.run_pending()
                
                # 1ë¶„ ëŒ€ê¸°
                time.sleep(60)
                
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ë¡œ ì¢…ë£Œ")
                break
            except Exception as e:
                logger.error(f"ë©”ì¸ ë£¨í”„ ì˜¤ë¥˜: {e}")
                logger.error(traceback.format_exc())
                time.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œì‘
        
        logger.info("ğŸ‘‹ 24ì‹œê°„ ì§€ì† í•™ìŠµ ë°ëª¬ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AutoCI 24ì‹œê°„ ì§€ì† í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë°ëª¬ ì´ˆê¸°í™” ë° ì‹¤í–‰
        daemon = ContinuousLearningDaemon()
        daemon.run_daemon()
        
    except Exception as e:
        logger.error(f"ë°ëª¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())