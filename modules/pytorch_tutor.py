#!/usr/bin/env python3
"""
PyTorch íŠœí„° ì‹œìŠ¤í…œ
PyTorchì˜ ê¸°ì´ˆë¶€í„° ê³ ê¸‰ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ í•™ìŠµì„ ë„ì™€ì£¼ëŠ” AI íŠœí„°
"""

import os
import json
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class PyTorchTopic:
    """PyTorch í•™ìŠµ ì£¼ì œ"""
    topic_id: str
    category: str  # basics, tensors, autograd, nn, optimization, etc.
    title: str
    difficulty: str  # beginner, intermediate, advanced
    prerequisites: List[str]
    content: Dict[str, Any]

class PyTorchTutor:
    """PyTorch í•™ìŠµ ë„ìš°ë¯¸"""
    
    def __init__(self):
        self.current_topic = None
        self.user_progress = {}
        self.learning_path = []
        
        # PyTorch í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼
        self.curriculum = {
            "basics": {
                "title": "PyTorch ê¸°ì´ˆ",
                "topics": [
                    {
                        "id": "pytorch_intro",
                        "title": "PyTorchë€ ë¬´ì—‡ì¸ê°€?",
                        "content": {
                            "ì„¤ëª…": "PyTorchëŠ” Facebookì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
                            "íŠ¹ì§•": [
                                "ë™ì  ê³„ì‚° ê·¸ë˜í”„ (Dynamic Computational Graph)",
                                "Pythonicí•œ ë¬¸ë²•",
                                "GPU ê°€ì† ì§€ì›",
                                "ìë™ ë¯¸ë¶„ (Autograd)"
                            ],
                            "ì˜ˆì œ": """
import torch

# PyTorch ë²„ì „ í™•ì¸
print(torch.__version__)

# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
print(torch.cuda.is_available())
"""
                        }
                    },
                    {
                        "id": "tensor_basics",
                        "title": "í…ì„œ(Tensor) ê¸°ì´ˆ",
                        "content": {
                            "ì„¤ëª…": "í…ì„œëŠ” PyTorchì˜ ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¡œ, ë‹¤ì°¨ì› ë°°ì—´ì…ë‹ˆë‹¤.",
                            "ì£¼ìš”_ë©”ì„œë“œ": {
                                "ìƒì„±": ["torch.tensor()", "torch.zeros()", "torch.ones()", "torch.randn()"],
                                "í˜•íƒœ": ["shape", "size()", "reshape()", "view()"],
                                "ì—°ì‚°": ["add()", "mul()", "matmul()", "sum()"]
                            },
                            "ì˜ˆì œ": """
import torch

# í…ì„œ ìƒì„±
x = torch.tensor([1, 2, 3])
print(f"1D í…ì„œ: {x}")

# 2D í…ì„œ ìƒì„±
matrix = torch.tensor([[1, 2], [3, 4]])
print(f"2D í…ì„œ:\\n{matrix}")

# ëœë¤ í…ì„œ
random_tensor = torch.randn(3, 3)
print(f"ëœë¤ í…ì„œ:\\n{random_tensor}")

# í…ì„œ ì—°ì‚°
y = torch.tensor([4, 5, 6])
z = x + y
print(f"ë§ì…ˆ ê²°ê³¼: {z}")
"""
                        }
                    }
                ]
            },
            "tensors": {
                "title": "í…ì„œ ì‹¬í™”",
                "topics": [
                    {
                        "id": "tensor_operations",
                        "title": "í…ì„œ ì—°ì‚° ì‹¬í™”",
                        "content": {
                            "ì„¤ëª…": "í…ì„œì˜ ë‹¤ì–‘í•œ ì—°ì‚°ê³¼ ë³€í™˜ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ì—°ì‚°_ì¢…ë¥˜": {
                                "ì‚°ìˆ ì—°ì‚°": ["add", "sub", "mul", "div"],
                                "í–‰ë ¬ì—°ì‚°": ["mm", "bmm", "matmul"],
                                "ì§‘ê³„ì—°ì‚°": ["sum", "mean", "max", "min"],
                                "í˜•íƒœë³€í™˜": ["reshape", "view", "squeeze", "unsqueeze"]
                            },
                            "ì˜ˆì œ": """
import torch

# í–‰ë ¬ ê³±ì…ˆ
A = torch.randn(2, 3)
B = torch.randn(3, 4)
C = torch.matmul(A, B)  # ë˜ëŠ” A @ B
print(f"í–‰ë ¬ ê³±ì…ˆ ê²°ê³¼ shape: {C.shape}")

# Broadcasting
x = torch.tensor([1, 2, 3])
y = torch.tensor([[1], [2], [3]])
z = x + y  # Broadcasting ì ìš©
print(f"Broadcasting ê²°ê³¼:\\n{z}")

# In-place ì—°ì‚°
x = torch.tensor([1, 2, 3], dtype=torch.float32)
x.add_(1)  # In-place addition
print(f"In-place ë§ì…ˆ: {x}")
"""
                        }
                    },
                    {
                        "id": "tensor_indexing",
                        "title": "í…ì„œ ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±",
                        "content": {
                            "ì„¤ëª…": "í…ì„œì˜ íŠ¹ì • ìš”ì†Œë‚˜ ë¶€ë¶„ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ê¸°ë²•": [
                                "ê¸°ë³¸ ì¸ë±ì‹±",
                                "ìŠ¬ë¼ì´ì‹±",
                                "ë§ˆìŠ¤í‚¹",
                                "gatherì™€ scatter"
                            ],
                            "ì˜ˆì œ": """
import torch

# 2D í…ì„œ ìƒì„±
tensor = torch.randn(5, 5)

# ê¸°ë³¸ ì¸ë±ì‹±
print(f"ì²« ë²ˆì§¸ í–‰: {tensor[0]}")
print(f"(1,2) ìœ„ì¹˜ ìš”ì†Œ: {tensor[1, 2]}")

# ìŠ¬ë¼ì´ì‹±
print(f"ì²˜ìŒ 3ê°œ í–‰:\\n{tensor[:3]}")
print(f"2-4ì—´:\\n{tensor[:, 1:4]}")

# ì¡°ê±´ë¶€ ì„ íƒ (ë§ˆìŠ¤í‚¹)
mask = tensor > 0
positive_values = tensor[mask]
print(f"ì–‘ìˆ˜ ê°’ë“¤: {positive_values}")

# Fancy indexing
indices = torch.tensor([0, 2, 4])
selected_rows = tensor[indices]
print(f"ì„ íƒëœ í–‰ë“¤:\\n{selected_rows}")
"""
                        }
                    }
                ]
            },
            "autograd": {
                "title": "ìë™ ë¯¸ë¶„ (Autograd)",
                "topics": [
                    {
                        "id": "autograd_basics",
                        "title": "Autograd ê¸°ì´ˆ",
                        "content": {
                            "ì„¤ëª…": "PyTorchì˜ ìë™ ë¯¸ë¶„ ì‹œìŠ¤í…œì„ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "í•µì‹¬ê°œë…": [
                                "requires_grad",
                                "backward()",
                                "grad",
                                "ê³„ì‚° ê·¸ë˜í”„"
                            ],
                            "ì˜ˆì œ": """
import torch

# requires_grad=Trueë¡œ í…ì„œ ìƒì„±
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2 + 3 * x + 1

# ì—­ì „íŒŒ
y.backward()

# ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
print(f"xì˜ ê°’: {x}")
print(f"yì˜ ê°’: {y}")
print(f"dy/dx: {x.grad}")  # 2*x + 3 = 2*2 + 3 = 7

# ë” ë³µì¡í•œ ì˜ˆì œ
x = torch.randn(3, requires_grad=True)
y = x * 2
z = y * y * 3
out = z.mean()

out.backward()
print(f"xì˜ ê·¸ë˜ë””ì–¸íŠ¸: {x.grad}")
"""
                        }
                    },
                    {
                        "id": "computational_graph",
                        "title": "ê³„ì‚° ê·¸ë˜í”„ ì´í•´í•˜ê¸°",
                        "content": {
                            "ì„¤ëª…": "PyTorchê°€ ì—°ì‚°ì„ ì¶”ì í•˜ê³  ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ì£¼ìš”ê°œë…": [
                                "ë™ì  ê³„ì‚° ê·¸ë˜í”„",
                                "leaf node",
                                "grad_fn",
                                "detach()"
                            ],
                            "ì˜ˆì œ": """
import torch

# ê³„ì‚° ê·¸ë˜í”„ ìƒì„±
x = torch.tensor(1.0, requires_grad=True)
y = torch.tensor(2.0, requires_grad=True)

z = x * y
w = z + x

# ê·¸ë˜í”„ ì •ë³´ í™•ì¸
print(f"z.grad_fn: {z.grad_fn}")
print(f"w.grad_fn: {w.grad_fn}")
print(f"x.is_leaf: {x.is_leaf}")
print(f"w.is_leaf: {w.is_leaf}")

# ì—­ì „íŒŒ
w.backward()
print(f"x.grad: {x.grad}")  # dw/dx = 1 + y = 3
print(f"y.grad: {y.grad}")  # dw/dy = x = 1

# detach() ì‚¬ìš©
x_detached = x.detach()
print(f"x_detached.requires_grad: {x_detached.requires_grad}")
"""
                        }
                    }
                ]
            },
            "nn": {
                "title": "ì‹ ê²½ë§ êµ¬ì¶• (torch.nn)",
                "topics": [
                    {
                        "id": "nn_module",
                        "title": "nn.Module ê¸°ì´ˆ",
                        "content": {
                            "ì„¤ëª…": "PyTorchì—ì„œ ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ëŠ” ê¸°ë³¸ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "í•µì‹¬ìš”ì†Œ": [
                                "nn.Module ìƒì†",
                                "__init__ ë©”ì„œë“œ",
                                "forward ë©”ì„œë“œ",
                                "íŒŒë¼ë¯¸í„° ê´€ë¦¬"
                            ],
                            "ì˜ˆì œ": """
import torch
import torch.nn as nn
import torch.nn.functional as F

# ê°„ë‹¨í•œ ì‹ ê²½ë§ ì •ì˜
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# ëª¨ë¸ ìƒì„±
model = SimpleNet(10, 20, 5)
print(model)

# íŒŒë¼ë¯¸í„° í™•ì¸
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# ìˆœì „íŒŒ
input_data = torch.randn(1, 10)
output = model(input_data)
print(f"ì¶œë ¥ shape: {output.shape}")
"""
                        }
                    },
                    {
                        "id": "common_layers",
                        "title": "ì£¼ìš” ë ˆì´ì–´ ì´í•´í•˜ê¸°",
                        "content": {
                            "ì„¤ëª…": "ìì£¼ ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ë ˆì´ì–´ë“¤ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ë ˆì´ì–´_ì¢…ë¥˜": {
                                "Linear": "ì™„ì „ ì—°ê²° ë ˆì´ì–´",
                                "Conv2d": "2D í•©ì„±ê³± ë ˆì´ì–´",
                                "MaxPool2d": "2D ìµœëŒ€ í’€ë§",
                                "BatchNorm2d": "ë°°ì¹˜ ì •ê·œí™”",
                                "Dropout": "ë“œë¡­ì•„ì›ƒ"
                            },
                            "ì˜ˆì œ": """
import torch
import torch.nn as nn

# CNN ì˜ˆì œ
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # í•©ì„±ê³± ë ˆì´ì–´
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        
        # í’€ë§ ë ˆì´ì–´
        self.pool = nn.MaxPool2d(2, 2)
        
        # ì •ê·œí™”ì™€ ë“œë¡­ì•„ì›ƒ
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.dropout = nn.Dropout(0.25)
        
        # ì™„ì „ ì—°ê²° ë ˆì´ì–´
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        
    def forward(self, x):
        # Conv -> BN -> ReLU -> Pool
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        
        # Flatten
        x = x.view(-1, 64 * 7 * 7)
        
        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# ëª¨ë¸ í…ŒìŠ¤íŠ¸
model = SimpleCNN()
input_tensor = torch.randn(1, 1, 28, 28)
output = model(input_tensor)
print(f"ì…ë ¥ shape: {input_tensor.shape}")
print(f"ì¶œë ¥ shape: {output.shape}")
"""
                        }
                    }
                ]
            },
            "training": {
                "title": "ëª¨ë¸ í•™ìŠµ",
                "topics": [
                    {
                        "id": "loss_functions",
                        "title": "ì†ì‹¤ í•¨ìˆ˜",
                        "content": {
                            "ì„¤ëª…": "ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ì™€ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ì†ì‹¤í•¨ìˆ˜": {
                                "MSELoss": "íšŒê·€ ë¬¸ì œ",
                                "CrossEntropyLoss": "ë‹¤ì¤‘ ë¶„ë¥˜",
                                "BCELoss": "ì´ì§„ ë¶„ë¥˜",
                                "L1Loss": "MAE ì†ì‹¤"
                            },
                            "ì˜ˆì œ": """
import torch
import torch.nn as nn

# íšŒê·€ ì†ì‹¤
mse_loss = nn.MSELoss()
predictions = torch.randn(10, 1)
targets = torch.randn(10, 1)
loss = mse_loss(predictions, targets)
print(f"MSE Loss: {loss.item()}")

# ë¶„ë¥˜ ì†ì‹¤
ce_loss = nn.CrossEntropyLoss()
logits = torch.randn(5, 10)  # 5ê°œ ìƒ˜í”Œ, 10ê°œ í´ë˜ìŠ¤
labels = torch.randint(0, 10, (5,))  # ì •ë‹µ ë ˆì´ë¸”
loss = ce_loss(logits, labels)
print(f"Cross Entropy Loss: {loss.item()}")

# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜
def custom_loss(output, target):
    return torch.mean((output - target) ** 2 + 0.01 * torch.abs(output))

output = torch.randn(10, 1, requires_grad=True)
target = torch.randn(10, 1)
loss = custom_loss(output, target)
print(f"Custom Loss: {loss.item()}")
"""
                        }
                    },
                    {
                        "id": "optimizers",
                        "title": "ì˜µí‹°ë§ˆì´ì €",
                        "content": {
                            "ì„¤ëª…": "ë‹¤ì–‘í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "ì˜µí‹°ë§ˆì´ì €_ì¢…ë¥˜": {
                                "SGD": "í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•",
                                "Adam": "ì ì‘ì  ëª¨ë©˜íŠ¸ ì¶”ì •",
                                "RMSprop": "Root Mean Square Propagation",
                                "AdamW": "Weight Decayê°€ ê°œì„ ëœ Adam"
                            },
                            "ì˜ˆì œ": """
import torch
import torch.nn as nn
import torch.optim as optim

# ê°„ë‹¨í•œ ëª¨ë¸
model = nn.Linear(10, 1)

# ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €
optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer_adam = optim.Adam(model.parameters(), lr=0.001)
optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)

# í•™ìŠµ ë£¨í”„ ì˜ˆì œ
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    # ê°€ìƒì˜ ë°ì´í„°
    inputs = torch.randn(32, 10)
    targets = torch.randn(32, 1)
    
    # ìˆœì „íŒŒ
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    # ì—­ì „íŒŒ ë° ìµœì í™”
    optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
    loss.backward()        # ì—­ì „íŒŒ
    optimizer.step()       # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
for epoch in range(10):
    # í•™ìŠµ ì½”ë“œ...
    scheduler.step()
    print(f"Current LR: {scheduler.get_last_lr()}")
"""
                        }
                    },
                    {
                        "id": "training_loop",
                        "title": "ì™„ì „í•œ í•™ìŠµ ë£¨í”„",
                        "content": {
                            "ì„¤ëª…": "ì „ì²´ í•™ìŠµ ê³¼ì •ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                            "êµ¬ì„±ìš”ì†Œ": [
                                "ë°ì´í„° ë¡œë”©",
                                "ëª¨ë¸ ì •ì˜",
                                "ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €",
                                "í•™ìŠµ ë£¨í”„",
                                "ê²€ì¦ ë£¨í”„",
                                "ëª¨ë¸ ì €ì¥/ë¡œë“œ"
                            ],
                            "ì˜ˆì œ": """
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ê°€ìƒì˜ ë°ì´í„°ì…‹
X_train = torch.randn(1000, 20)
y_train = torch.randint(0, 2, (1000,))
X_val = torch.randn(200, 20)
y_val = torch.randint(0, 2, (200,))

# ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë”
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# ëª¨ë¸ ì •ì˜
class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(20, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc3(x))
        return x

# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
model = BinaryClassifier()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ í•¨ìˆ˜
def train_epoch(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0
    correct = 0
    
    for batch_idx, (data, target) in enumerate(loader):
        optimizer.zero_grad()
        output = model(data).squeeze()
        loss = criterion(output, target.float())
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pred = (output > 0.5).float()
        correct += pred.eq(target.float()).sum().item()
    
    return total_loss / len(loader), correct / len(loader.dataset)

# ê²€ì¦ í•¨ìˆ˜
def validate(model, loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    
    with torch.no_grad():
        for data, target in loader:
            output = model(data).squeeze()
            loss = criterion(output, target.float())
            
            total_loss += loss.item()
            pred = (output > 0.5).float()
            correct += pred.eq(target.float()).sum().item()
    
    return total_loss / len(loader), correct / len(loader.dataset)

# í•™ìŠµ ë£¨í”„
num_epochs = 10
best_val_acc = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)
    val_loss, val_acc = validate(model, val_loader, criterion)
    
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print("Model saved!")

# ëª¨ë¸ ë¡œë“œ
model.load_state_dict(torch.load('best_model.pth'))
print("Model loaded!")
"""
                        }
                    }
                ]
            }
        }
        
        # ëŒ€í™”í˜• í•™ìŠµ í”„ë¡¬í”„íŠ¸
        self.interactive_prompts = {
            "tensor_practice": [
                "torch.tensor([1, 2, 3])ì„ ìƒì„±í•´ë³´ì„¸ìš”",
                "3x3 í¬ê¸°ì˜ ëœë¤ í…ì„œë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”",
                "ë‘ í…ì„œë¥¼ ë”í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”"
            ],
            "autograd_practice": [
                "requires_grad=Trueì¸ í…ì„œë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”",
                "ê°„ë‹¨í•œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•´ë³´ì„¸ìš”",
                "backward()ë¥¼ í˜¸ì¶œí•˜ê³  gradë¥¼ í™•ì¸í•´ë³´ì„¸ìš”"
            ],
            "nn_practice": [
                "nn.Linear ë ˆì´ì–´ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”",
                "ê°„ë‹¨í•œ nn.Module í´ë˜ìŠ¤ë¥¼ ì •ì˜í•´ë³´ì„¸ìš”",
                "forward ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”"
            ]
        }
        
    def get_topic_explanation(self, topic_id: str) -> Dict[str, Any]:
        """íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ì„¤ëª…ì„ ë°˜í™˜"""
        for category, content in self.curriculum.items():
            for topic in content.get("topics", []):
                if topic["id"] == topic_id:
                    return {
                        "category": category,
                        "title": topic["title"],
                        "content": topic["content"],
                        "prerequisites": topic.get("prerequisites", [])
                    }
        return None
    
    def suggest_next_topic(self, current_topic: str) -> List[str]:
        """ë‹¤ìŒì— í•™ìŠµí•  ì£¼ì œ ì¶”ì²œ"""
        suggestions = []
        
        # í˜„ì¬ ì£¼ì œì˜ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
        current_category = None
        current_index = None
        
        for category, content in self.curriculum.items():
            topics = content.get("topics", [])
            for i, topic in enumerate(topics):
                if topic["id"] == current_topic:
                    current_category = category
                    current_index = i
                    break
        
        if current_category and current_index is not None:
            topics = self.curriculum[current_category]["topics"]
            
            # ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ìŒ ì£¼ì œ
            if current_index + 1 < len(topics):
                suggestions.append(topics[current_index + 1]["id"])
            
            # ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ì˜ ì²« ì£¼ì œ
            categories = list(self.curriculum.keys())
            current_cat_index = categories.index(current_category)
            if current_cat_index + 1 < len(categories):
                next_category = categories[current_cat_index + 1]
                if self.curriculum[next_category]["topics"]:
                    suggestions.append(self.curriculum[next_category]["topics"][0]["id"])
        
        return suggestions
    
    def generate_practice_code(self, topic_id: str) -> str:
        """ì£¼ì œì— ë§ëŠ” ì‹¤ìŠµ ì½”ë“œ ìƒì„±"""
        topic_info = self.get_topic_explanation(topic_id)
        if not topic_info:
            return None
            
        practice_template = f"""# {topic_info['title']} ì‹¤ìŠµ

import torch
import torch.nn as nn
import torch.nn.functional as F

# TODO: ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•´ë³´ì„¸ìš”

"""
        
        # ì£¼ì œë³„ ì‹¤ìŠµ ì½”ë“œ í…œí”Œë¦¿ ì¶”ê°€
        if topic_id == "tensor_basics":
            practice_template += """# 1. ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ì„œ ìƒì„±í•˜ê¸°
# TODO: í¬ê¸°ê°€ (3, 4)ì¸ 0ìœ¼ë¡œ ì±„ì›Œì§„ í…ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”
zeros_tensor = # ???

# TODO: í¬ê¸°ê°€ (2, 3)ì¸ 1ìœ¼ë¡œ ì±„ì›Œì§„ í…ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”
ones_tensor = # ???

# TODO: 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ìë¡œ í…ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”
range_tensor = # ???

# 2. í…ì„œ ì—°ì‚°
# TODO: ë‘ í…ì„œë¥¼ element-wiseë¡œ ê³±í•˜ì„¸ìš”
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
result = # ???

print(f"Zeros: {zeros_tensor}")
print(f"Ones: {ones_tensor}")
print(f"Range: {range_tensor}")
print(f"Multiplication: {result}")
"""
        elif topic_id == "autograd_basics":
            practice_template += """# 1. ìë™ ë¯¸ë¶„ ì—°ìŠµ
# TODO: xì— ëŒ€í•´ requires_grad=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”
x = torch.tensor(3.0)  # ???

# TODO: y = x^2 + 2x + 1 í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì„¸ìš”
y = # ???

# TODO: ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”
# ???

# TODO: xì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì¶œë ¥í•˜ì„¸ìš”
print(f"dy/dx at x=3: {???}")

# 2. ë” ë³µì¡í•œ í•¨ìˆ˜
# TODO: ë‘ ë³€ìˆ˜ í•¨ìˆ˜ z = x^2 + y^2ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì„¸ìš”
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)
z = # ???

# ???

print(f"dz/dx: {???}")
print(f"dz/dy: {???}")
"""
        elif topic_id == "nn_module":
            practice_template += """# 1. ê°„ë‹¨í•œ ì‹ ê²½ë§ ë§Œë“¤ê¸°
# TODO: 2ì¸µ ì‹ ê²½ë§ì„ ì™„ì„±í•˜ì„¸ìš”
class TwoLayerNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(TwoLayerNet, self).__init__()
        # TODO: ì²« ë²ˆì§¸ Linear ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ì„¸ìš”
        self.layer1 = # ???
        # TODO: ë‘ ë²ˆì§¸ Linear ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ì„¸ìš”
        self.layer2 = # ???
        
    def forward(self, x):
        # TODO: ì²« ë²ˆì§¸ ë ˆì´ì–´ í†µê³¼ í›„ ReLU ì ìš©
        x = # ???
        # TODO: ë‘ ë²ˆì§¸ ë ˆì´ì–´ í†µê³¼
        x = # ???
        return x

# TODO: ì…ë ¥ í¬ê¸° 10, ì€ë‹‰ì¸µ í¬ê¸° 20, ì¶œë ¥ í¬ê¸° 5ì¸ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”
model = # ???

# ëª¨ë¸ í…ŒìŠ¤íŠ¸
test_input = torch.randn(1, 10)
output = model(test_input)
print(f"Model output shape: {output.shape}")
"""
        
        return practice_template
    
    def check_understanding(self, topic_id: str, user_code: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìì˜ ì´í•´ë„ë¥¼ ì²´í¬í•˜ê³  í”¼ë“œë°± ì œê³µ"""
        feedback = {
            "correct": False,
            "hints": [],
            "suggestions": []
        }
        
        # ê¸°ë³¸ì ì¸ êµ¬ë¬¸ ì²´í¬
        required_elements = {
            "tensor_basics": ["torch.zeros", "torch.ones", "torch.tensor"],
            "autograd_basics": ["requires_grad=True", "backward()"],
            "nn_module": ["nn.Module", "super()", "forward"]
        }
        
        if topic_id in required_elements:
            for element in required_elements[topic_id]:
                if element in user_code:
                    feedback["correct"] = True
                else:
                    feedback["hints"].append(f"'{element}'ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
        
        # ì£¼ì œë³„ ì¶”ê°€ í”¼ë“œë°±
        if topic_id == "tensor_basics":
            if "torch.zeros((3, 4))" in user_code or "torch.zeros(3, 4)" in user_code:
                feedback["suggestions"].append("ì¢‹ìŠµë‹ˆë‹¤! torch.zerosë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.")
            
        elif topic_id == "autograd_basics":
            if "x.grad" in user_code:
                feedback["suggestions"].append("ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒ ì˜í–ˆìŠµë‹ˆë‹¤!")
            if "zero_grad()" not in user_code:
                feedback["hints"].append("ì‹¤ì œ í•™ìŠµì—ì„œëŠ” optimizer.zero_grad()ë„ ì¤‘ìš”í•©ë‹ˆë‹¤")
                
        return feedback
    
    def get_learning_path(self, skill_level: str = "beginner") -> List[str]:
        """ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” í•™ìŠµ ê²½ë¡œ ì œê³µ"""
        if skill_level == "beginner":
            return [
                "pytorch_intro",
                "tensor_basics",
                "tensor_operations",
                "autograd_basics",
                "nn_module",
                "loss_functions",
                "optimizers",
                "training_loop"
            ]
        elif skill_level == "intermediate":
            return [
                "tensor_indexing",
                "computational_graph",
                "common_layers",
                "training_loop"
            ]
        else:  # advanced
            return [
                "computational_graph",
                "common_layers",
                "training_loop"
            ]
    
    def search_topic(self, query: str) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¡œ ê´€ë ¨ ì£¼ì œ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        for category, content in self.curriculum.items():
            for topic in content.get("topics", []):
                # ì œëª©, ì„¤ëª…, ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                if (query_lower in topic["title"].lower() or
                    query_lower in topic["content"].get("ì„¤ëª…", "").lower() or
                    any(query_lower in str(v).lower() for v in topic["content"].values())):
                    
                    results.append({
                        "topic_id": topic["id"],
                        "title": topic["title"],
                        "category": category,
                        "category_title": content["title"]
                    })
        
        return results
    
    def format_response(self, topic_id: str, style: str = "detailed") -> str:
        """ì£¼ì œë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        topic_info = self.get_topic_explanation(topic_id)
        if not topic_info:
            return "í•´ë‹¹ ì£¼ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        response = f"ğŸ“š **{topic_info['title']}**\n\n"
        
        if style == "detailed":
            content = topic_info["content"]
            
            # ì„¤ëª…
            if "ì„¤ëª…" in content:
                response += f"**ğŸ“ ì„¤ëª…**\n{content['ì„¤ëª…']}\n\n"
            
            # íŠ¹ì§•ì´ë‚˜ ì£¼ìš” ê°œë…
            for key in ["íŠ¹ì§•", "í•µì‹¬ê°œë…", "ì£¼ìš”ê°œë…", "ì—°ì‚°_ì¢…ë¥˜", "ë ˆì´ì–´_ì¢…ë¥˜", "ì†ì‹¤í•¨ìˆ˜", "ì˜µí‹°ë§ˆì´ì €_ì¢…ë¥˜"]:
                if key in content:
                    response += f"**âœ¨ {key.replace('_', ' ').title()}**\n"
                    if isinstance(content[key], list):
                        for item in content[key]:
                            response += f"  â€¢ {item}\n"
                    elif isinstance(content[key], dict):
                        for k, v in content[key].items():
                            response += f"  â€¢ {k}: {v}\n"
                    response += "\n"
            
            # ì˜ˆì œ ì½”ë“œ
            if "ì˜ˆì œ" in content:
                response += f"**ğŸ’» ì˜ˆì œ ì½”ë“œ**\n```python\n{content['ì˜ˆì œ'].strip()}\n```\n\n"
            
            # ë‹¤ìŒ í•™ìŠµ ì£¼ì œ ì¶”ì²œ
            next_topics = self.suggest_next_topic(topic_id)
            if next_topics:
                response += "**ğŸ¯ ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ**\n"
                for next_id in next_topics:
                    next_info = self.get_topic_explanation(next_id)
                    if next_info:
                        response += f"  â€¢ {next_info['title']} (`{next_id}`)\n"
        
        elif style == "summary":
            response += topic_info["content"].get("ì„¤ëª…", "")
            
        return response