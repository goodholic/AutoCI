#!/usr/bin/env python3
"""
Failure Tracking and Learning System
ì‹¤íŒ¨ë¥¼ ì¶”ì í•˜ê³  í•™ìŠµí•˜ì—¬ ë¯¸ë˜ì˜ ê°œë°œì„ ê°œì„ í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import json
import os
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from enum import Enum, auto
import logging
import hashlib
import re
from collections import defaultdict, Counter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FailureType(Enum):
    """ì‹¤íŒ¨ ìœ í˜•"""
    SYNTAX_ERROR = "syntax_error"
    RUNTIME_ERROR = "runtime_error"
    LOGIC_ERROR = "logic_error"
    RESOURCE_MISSING = "resource_missing"
    PERFORMANCE_ISSUE = "performance_issue"
    COMPATIBILITY_ISSUE = "compatibility_issue"
    BUILD_FAILURE = "build_failure"
    TEST_FAILURE = "test_failure"
    INTEGRATION_ERROR = "integration_error"
    DEPENDENCY_ERROR = "dependency_error"

class FailureSeverity(Enum):
    """ì‹¤íŒ¨ ì‹¬ê°ë„"""
    CRITICAL = 5  # í”„ë¡œê·¸ë¨ ì‹¤í–‰ ë¶ˆê°€
    HIGH = 4      # ì£¼ìš” ê¸°ëŠ¥ ì‘ë™ ì•ˆí•¨
    MEDIUM = 3    # ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ
    LOW = 2       # ì‚¬ì†Œí•œ ë¬¸ì œ
    INFO = 1      # ì •ë³´ì„± ê²½ê³ 

class FailureTrackingSystem:
    """ì‹¤íŒ¨ ì¶”ì  ë° í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.failure_db_path = self.project_root / "knowledge_base" / "failure_tracking.db"
        self.failure_db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.failure_patterns = {}
        self.solution_database = {}
        self.learning_insights = {}
        
        # í†µê³„
        self.stats = {
            "total_failures": 0,
            "resolved_failures": 0,
            "recurring_failures": 0,
            "prevention_success": 0
        }
        
        # ì‹¤íŒ¨ í•´ê²° ì „ëµ
        self.resolution_strategies = {
            FailureType.SYNTAX_ERROR: self._resolve_syntax_error,
            FailureType.RUNTIME_ERROR: self._resolve_runtime_error,
            FailureType.RESOURCE_MISSING: self._resolve_resource_missing,
            FailureType.PERFORMANCE_ISSUE: self._resolve_performance_issue,
            FailureType.DEPENDENCY_ERROR: self._resolve_dependency_error
        }
        
        # íŒ¨í„´ ë§¤ì¹­ ê·œì¹™
        self.pattern_rules = self._load_pattern_rules()
        
        # AI ëª¨ë¸ ì—°ë™
        self.ai_model = None
        try:
            from modules.ai_model_integration import get_ai_integration
            self.ai_model = get_ai_integration()
        except ImportError:
            logger.warning("AI ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.failure_db_path)
        cursor = conn.cursor()
        
        # ì‹¤íŒ¨ ê¸°ë¡ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS failures (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                project_name TEXT NOT NULL,
                failure_type TEXT NOT NULL,
                severity INTEGER NOT NULL,
                error_message TEXT,
                stack_trace TEXT,
                context TEXT,
                file_path TEXT,
                line_number INTEGER,
                code_snippet TEXT,
                resolved BOOLEAN DEFAULT FALSE,
                resolution_method TEXT,
                resolution_time REAL,
                prevention_applied BOOLEAN DEFAULT FALSE,
                recurrence_count INTEGER DEFAULT 0,
                pattern_hash TEXT
            )
        """)
        
        # í•´ê²°ì±… í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS solutions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                failure_pattern TEXT NOT NULL,
                solution_description TEXT NOT NULL,
                solution_code TEXT,
                success_rate REAL DEFAULT 0.0,
                usage_count INTEGER DEFAULT 0,
                last_used TEXT,
                tags TEXT
            )
        """)
        
        # í•™ìŠµ ì¸ì‚¬ì´íŠ¸ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS learning_insights (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                insight_type TEXT NOT NULL,
                description TEXT NOT NULL,
                related_failures TEXT,
                prevention_strategy TEXT,
                effectiveness_score REAL DEFAULT 0.0
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_failure_type ON failures(failure_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_project ON failures(project_name)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_pattern ON failures(pattern_hash)")
        
        conn.commit()
        conn.close()
    
    def _load_pattern_rules(self) -> Dict[str, Dict[str, Any]]:
        """íŒ¨í„´ ë§¤ì¹­ ê·œì¹™ ë¡œë“œ"""
        return {
            "import_error": {
                "pattern": r"ImportError: .*'(\w+)'",
                "type": FailureType.DEPENDENCY_ERROR,
                "solution": "Install missing module or check import path"
            },
            "attribute_error": {
                "pattern": r"AttributeError: '(\w+)' object has no attribute '(\w+)'",
                "type": FailureType.RUNTIME_ERROR,
                "solution": "Check object type and available attributes"
            },
            "type_error": {
                "pattern": r"TypeError: .* expected (\w+), got (\w+)",
                "type": FailureType.RUNTIME_ERROR,
                "solution": "Fix type mismatch"
            },
            "godot_signal_error": {
                "pattern": r"Signal '(\w+)' is already connected",
                "type": FailureType.RUNTIME_ERROR,
                "solution": "Check for duplicate signal connections"
            },
            "godot_node_error": {
                "pattern": r"Node not found: (.+)",
                "type": FailureType.RUNTIME_ERROR,
                "solution": "Verify node path and scene structure"
            },
            "resource_not_found": {
                "pattern": r"Failed to load resource: (.+)",
                "type": FailureType.RESOURCE_MISSING,
                "solution": "Check resource path or create missing resource"
            }
        }
    
    async def track_failure(
        self,
        error: Exception,
        context: Dict[str, Any],
        project_name: str,
        file_path: Optional[str] = None,
        line_number: Optional[int] = None
    ) -> int:
        """ì‹¤íŒ¨ ì¶”ì """
        # ì‹¤íŒ¨ ìœ í˜• íŒë³„
        failure_type = self._classify_failure(error, context)
        severity = self._assess_severity(error, failure_type)
        
        # íŒ¨í„´ í•´ì‹œ ìƒì„±
        pattern_hash = self._generate_pattern_hash(error, failure_type)
        
        # ì½”ë“œ ìŠ¤ë‹ˆí« ì¶”ì¶œ
        code_snippet = None
        if file_path and line_number:
            code_snippet = self._extract_code_snippet(file_path, line_number)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
        conn = sqlite3.connect(self.failure_db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO failures (
                timestamp, project_name, failure_type, severity,
                error_message, stack_trace, context, file_path,
                line_number, code_snippet, pattern_hash
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            project_name,
            failure_type.value,
            severity.value,
            str(error),
            self._get_stack_trace(error),
            json.dumps(context),
            file_path,
            line_number,
            code_snippet,
            pattern_hash
        ))
        
        failure_id = cursor.lastrowid
        
        # ì¬ë°œ ì²´í¬
        cursor.execute("""
            UPDATE failures 
            SET recurrence_count = recurrence_count + 1 
            WHERE pattern_hash = ? AND id != ?
        """, (pattern_hash, failure_id))
        
        conn.commit()
        conn.close()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["total_failures"] += 1
        
        # ìë™ í•´ê²° ì‹œë„
        if severity.value >= FailureSeverity.MEDIUM.value:
            await self._attempt_auto_resolution(failure_id, error, failure_type, context)
        
        # í•™ìŠµ
        await self._learn_from_failure(failure_id, error, failure_type, context)
        
        logger.info(f"âœ… ì‹¤íŒ¨ ì¶”ì  ì™„ë£Œ: ID={failure_id}, Type={failure_type.value}, Severity={severity.value}")
        
        return failure_id
    
    def _classify_failure(self, error: Exception, context: Dict[str, Any]) -> FailureType:
        """ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜"""
        error_str = str(error).lower()
        error_type = type(error).__name__
        
        # ì—ëŸ¬ ë©”ì‹œì§€ ê¸°ë°˜ ë¶„ë¥˜
        if "syntax" in error_str or error_type == "SyntaxError":
            return FailureType.SYNTAX_ERROR
        elif "import" in error_str or error_type == "ImportError":
            return FailureType.DEPENDENCY_ERROR
        elif "resource" in error_str or "file not found" in error_str:
            return FailureType.RESOURCE_MISSING
        elif "performance" in error_str or "timeout" in error_str:
            return FailureType.PERFORMANCE_ISSUE
        elif "build" in error_str or "compile" in error_str:
            return FailureType.BUILD_FAILURE
        elif "test" in error_str or "assert" in error_str:
            return FailureType.TEST_FAILURE
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜
        if context.get("phase") == "integration":
            return FailureType.INTEGRATION_ERROR
        elif context.get("phase") == "logic":
            return FailureType.LOGIC_ERROR
        
        # ê¸°ë³¸ê°’
        return FailureType.RUNTIME_ERROR
    
    def _assess_severity(self, error: Exception, failure_type: FailureType) -> FailureSeverity:
        """ì‹¤íŒ¨ ì‹¬ê°ë„ í‰ê°€"""
        # ì¹˜ëª…ì  ì˜¤ë¥˜
        if failure_type in [FailureType.BUILD_FAILURE, FailureType.SYNTAX_ERROR]:
            return FailureSeverity.CRITICAL
        
        # ë†’ì€ ì‹¬ê°ë„
        if failure_type in [FailureType.RUNTIME_ERROR, FailureType.DEPENDENCY_ERROR]:
            return FailureSeverity.HIGH
        
        # ì¤‘ê°„ ì‹¬ê°ë„
        if failure_type in [FailureType.LOGIC_ERROR, FailureType.INTEGRATION_ERROR]:
            return FailureSeverity.MEDIUM
        
        # ë‚®ì€ ì‹¬ê°ë„
        if failure_type in [FailureType.PERFORMANCE_ISSUE, FailureType.TEST_FAILURE]:
            return FailureSeverity.LOW
        
        # ì •ë³´ì„±
        return FailureSeverity.INFO
    
    def _generate_pattern_hash(self, error: Exception, failure_type: FailureType) -> str:
        """íŒ¨í„´ í•´ì‹œ ìƒì„±"""
        # ì—ëŸ¬ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ í•´ì‹œ ìƒì„±
        pattern_string = f"{failure_type.value}:{type(error).__name__}"
        
        # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ë³€ìˆ˜ ë¶€ë¶„ ì œê±°
        error_msg = str(error)
        # ìˆ«ì, ê²½ë¡œ, ë³€ìˆ˜ëª… ë“±ì„ ì¼ë°˜í™”
        error_msg = re.sub(r'\d+', 'NUM', error_msg)
        error_msg = re.sub(r'/[^\s]+', 'PATH', error_msg)
        error_msg = re.sub(r"'[^']+'", 'VAR', error_msg)
        
        pattern_string += f":{error_msg}"
        
        return hashlib.md5(pattern_string.encode()).hexdigest()
    
    def _extract_code_snippet(self, file_path: str, line_number: int, context_lines: int = 5) -> str:
        """ì½”ë“œ ìŠ¤ë‹ˆí« ì¶”ì¶œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            start = max(0, line_number - context_lines - 1)
            end = min(len(lines), line_number + context_lines)
            
            snippet_lines = []
            for i in range(start, end):
                prefix = ">>> " if i == line_number - 1 else "    "
                snippet_lines.append(f"{i+1:4d} {prefix}{lines[i].rstrip()}")
            
            return "\n".join(snippet_lines)
        except:
            return ""
    
    def _get_stack_trace(self, error: Exception) -> str:
        """ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶”ì¶œ"""
        import traceback
        return traceback.format_exc()
    
    async def _attempt_auto_resolution(
        self,
        failure_id: int,
        error: Exception,
        failure_type: FailureType,
        context: Dict[str, Any]
    ):
        """ìë™ í•´ê²° ì‹œë„"""
        logger.info(f"ğŸ”§ ìë™ í•´ê²° ì‹œë„: {failure_type.value}")
        
        # í•´ê²° ì „ëµ ì„ íƒ
        if failure_type in self.resolution_strategies:
            strategy = self.resolution_strategies[failure_type]
            resolution_start = datetime.now()
            
            try:
                # í•´ê²° ì‹œë„
                success, resolution_method = await strategy(error, context)
                
                if success:
                    resolution_time = (datetime.now() - resolution_start).total_seconds()
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
                    conn = sqlite3.connect(self.failure_db_path)
                    cursor = conn.cursor()
                    
                    cursor.execute("""
                        UPDATE failures 
                        SET resolved = TRUE, 
                            resolution_method = ?,
                            resolution_time = ?
                        WHERE id = ?
                    """, (resolution_method, resolution_time, failure_id))
                    
                    conn.commit()
                    conn.close()
                    
                    self.stats["resolved_failures"] += 1
                    logger.info(f"âœ… ìë™ í•´ê²° ì„±ê³µ: {resolution_method}")
                else:
                    logger.warning("âŒ ìë™ í•´ê²° ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.error(f"ìë™ í•´ê²° ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _resolve_syntax_error(self, error: Exception, context: Dict[str, Any]) -> Tuple[bool, str]:
        """êµ¬ë¬¸ ì˜¤ë¥˜ í•´ê²°"""
        file_path = context.get("file_path")
        if not file_path:
            return False, "No file path provided"
        
        try:
            # AI ëª¨ë¸ ì‚¬ìš©
            if self.ai_model:
                with open(file_path, 'r', encoding='utf-8') as f:
                    code = f.read()
                
                prompt = f"""
                ë‹¤ìŒ ì½”ë“œì— êµ¬ë¬¸ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤:
                
                ì˜¤ë¥˜: {str(error)}
                
                ì½”ë“œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”. ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
                
                ì›ë³¸ ì½”ë“œ:
                {code}
                """
                
                fixed_code = await self.ai_model.generate_response(prompt)
                
                if fixed_code and "def" in fixed_code or "func" in fixed_code:
                    # ë°±ì—… ìƒì„±
                    backup_path = f"{file_path}.backup"
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        f.write(code)
                    
                    # ìˆ˜ì •ëœ ì½”ë“œ ì €ì¥
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(fixed_code)
                    
                    return True, "AI-assisted syntax fix"
            
            # ê°„ë‹¨í•œ ìë™ ìˆ˜ì • ì‹œë„
            # ì˜ˆ: ëˆ„ë½ëœ ì½œë¡ , ê´„í˜¸ ë“±
            return False, "Manual fix required"
            
        except Exception as e:
            logger.error(f"êµ¬ë¬¸ ì˜¤ë¥˜ í•´ê²° ì‹¤íŒ¨: {e}")
            return False, f"Resolution failed: {str(e)}"
    
    async def _resolve_runtime_error(self, error: Exception, context: Dict[str, Any]) -> Tuple[bool, str]:
        """ëŸ°íƒ€ì„ ì˜¤ë¥˜ í•´ê²°"""
        error_str = str(error)
        
        # ì¼ë°˜ì ì¸ ëŸ°íƒ€ì„ ì˜¤ë¥˜ íŒ¨í„´ ì²˜ë¦¬
        if "division by zero" in error_str:
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ ì½”ë“œ ì¶”ê°€
            return await self._add_zero_division_check(context)
        elif "list index out of range" in error_str:
            # ë°°ì—´ ë²”ìœ„ ì²´í¬ ì¶”ê°€
            return await self._add_bounds_check(context)
        elif "NoneType" in error_str:
            # None ì²´í¬ ì¶”ê°€
            return await self._add_none_check(context)
        
        return False, "Complex runtime error - manual intervention needed"
    
    async def _resolve_resource_missing(self, error: Exception, context: Dict[str, Any]) -> Tuple[bool, str]:
        """ë¦¬ì†ŒìŠ¤ ëˆ„ë½ í•´ê²°"""
        resource_path = context.get("resource_path")
        if not resource_path:
            # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ë¦¬ì†ŒìŠ¤ ê²½ë¡œ ì¶”ì¶œ ì‹œë„
            match = re.search(r'res://([^\s"\']+)', str(error))
            if match:
                resource_path = match.group(0)
        
        if resource_path:
            try:
                from modules.auto_resource_generator import get_resource_generator
                generator = get_resource_generator()
                
                project_path = Path(context.get("project_path", "."))
                if await generator.generate_missing_resource(resource_path, project_path):
                    return True, f"Auto-generated resource: {resource_path}"
            except:
                pass
        
        return False, "Could not generate missing resource"
    
    async def _resolve_performance_issue(self, error: Exception, context: Dict[str, Any]) -> Tuple[bool, str]:
        """ì„±ëŠ¥ ë¬¸ì œ í•´ê²°"""
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° ìµœì í™”
        optimization_applied = []
        
        # 1. ìºì‹± ì¶”ê°€
        if "repeated_calculation" in context:
            optimization_applied.append("Added caching")
        
        # 2. ì•Œê³ ë¦¬ì¦˜ ìµœì í™”
        if "inefficient_loop" in context:
            optimization_applied.append("Optimized loop")
        
        # 3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìµœì í™”
        if "memory_intensive" in context:
            optimization_applied.append("Reduced memory usage")
        
        if optimization_applied:
            return True, f"Applied optimizations: {', '.join(optimization_applied)}"
        
        return False, "Performance optimization requires manual analysis"
    
    async def _resolve_dependency_error(self, error: Exception, context: Dict[str, Any]) -> Tuple[bool, str]:
        """ì˜ì¡´ì„± ì˜¤ë¥˜ í•´ê²°"""
        error_str = str(error)
        
        # ëˆ„ë½ëœ ëª¨ë“ˆ ì°¾ê¸°
        match = re.search(r"No module named '(\w+)'", error_str)
        if match:
            module_name = match.group(1)
            
            # ì¼ë°˜ì ì¸ ëŒ€ì²´ ëª¨ë“ˆ ë§¤í•‘
            alternatives = {
                "cv2": "opencv-python",
                "sklearn": "scikit-learn",
                "np": "numpy",
                "pd": "pandas"
            }
            
            install_name = alternatives.get(module_name, module_name)
            
            # pip install ì‹œë„
            try:
                import subprocess
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", install_name],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode == 0:
                    return True, f"Installed missing module: {install_name}"
            except:
                pass
        
        return False, "Could not resolve dependency"
    
    async def _add_zero_division_check(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì²´í¬ ì¶”ê°€"""
        file_path = context.get("file_path")
        line_number = context.get("line_number")
        
        if file_path and line_number:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # ë‚˜ëˆ„ê¸° ì—°ì‚° ì°¾ê¸°
                if line_number <= len(lines):
                    line = lines[line_number - 1]
                    if "/" in line:
                        # ê°„ë‹¨í•œ ì²´í¬ ì¶”ê°€
                        indent = len(line) - len(line.lstrip())
                        check_line = " " * indent + "if denominator != 0:  # Added by failure tracking\n"
                        lines.insert(line_number - 1, check_line)
                        lines[line_number] = " " * (indent + 4) + lines[line_number].lstrip()
                        
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.writelines(lines)
                        
                        return True, "Added zero division check"
            except:
                pass
        
        return False, "Could not add zero division check"
    
    async def _add_bounds_check(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """ë°°ì—´ ë²”ìœ„ ì²´í¬ ì¶”ê°€"""
        # êµ¬í˜„ ìƒëµ - ì‹¤ì œë¡œëŠ” ì½”ë“œ ë¶„ì„ í›„ ì ì ˆí•œ ì²´í¬ ì¶”ê°€
        return False, "Bounds check requires manual implementation"
    
    async def _add_none_check(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """None ì²´í¬ ì¶”ê°€"""
        # êµ¬í˜„ ìƒëµ - ì‹¤ì œë¡œëŠ” ì½”ë“œ ë¶„ì„ í›„ ì ì ˆí•œ ì²´í¬ ì¶”ê°€
        return False, "None check requires manual implementation"
    
    async def _learn_from_failure(
        self,
        failure_id: int,
        error: Exception,
        failure_type: FailureType,
        context: Dict[str, Any]
    ):
        """ì‹¤íŒ¨ë¡œë¶€í„° í•™ìŠµ"""
        # íŒ¨í„´ ë¶„ì„
        pattern_hash = self._generate_pattern_hash(error, failure_type)
        
        # ìœ ì‚¬í•œ ì‹¤íŒ¨ ì°¾ê¸°
        conn = sqlite3.connect(self.failure_db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT COUNT(*), AVG(resolution_time), GROUP_CONCAT(resolution_method)
            FROM failures
            WHERE pattern_hash = ? AND resolved = TRUE
        """, (pattern_hash,))
        
        count, avg_resolution_time, resolution_methods = cursor.fetchone()
        
        if count > 0:
            # ì„±ê³µì ì¸ í•´ê²° ë°©ë²• í•™ìŠµ
            insight = {
                "pattern": pattern_hash,
                "failure_type": failure_type.value,
                "successful_resolutions": count,
                "avg_resolution_time": avg_resolution_time,
                "methods": resolution_methods.split(",") if resolution_methods else []
            }
            
            self.learning_insights[pattern_hash] = insight
            
            # ì¸ì‚¬ì´íŠ¸ ì €ì¥
            cursor.execute("""
                INSERT INTO learning_insights (
                    timestamp, insight_type, description,
                    related_failures, prevention_strategy, effectiveness_score
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                "pattern_recognition",
                f"Learned pattern for {failure_type.value}",
                pattern_hash,
                json.dumps(insight),
                count / (count + 1)  # íš¨ê³¼ì„± ì ìˆ˜
            ))
        
        conn.commit()
        conn.close()
    
    async def get_failure_report(self, project_name: Optional[str] = None) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ë³´ê³ ì„œ ìƒì„±"""
        conn = sqlite3.connect(self.failure_db_path)
        cursor = conn.cursor()
        
        # í”„ë¡œì íŠ¸ë³„ í•„í„°
        where_clause = "WHERE project_name = ?" if project_name else ""
        params = (project_name,) if project_name else ()
        
        # ì „ì²´ í†µê³„
        cursor.execute(f"""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN resolved = TRUE THEN 1 ELSE 0 END) as resolved,
                AVG(resolution_time) as avg_resolution_time,
                MAX(recurrence_count) as max_recurrence
            FROM failures {where_clause}
        """, params)
        
        stats = cursor.fetchone()
        
        # ìœ í˜•ë³„ ë¶„í¬
        cursor.execute(f"""
            SELECT failure_type, COUNT(*) as count
            FROM failures {where_clause}
            GROUP BY failure_type
            ORDER BY count DESC
        """, params)
        
        type_distribution = cursor.fetchall()
        
        # ì‹¬ê°ë„ë³„ ë¶„í¬
        cursor.execute(f"""
            SELECT severity, COUNT(*) as count
            FROM failures {where_clause}
            GROUP BY severity
            ORDER BY severity DESC
        """, params)
        
        severity_distribution = cursor.fetchall()
        
        # ìµœê·¼ ì‹¤íŒ¨
        cursor.execute(f"""
            SELECT timestamp, failure_type, error_message, resolved
            FROM failures {where_clause}
            ORDER BY timestamp DESC
            LIMIT 10
        """, params)
        
        recent_failures = cursor.fetchall()
        
        # í•™ìŠµ ì¸ì‚¬ì´íŠ¸
        cursor.execute("""
            SELECT description, effectiveness_score
            FROM learning_insights
            ORDER BY effectiveness_score DESC
            LIMIT 5
        """)
        
        top_insights = cursor.fetchall()
        
        conn.close()
        
        return {
            "statistics": {
                "total_failures": stats[0] or 0,
                "resolved_failures": stats[1] or 0,
                "resolution_rate": (stats[1] / stats[0] * 100) if stats[0] else 0,
                "avg_resolution_time": stats[2] or 0,
                "max_recurrence": stats[3] or 0
            },
            "type_distribution": [
                {"type": t, "count": c} for t, c in type_distribution
            ],
            "severity_distribution": [
                {"severity": s, "count": c} for s, c in severity_distribution
            ],
            "recent_failures": [
                {
                    "timestamp": f[0],
                    "type": f[1],
                    "error": f[2],
                    "resolved": bool(f[3])
                } for f in recent_failures
            ],
            "top_insights": [
                {"insight": i[0], "effectiveness": i[1]} for i in top_insights
            ]
        }
    
    async def suggest_preventive_measures(
        self,
        project_path: Path
    ) -> List[Dict[str, Any]]:
        """ì˜ˆë°© ì¡°ì¹˜ ì œì•ˆ"""
        suggestions = []
        
        # ë°˜ë³µì ì¸ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        conn = sqlite3.connect(self.failure_db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT pattern_hash, failure_type, COUNT(*) as count, 
                   MAX(error_message) as sample_error
            FROM failures
            WHERE recurrence_count > 2
            GROUP BY pattern_hash
            ORDER BY count DESC
            LIMIT 10
        """)
        
        recurring_patterns = cursor.fetchall()
        
        for pattern in recurring_patterns:
            pattern_hash, failure_type, count, sample_error = pattern
            
            # ì˜ˆë°© ì „ëµ ìƒì„±
            prevention = self._generate_prevention_strategy(
                failure_type, sample_error, count
            )
            
            suggestions.append({
                "pattern": pattern_hash,
                "type": failure_type,
                "occurrences": count,
                "prevention": prevention,
                "priority": "high" if count > 5 else "medium"
            })
        
        conn.close()
        
        # ì½”ë“œ í’ˆì§ˆ ê°œì„  ì œì•ˆ
        quality_suggestions = await self._analyze_code_quality_issues(project_path)
        suggestions.extend(quality_suggestions)
        
        return suggestions
    
    def _generate_prevention_strategy(
        self,
        failure_type: str,
        sample_error: str,
        occurrence_count: int
    ) -> Dict[str, Any]:
        """ì˜ˆë°© ì „ëµ ìƒì„±"""
        strategies = {
            FailureType.SYNTAX_ERROR.value: {
                "action": "Add linting and syntax checking",
                "tools": ["pylint", "flake8", "black"],
                "automation": "Pre-commit hooks"
            },
            FailureType.RUNTIME_ERROR.value: {
                "action": "Add comprehensive error handling",
                "patterns": ["try-except blocks", "input validation", "type checking"],
                "testing": "Unit tests for edge cases"
            },
            FailureType.RESOURCE_MISSING.value: {
                "action": "Resource validation on startup",
                "checks": ["File existence", "Path validation", "Fallback resources"],
                "tools": ["Resource manifest", "Asset pipeline"]
            },
            FailureType.DEPENDENCY_ERROR.value: {
                "action": "Dependency management",
                "tools": ["requirements.txt", "virtual environments", "dependency lock files"],
                "validation": "Import checks on startup"
            },
            FailureType.PERFORMANCE_ISSUE.value: {
                "action": "Performance monitoring",
                "techniques": ["Profiling", "Benchmarking", "Resource limits"],
                "optimization": ["Caching", "Algorithm optimization", "Lazy loading"]
            }
        }
        
        base_strategy = strategies.get(failure_type, {
            "action": "General quality improvement",
            "recommendation": "Code review and testing"
        })
        
        # ë°œìƒ ë¹ˆë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ì¡°ì •
        base_strategy["urgency"] = "critical" if occurrence_count > 10 else "important"
        base_strategy["estimated_impact"] = f"Prevent ~{occurrence_count} failures/month"
        
        return base_strategy
    
    async def _analyze_code_quality_issues(self, project_path: Path) -> List[Dict[str, Any]]:
        """ì½”ë“œ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„"""
        suggestions = []
        
        # ê¸°ë³¸ ì½”ë“œ í’ˆì§ˆ ì²´í¬
        quality_checks = [
            {
                "check": "error_handling",
                "description": "Add error handling to critical functions",
                "pattern": r"def\s+\w+.*:\s*\n(?!.*try:)",
                "priority": "high"
            },
            {
                "check": "input_validation",
                "description": "Validate user inputs",
                "pattern": r"input\(|get_node\(|get\(",
                "priority": "medium"
            },
            {
                "check": "resource_checks",
                "description": "Check resource existence before use",
                "pattern": r"load\(|preload\(|open\(",
                "priority": "medium"
            }
        ]
        
        for check in quality_checks:
            matches = 0
            for file_path in project_path.rglob("*.gd"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if re.search(check["pattern"], content):
                            matches += 1
                except:
                    pass
            
            if matches > 0:
                suggestions.append({
                    "type": "code_quality",
                    "check": check["check"],
                    "description": check["description"],
                    "affected_files": matches,
                    "priority": check["priority"]
                })
        
        return suggestions
    
    def export_knowledge_base(self, output_path: Path):
        """ì§€ì‹ ë² ì´ìŠ¤ ë‚´ë³´ë‚´ê¸°"""
        conn = sqlite3.connect(self.failure_db_path)
        
        # ëª¨ë“  ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        knowledge = {
            "export_date": datetime.now().isoformat(),
            "statistics": self.stats,
            "failures": [],
            "solutions": [],
            "insights": []
        }
        
        # ì‹¤íŒ¨ ë°ì´í„°
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM failures")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge["failures"].append(dict(zip(columns, row)))
        
        # í•´ê²°ì±… ë°ì´í„°
        cursor.execute("SELECT * FROM solutions")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge["solutions"].append(dict(zip(columns, row)))
        
        # ì¸ì‚¬ì´íŠ¸ ë°ì´í„°
        cursor.execute("SELECT * FROM learning_insights")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge["insights"].append(dict(zip(columns, row)))
        
        conn.close()
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_failure_tracker = None

def get_failure_tracker() -> FailureTrackingSystem:
    """ì‹¤íŒ¨ ì¶”ì  ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _failure_tracker
    if _failure_tracker is None:
        _failure_tracker = FailureTrackingSystem()
    return _failure_tracker


# í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ
async def test_failure_tracking():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    tracker = get_failure_tracker()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì¶”ì 
    try:
        # ì˜ë„ì ì¸ ì˜¤ë¥˜ ë°œìƒ
        result = 10 / 0
    except Exception as e:
        failure_id = await tracker.track_failure(
            error=e,
            context={
                "operation": "division",
                "phase": "calculation",
                "values": {"numerator": 10, "denominator": 0}
            },
            project_name="TestProject",
            file_path=__file__,
            line_number=783
        )
        
        print(f"Tracked failure: ID={failure_id}")
    
    # ë³´ê³ ì„œ ìƒì„±
    report = await tracker.get_failure_report("TestProject")
    print("\nFailure Report:")
    print(json.dumps(report, indent=2))
    
    # ì˜ˆë°© ì¡°ì¹˜ ì œì•ˆ
    suggestions = await tracker.suggest_preventive_measures(Path("."))
    print("\nPreventive Measures:")
    for suggestion in suggestions:
        print(f"- {suggestion['description']} (Priority: {suggestion['priority']})")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_failure_tracking())