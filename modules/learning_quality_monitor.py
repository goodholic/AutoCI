#!/usr/bin/env python3
"""
ÌïôÏäµ ÌíàÏßà Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏä§ÌÖú
Í≥ºÏ†ÅÌï© Î∞©ÏßÄ, ÌïôÏäµ ÏßÑÌñâ Ï∂îÏ†Å, ÌíàÏßà ÌèâÍ∞ÄÎ•º Îã¥Îãπ
"""

import os
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import logging
try:
    import matplotlib
    matplotlib.use('Agg')  # ÎπÑÎåÄÌôîÌòï Î∞±ÏóîÎìú ÏÇ¨Ïö©
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    import logging
    logging.getLogger(__name__).warning("MatplotlibÏù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÌîåÎ°Ø ÏÉùÏÑ±Ïù¥ ÎπÑÌôúÏÑ±ÌôîÎê©ÎãàÎã§.")
from collections import deque

logger = logging.getLogger(__name__)

@dataclass
class LearningMetrics:
    """ÌïôÏäµ Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞"""
    timestamp: str
    epoch: int
    iteration: int
    train_loss: float
    validation_loss: Optional[float] = None
    learning_rate: float = 0.0
    quality_score: float = 0.0
    overfitting_score: float = 0.0  # 0-1, ÎÜíÏùÑÏàòÎ°ù Í≥ºÏ†ÅÌï©
    
    def to_dict(self) -> Dict:
        return asdict(self)

class LearningQualityMonitor:
    """ÌïôÏäµ ÌíàÏßà Î™®ÎãàÌÑ∞ÎßÅ Î∞è Í≥ºÏ†ÅÌï© Î∞©ÏßÄ ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.monitor_dir = self.project_root / "continuous_learning" / "monitoring"
        self.monitor_dir.mkdir(parents=True, exist_ok=True)
        
        # Î©îÌä∏Î¶≠ Ï†ÄÏû• Í≤ΩÎ°ú
        self.metrics_file = self.monitor_dir / "learning_metrics.jsonl"
        self.plots_dir = self.monitor_dir / "plots"
        self.plots_dir.mkdir(exist_ok=True)
        
        # Í≥ºÏ†ÅÌï© Í∞êÏßÄ ÏÑ§Ï†ï
        self.overfitting_threshold = 0.1  # validation_lossÍ∞Ä train_lossÎ≥¥Îã§ 10% Ïù¥ÏÉÅ ÎÜíÏúºÎ©¥ Í≤ΩÍ≥†
        self.early_stopping_patience = 5  # validation_lossÍ∞Ä 5Î≤à Ïó∞ÏÜç Ï¶ùÍ∞ÄÌïòÎ©¥ Ï§ëÎã®
        self.min_delta = 0.001  # ÏµúÏÜå Í∞úÏÑ† Ìè≠
        
        # Î©îÌä∏Î¶≠ ÌûàÏä§ÌÜ†Î¶¨
        self.train_loss_history = deque(maxlen=100)
        self.val_loss_history = deque(maxlen=100)
        self.quality_score_history = deque(maxlen=100)
        
        # Ï°∞Í∏∞ Ï¢ÖÎ£å Í¥ÄÎ†®
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.should_stop = False
        
        # ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï
        self.initial_learning_rate = 2e-5
        self.min_learning_rate = 1e-6
        self.lr_decay_factor = 0.95
        
        # ÌíàÏßà ÌèâÍ∞Ä Í∏∞Ï§Ä
        self.quality_criteria = {
            "coherence": 0.3,      # ÏùëÎãµÏùò ÏùºÍ¥ÄÏÑ±
            "relevance": 0.3,      # ÏßàÎ¨∏Í≥ºÏùò Í¥ÄÎ†®ÏÑ±
            "completeness": 0.2,   # ÎãµÎ≥ÄÏùò ÏôÑÏÑ±ÎèÑ
            "accuracy": 0.2        # Í∏∞Ïà†Ï†Å Ï†ïÌôïÏÑ±
        }
        
        # Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ ÏÉÅÌÉú
        self.monitoring_active = False
        self.start_time = None
        self.total_iterations = 0
        
    def start_monitoring(self, model_name: str, dataset_size: int):
        """Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë"""
        self.monitoring_active = True
        self.start_time = datetime.now()
        self.total_iterations = 0
        self.should_stop = False
        
        # Î™®ÎãàÌÑ∞ÎßÅ ÏÑ∏ÏÖò Ï†ïÎ≥¥ Ï†ÄÏû•
        session_info = {
            "session_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_name": model_name,
            "dataset_size": dataset_size,
            "start_time": self.start_time.isoformat(),
            "initial_learning_rate": self.initial_learning_rate,
            "overfitting_threshold": self.overfitting_threshold
        }
        
        session_file = self.monitor_dir / "current_session.json"
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_info, f, indent=2)
        
        logger.info(f"ÌïôÏäµ Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë: {model_name} (Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞: {dataset_size})")
    
    def update_metrics(self, epoch: int, iteration: int, 
                      train_loss: float, validation_loss: Optional[float] = None,
                      learning_rate: Optional[float] = None) -> LearningMetrics:
        """Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Í≥ºÏ†ÅÌï© Ï≤¥ÌÅ¨"""
        self.total_iterations += 1
        
        # ÌòÑÏû¨ ÌïôÏäµÎ•† (Ï†úÍ≥µÎêòÏßÄ ÏïäÏúºÎ©¥ Í≥ÑÏÇ∞)
        if learning_rate is None:
            learning_rate = self.calculate_learning_rate(epoch)
        
        # Í≥ºÏ†ÅÌï© Ï†êÏàò Í≥ÑÏÇ∞
        overfitting_score = 0.0
        if validation_loss is not None:
            overfitting_score = self.calculate_overfitting_score(train_loss, validation_loss)
            
            # Ï°∞Í∏∞ Ï¢ÖÎ£å Ï≤¥ÌÅ¨
            self.check_early_stopping(validation_loss)
        
        # ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞ (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±)
        quality_score = self.estimate_quality_score(train_loss, validation_loss)
        
        # Î©îÌä∏Î¶≠ ÏÉùÏÑ±
        metrics = LearningMetrics(
            timestamp=datetime.now().isoformat(),
            epoch=epoch,
            iteration=iteration,
            train_loss=train_loss,
            validation_loss=validation_loss,
            learning_rate=learning_rate,
            quality_score=quality_score,
            overfitting_score=overfitting_score
        )
        
        # ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏
        self.train_loss_history.append(train_loss)
        if validation_loss is not None:
            self.val_loss_history.append(validation_loss)
        self.quality_score_history.append(quality_score)
        
        # Î©îÌä∏Î¶≠ Ï†ÄÏû•
        with open(self.metrics_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(metrics.to_dict()) + '\n')
        
        # Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌîåÎ°Ø ÏÉùÏÑ± (100 iterationsÎßàÎã§)
        if self.total_iterations % 100 == 0:
            self.generate_plots()
        
        # Í≤ΩÍ≥† Î∞è Í∂åÏû•ÏÇ¨Ìï≠ Ï∂úÎ†•
        self.print_recommendations(metrics)
        
        return metrics
    
    def calculate_overfitting_score(self, train_loss: float, val_loss: float) -> float:
        """Í≥ºÏ†ÅÌï© Ï†êÏàò Í≥ÑÏÇ∞ (0-1)"""
        if train_loss == 0:
            return 0.0
        
        # validation lossÍ∞Ä train lossÎ≥¥Îã§ ÏñºÎßàÎÇò ÎÜíÏùÄÏßÄ
        gap_ratio = (val_loss - train_loss) / train_loss
        
        # 0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî
        overfitting_score = max(0.0, min(1.0, gap_ratio))
        
        return overfitting_score
    
    def check_early_stopping(self, val_loss: float) -> bool:
        """Ï°∞Í∏∞ Ï¢ÖÎ£å Ï≤¥ÌÅ¨"""
        if val_loss < self.best_val_loss - self.min_delta:
            # Í∞úÏÑ†Îê®
            self.best_val_loss = val_loss
            self.patience_counter = 0
        else:
            # Í∞úÏÑ† ÏïàÎê®
            self.patience_counter += 1
            
            if self.patience_counter >= self.early_stopping_patience:
                self.should_stop = True
                logger.warning(f"Ï°∞Í∏∞ Ï¢ÖÎ£å Ìä∏Î¶¨Í±∞Îê®! Validation lossÍ∞Ä {self.patience_counter}Î≤à Ïó∞ÏÜç Í∞úÏÑ†ÎêòÏßÄ ÏïäÏùå")
                return True
        
        return False
    
    def calculate_learning_rate(self, epoch: int) -> float:
        """ÏóêÌè¨ÌÅ¨Ïóê Îî∞Î•∏ ÌïôÏäµÎ•† Í≥ÑÏÇ∞ (Í∞êÏá† Ï†ÅÏö©)"""
        lr = self.initial_learning_rate * (self.lr_decay_factor ** epoch)
        return max(lr, self.min_learning_rate)
    
    def estimate_quality_score(self, train_loss: float, val_loss: Optional[float]) -> float:
        """ÌïôÏäµ ÌíàÏßà Ï†êÏàò Ï∂îÏ†ï (0-1)"""
        # Îã®Ïàú Ìú¥Î¶¨Ïä§Ìã±: lossÍ∞Ä ÎÇÆÏùÑÏàòÎ°ù ÎÜíÏùÄ Ï†êÏàò
        base_score = 1.0 / (1.0 + train_loss)
        
        # validation lossÍ∞Ä ÏûàÏúºÎ©¥ Í≥ºÏ†ÅÌï© ÌéòÎÑêÌã∞ Ï†ÅÏö©
        if val_loss is not None:
            overfitting_penalty = max(0, (val_loss - train_loss) / (train_loss + 1e-8))
            base_score *= (1.0 - min(overfitting_penalty, 0.5))
        
        return min(1.0, base_score)
    
    def print_recommendations(self, metrics: LearningMetrics):
        """ÌïôÏäµ ÏÉÅÌÉúÏóê Îî∞Î•∏ Í∂åÏû•ÏÇ¨Ìï≠ Ï∂úÎ†•"""
        recommendations = []
        
        # Í≥ºÏ†ÅÌï© Í≤ΩÍ≥†
        if metrics.overfitting_score > 0.3:
            recommendations.append("‚ö†Ô∏è Í≥ºÏ†ÅÌï© ÏßïÌõÑ Í∞êÏßÄ! Îã§ÏùåÏùÑ Í∂åÏû•Ìï©ÎãàÎã§:")
            recommendations.append("  - ÌïôÏäµÎ•† Í∞êÏÜå (ÌòÑÏû¨: {:.2e})".format(metrics.learning_rate))
            recommendations.append("  - ÎìúÎ°≠ÏïÑÏõÉ ÎπÑÏú® Ï¶ùÍ∞Ä")
            recommendations.append("  - Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Ï†ÅÏö©")
            recommendations.append("  - Ï†ïÍ∑úÌôî Í∞ïÌôî")
        
        # ÌïôÏäµ Ï†ïÏ≤¥ Í≤ΩÍ≥†
        if len(self.train_loss_history) > 10:
            recent_losses = list(self.train_loss_history)[-10:]
            loss_variance = np.var(recent_losses)
            if loss_variance < 0.0001:
                recommendations.append("‚ö†Ô∏è ÌïôÏäµÏù¥ Ï†ïÏ≤¥ÎêòÏóàÏäµÎãàÎã§! Îã§ÏùåÏùÑ ÏãúÎèÑÌïòÏÑ∏Ïöî:")
                recommendations.append("  - ÌïôÏäµÎ•† Ï°∞Ï†ï")
                recommendations.append("  - Îã§Î•∏ ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏãúÎèÑ")
                recommendations.append("  - Î∞∞Ïπò ÌÅ¨Í∏∞ Î≥ÄÍ≤Ω")
        
        # Ï°∞Í∏∞ Ï¢ÖÎ£å Í≤ΩÍ≥†
        if self.should_stop:
            recommendations.append("üõë Ï°∞Í∏∞ Ï¢ÖÎ£åÎ•º Í∂åÏû•Ìï©ÎãàÎã§! Îçî Ïù¥ÏÉÅÏùò ÌïôÏäµÏùÄ Í≥ºÏ†ÅÌï©ÏùÑ Ïú†Î∞úÌï† Ïàò ÏûàÏäµÎãàÎã§.")
        
        # Í∂åÏû•ÏÇ¨Ìï≠ Ï∂úÎ†•
        if recommendations:
            logger.warning("\n".join(recommendations))
    
    def generate_plots(self):
        """ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© ÌîåÎ°Ø ÏÉùÏÑ±"""
        if not MATPLOTLIB_AVAILABLE:
            logger.warning("MatplotlibÏù¥ ÏóÜÏñ¥ ÌîåÎ°ØÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§")
            return
            
        if len(self.train_loss_history) < 2:
            return
        
        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. Loss Í≥°ÏÑ†
        ax1 = axes[0, 0]
        iterations = range(len(self.train_loss_history))
        ax1.plot(iterations, self.train_loss_history, 'b-', label='Train Loss', alpha=0.7)
        if self.val_loss_history:
            val_iterations = range(len(self.val_loss_history))
            ax1.plot(val_iterations, self.val_loss_history, 'r-', label='Validation Loss', alpha=0.7)
        ax1.set_xlabel('Iterations')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Progress')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ÌíàÏßà Ï†êÏàò Ï∂îÏù¥
        ax2 = axes[0, 1]
        quality_iterations = range(len(self.quality_score_history))
        ax2.plot(quality_iterations, self.quality_score_history, 'g-', alpha=0.7)
        ax2.set_xlabel('Iterations')
        ax2.set_ylabel('Quality Score')
        ax2.set_title('Learning Quality')
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3)
        
        # 3. Í≥ºÏ†ÅÌï© Î∂ÑÏÑù
        if self.val_loss_history and len(self.val_loss_history) > 0:
            ax3 = axes[1, 0]
            train_losses = list(self.train_loss_history)[:len(self.val_loss_history)]
            val_losses = list(self.val_loss_history)
            
            if train_losses and val_losses:
                overfitting_scores = [
                    self.calculate_overfitting_score(t, v) 
                    for t, v in zip(train_losses, val_losses)
                ]
                ax3.plot(range(len(overfitting_scores)), overfitting_scores, 'r-', alpha=0.7)
                ax3.axhline(y=0.3, color='r', linestyle='--', label='Warning Threshold')
                ax3.set_xlabel('Iterations')
                ax3.set_ylabel('Overfitting Score')
                ax3.set_title('Overfitting Detection')
                ax3.set_ylim(0, 1)
                ax3.legend()
                ax3.grid(True, alpha=0.3)
        
        # 4. ÌïôÏäµ ÏãúÍ∞Ñ Î∞è ÏÜçÎèÑ
        ax4 = axes[1, 1]
        if self.start_time:
            elapsed_time = (datetime.now() - self.start_time).total_seconds() / 3600  # hours
            speed = self.total_iterations / (elapsed_time + 1e-8)  # iterations per hour
            
            info_text = f"""ÌïôÏäµ Ï†ïÎ≥¥:
Ï¥ù Î∞òÎ≥µ: {self.total_iterations}
Í≤ΩÍ≥º ÏãúÍ∞Ñ: {elapsed_time:.2f} ÏãúÍ∞Ñ
ÌïôÏäµ ÏÜçÎèÑ: {speed:.1f} iterations/hour
ÌòÑÏû¨ Train Loss: {self.train_loss_history[-1]:.4f}
"""
            if self.val_loss_history:
                info_text += f"ÌòÑÏû¨ Val Loss: {self.val_loss_history[-1]:.4f}\n"
                info_text += f"Í≥ºÏ†ÅÌï© Ï†êÏàò: {overfitting_scores[-1]:.3f}" if 'overfitting_scores' in locals() else ""
            
            ax4.text(0.1, 0.5, info_text, transform=ax4.transAxes, 
                    fontsize=12, verticalalignment='center')
            ax4.axis('off')
        
        # ÌîåÎ°Ø Ï†ÄÏû•
        plot_file = self.plots_dir / f"learning_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.tight_layout()
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        # ÏµúÏã† ÌîåÎ°ØÏúºÎ°ú Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨ ÏÉùÏÑ± (Linux/WSL only)
        latest_plot = self.plots_dir / "latest_progress.png"
        if latest_plot.exists():
            latest_plot.unlink()
        try:
            latest_plot.symlink_to(plot_file.name)
        except:
            # WindowsÏóêÏÑúÎäî Î≥µÏÇ¨
            import shutil
            shutil.copy(plot_file, latest_plot)
    
    def get_learning_summary(self) -> Dict[str, Any]:
        """ÌïôÏäµ ÏöîÏïΩ Ï†ïÎ≥¥ Î∞òÌôò"""
        summary = {
            "total_iterations": self.total_iterations,
            "should_stop": self.should_stop,
            "current_train_loss": self.train_loss_history[-1] if self.train_loss_history else None,
            "current_val_loss": self.val_loss_history[-1] if self.val_loss_history else None,
            "best_val_loss": self.best_val_loss if self.best_val_loss < float('inf') else None,
            "average_quality_score": np.mean(self.quality_score_history) if self.quality_score_history else 0,
            "overfitting_detected": any(s > 0.3 for s in self.quality_score_history[-10:]) if len(self.quality_score_history) > 10 else False,
            "elapsed_time": str(datetime.now() - self.start_time) if self.start_time else "0:00:00"
        }
        
        return summary
    
    def export_report(self) -> Path:
        """ÌïôÏäµ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        report_path = self.monitor_dir / f"learning_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Ï†ÑÏ≤¥ Î©îÌä∏Î¶≠ Î°úÎìú
        all_metrics = []
        if self.metrics_file.exists():
            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        all_metrics.append(json.loads(line.strip()))
                    except:
                        continue
        
        # Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        report = {
            "summary": self.get_learning_summary(),
            "metrics_count": len(all_metrics),
            "final_metrics": all_metrics[-1] if all_metrics else None,
            "best_metrics": min(all_metrics, key=lambda x: x.get('validation_loss', float('inf'))) if all_metrics else None,
            "recommendations": self.generate_final_recommendations()
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ÌïôÏäµ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±: {report_path}")
        return report_path
    
    def generate_final_recommendations(self) -> List[str]:
        """ÏµúÏ¢Ö Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []
        
        summary = self.get_learning_summary()
        
        if summary["overfitting_detected"]:
            recommendations.append("Í≥ºÏ†ÅÌï©Ïù¥ Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§. Îã§Ïùå ÌïôÏäµ Ïãú Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÍ≥º Ï†ïÍ∑úÌôîÎ•º Í∞ïÌôîÌïòÏÑ∏Ïöî.")
        
        if summary["should_stop"]:
            recommendations.append("Ï°∞Í∏∞ Ï¢ÖÎ£åÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.")
        
        avg_quality = summary["average_quality_score"]
        if avg_quality < 0.5:
            recommendations.append(f"ÌèâÍ∑† ÌíàÏßà Ï†êÏàòÍ∞Ä ÎÇÆÏäµÎãàÎã§ ({avg_quality:.2f}). Îç∞Ïù¥ÌÑ∞ÏÖã ÌíàÏßàÏùÑ Í∞úÏÑ†ÌïòÏÑ∏Ïöî.")
        elif avg_quality > 0.8:
            recommendations.append(f"Ïö∞ÏàòÌïú ÌïôÏäµ ÌíàÏßàÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§ ({avg_quality:.2f}). ÌòÑÏû¨ ÏÑ§Ï†ïÏùÑ Ïú†ÏßÄÌïòÏÑ∏Ïöî.")
        
        return recommendations

# Ïã±Í∏ÄÌÜ§ Ïù∏Ïä§ÌÑ¥Ïä§
_quality_monitor = None

def get_quality_monitor() -> LearningQualityMonitor:
    """ÌíàÏßà Î™®ÎãàÌÑ∞ Ïã±Í∏ÄÌÜ§ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _quality_monitor
    if _quality_monitor is None:
        _quality_monitor = LearningQualityMonitor()
    return _quality_monitor