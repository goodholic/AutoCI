"""
AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ - ì‘ë‹µ í’ˆì§ˆ ê´€ë¦¬ì
ìš°ë¦¬ê°€ AI ëª¨ë¸ì˜ ì¡°ì¢…ê¶Œì„ ì™„ì „íˆ ê°–ê¸° ìœ„í•œ í•µì‹¬ ì‹œìŠ¤í…œ

ì£¼ìš” ì œì–´ ê¸°ëŠ¥:
1. ì‘ë‹µ í’ˆì§ˆ ì‹¤ì‹œê°„ ê²€ì¦ ë° ìë™ ì¬ì‹œë„
2. ëª¨ë¸ë³„ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ë° íŒŒë¼ë¯¸í„° ì™„ì „ ì œì–´
3. ê¸ˆì§€ëœ ì‘ë‹µ íŒ¨í„´ ìë™ ì°¨ë‹¨
4. ìš°ë¦¬ ê¸°ì¤€ì— ë§ëŠ” ë‹µë³€ë§Œ í—ˆìš©
5. ìƒì„¸í•œ í’ˆì§ˆ ë¡œê¹… ë° í†µê³„ ë¶„ì„
"""

import json
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class ResponseQuality:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    score: float  # 0.0 - 1.0
    is_acceptable: bool
    issues: List[str]
    improvements: List[str]
    confidence: float
    
@dataclass
class ModelControl:
    """ëª¨ë¸ ì œì–´ ì„¤ì •"""
    model_name: str
    max_attempts: int
    quality_threshold: float
    custom_prompts: Dict[str, str]
    parameter_overrides: Dict[str, Any]
    
class AIModelController:
    """AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.quality_standards = self._load_quality_standards()
        self.model_controls = self._load_model_controls()
        self.response_history = []
        self.banned_phrases = self._load_banned_phrases()
        self.required_patterns = self._load_required_patterns()
        
    def _load_quality_standards(self) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ ê¸°ì¤€ ë¡œë“œ"""
        return {
            "min_length": 50,
            "max_length": 2000,
            "required_korean_ratio": 0.3,  # í•œê¸€ ì§ˆë¬¸ì‹œ í•œê¸€ ì‘ë‹µ ë¹„ìœ¨
            "code_quality_patterns": [
                r"class\s+\w+",
                r"public\s+\w+",
                r"//.*",  # ì£¼ì„
                r"{\s*.*\s*}"  # ì½”ë“œ ë¸”ë¡
            ],
            "explanation_patterns": [
                r"(ì™œëƒí•˜ë©´|because|ë”°ë¼ì„œ|therefore)",
                r"(ì˜ˆë¥¼ ë“¤ì–´|for example|ì˜ˆì‹œ)",
                r"(ë°©ë²•ì€|method|í•´ê²°ì±…)"
            ],
            "forbidden_phrases": [
                "ì£„ì†¡í•©ë‹ˆë‹¤",
                "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
                "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                "I don't know",
                "I'm not sure"
            ]
        }
    
    def _load_model_controls(self) -> Dict[str, ModelControl]:
        """ëª¨ë¸ë³„ ì œì–´ ì„¤ì • - ìš°ë¦¬ê°€ ì™„ì „íˆ ì œì–´í•©ë‹ˆë‹¤!"""
        return {
            "deepseek-coder-7b": ModelControl(
                model_name="deepseek-coder-7b",
                max_attempts=3,
                quality_threshold=0.7,
                custom_prompts={
                    "system": "You are an expert C# and Godot developer. Always provide detailed, practical answers in Korean with code examples.",
                    "code_request": "Write clean, well-commented C# code. Explain each part in Korean.",
                    "korean_terms": "Provide accurate Korean translations for programming terms with examples.",
                    "quality_enforcer": "You MUST provide complete, working examples. Never say 'I don't know' or 'I'm not sure'."
                },
                parameter_overrides={
                    "temperature": 0.6,  # ë” ì¼ê´€ëœ ì‘ë‹µ
                    "max_new_tokens": 200,  # ë” ìƒì„¸í•œ ì‘ë‹µ
                    "top_p": 0.9,
                    "repetition_penalty": 1.1,
                    "do_sample": True,
                    "num_beams": 1
                }
            ),
            "llama-3.1-8b": ModelControl(
                model_name="llama-3.1-8b",
                max_attempts=2,
                quality_threshold=0.6,
                custom_prompts={
                    "system": "You are a helpful programming assistant. Answer in Korean when asked in Korean.",
                    "quality_enforcer": "Provide concrete, actionable answers. No apologies or uncertainty."
                },
                parameter_overrides={
                    "temperature": 0.7,
                    "max_new_tokens": 150,
                    "top_p": 0.95
                }
            ),
            "codellama-13b": ModelControl(
                model_name="codellama-13b",
                max_attempts=3,
                quality_threshold=0.75,
                custom_prompts={
                    "system": "You are a code generation expert. Provide production-ready code with Korean explanations.",
                    "code_request": "Generate optimized, well-structured code with detailed comments in Korean.",
                    "quality_enforcer": "Always provide complete, runnable code examples."
                },
                parameter_overrides={
                    "temperature": 0.5,
                    "max_new_tokens": 250,
                    "top_p": 0.9,
                    "repetition_penalty": 1.15
                }
            ),
            "qwen2.5-coder-32b": ModelControl(
                model_name="qwen2.5-coder-32b",
                max_attempts=2,
                quality_threshold=0.8,
                custom_prompts={
                    "system": "You are an advanced coding assistant specializing in C#, Godot, and game development.",
                    "korean_mode": "í•œêµ­ì–´ë¡œ ì§ˆë¬¸ë°›ìœ¼ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì½”ë“œëŠ” ì˜ì–´ë¡œ, ì„¤ëª…ì€ í•œêµ­ì–´ë¡œ.",
                    "quality_enforcer": "Provide expert-level, detailed solutions. Never give incomplete answers."
                },
                parameter_overrides={
                    "temperature": 0.55,
                    "max_new_tokens": 300,
                    "top_p": 0.92,
                    "repetition_penalty": 1.1
                }
            )
        }
    
    def _load_banned_phrases(self) -> List[str]:
        """ê¸ˆì§€ëœ ì‘ë‹µ íŒ¨í„´"""
        return [
            "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "I cannot",
            "I'm unable to",
            "ì£„ì†¡í•˜ì§€ë§Œ",
            "ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤"
        ]
    
    def _load_required_patterns(self) -> Dict[str, List[str]]:
        """ìš”êµ¬ë˜ëŠ” ì‘ë‹µ íŒ¨í„´"""
        return {
            "code_question": [
                r"(class|public|private|protected)",
                r"{\s*.*\s*}",  # ì½”ë“œ ë¸”ë¡
                r"//"  # ì£¼ì„
            ],
            "korean_question": [
                r"[ê°€-í£]{10,}",  # ìµœì†Œ 10ì ì´ìƒ í•œê¸€
            ],
            "explanation": [
                r"(ì´ìœ |ë°©ë²•|ì˜ˆì‹œ|ì„¤ëª…)"
            ]
        }
    
    def evaluate_response_quality(self, question: Dict[str, Any], response: str, model_name: str) -> ResponseQuality:
        """ì‘ë‹µ í’ˆì§ˆ ì¢…í•© í‰ê°€ - ìš°ë¦¬ì˜ ê¸°ì¤€ìœ¼ë¡œ!"""
        issues = []
        improvements = []
        score_factors = []
        
        # 1. ê¸°ë³¸ ê¸¸ì´ ê²€ì‚¬
        if len(response) < self.quality_standards["min_length"]:
            issues.append("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ")
            score_factors.append(0.2)
        elif len(response) > self.quality_standards["max_length"]:
            issues.append("ì‘ë‹µì´ ë„ˆë¬´ ê¹€")
            score_factors.append(0.8)
        else:
            score_factors.append(1.0)
        
        # 2. ê¸ˆì§€ëœ êµ¬ë¬¸ ê²€ì‚¬
        banned_found = False
        for banned in self.quality_standards["forbidden_phrases"]:
            if banned.lower() in response.lower():
                issues.append(f"ê¸ˆì§€ëœ êµ¬ë¬¸ ë°œê²¬: {banned}")
                score_factors.append(0.1)
                banned_found = True
                break
        if not banned_found:
            score_factors.append(1.0)
        
        # 3. í•œê¸€ ì§ˆë¬¸ì— ëŒ€í•œ í•œê¸€ ì‘ë‹µ ë¹„ìœ¨
        if question.get("language") == "korean":
            korean_chars = len(re.findall(r'[ê°€-í£]', response))
            total_chars = len(re.sub(r'\s+', '', response))
            korean_ratio = korean_chars / max(total_chars, 1)
            
            if korean_ratio < self.quality_standards["required_korean_ratio"]:
                issues.append(f"í•œê¸€ ì‘ë‹µ ë¹„ìœ¨ ë¶€ì¡±: {korean_ratio:.2f}")
                score_factors.append(korean_ratio)
            else:
                score_factors.append(1.0)
        
        # 4. ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ì½”ë“œ í’ˆì§ˆ
        if question.get("type") in ["example", "error", "optimize"]:
            code_score = self._evaluate_code_quality(response)
            score_factors.append(code_score)
            if code_score < 0.5:
                issues.append("ì½”ë“œ í’ˆì§ˆ ë¶€ì¡±")
        
        # 5. ì„¤ëª… í’ˆì§ˆ í‰ê°€
        explanation_score = self._evaluate_explanation_quality(response, question.get("language", "english"))
        score_factors.append(explanation_score)
        if explanation_score < 0.6:
            issues.append("ì„¤ëª… í’ˆì§ˆ ë¶€ì¡±")
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        final_score = sum(score_factors) / len(score_factors)
        threshold = self.model_controls.get(model_name, ModelControl("default", 3, 0.5, {}, {})).quality_threshold
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        if final_score < threshold:
            improvements = self._generate_improvements(issues, question, model_name)
        
        return ResponseQuality(
            score=final_score,
            is_acceptable=final_score >= threshold,
            issues=issues,
            improvements=improvements,
            confidence=final_score
        )
    
    def _evaluate_code_quality(self, response: str) -> float:
        """ì½”ë“œ í’ˆì§ˆ í‰ê°€"""
        score = 0.0
        
        # ì½”ë“œ ë¸”ë¡ ì¡´ì¬
        if "```" in response or re.search(r'{\s*.*\s*}', response):
            score += 0.3
        
        # ì£¼ì„ ì¡´ì¬
        if "//" in response or "/*" in response:
            score += 0.2
        
        # C# í‚¤ì›Œë“œ ì¡´ì¬
        csharp_keywords = ["class", "public", "private", "void", "string", "int", "bool"]
        if any(keyword in response for keyword in csharp_keywords):
            score += 0.3
        
        # ì˜ë¯¸ìˆëŠ” ë³€ìˆ˜ëª…
        if re.search(r'\b[a-zA-Z][a-zA-Z0-9]{2,}\b', response):
            score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_explanation_quality(self, response: str, language: str) -> float:
        """ì„¤ëª… í’ˆì§ˆ í‰ê°€"""
        score = 0.0
        
        if language == "korean":
            # í•œê¸€ ì„¤ëª… íŒ¨í„´
            patterns = [
                r'(ì´ìœ ëŠ”|ì™œëƒí•˜ë©´|ë•Œë¬¸ì—)',
                r'(ì˜ˆë¥¼ ë“¤ì–´|ì˜ˆì‹œ)',
                r'(ë°©ë²•ì€|ë°©ì‹)',
                r'(ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ê²°êµ­)'
            ]
        else:
            # ì˜ì–´ ì„¤ëª… íŒ¨í„´
            patterns = [
                r'(because|since|due to)',
                r'(for example|such as)',
                r'(method|way|approach)',
                r'(therefore|thus|consequently)'
            ]
        
        for pattern in patterns:
            if re.search(pattern, response, re.IGNORECASE):
                score += 0.25
        
        return min(score, 1.0)
    
    def _generate_improvements(self, issues: List[str], question: Dict[str, Any], model_name: str) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        improvements = []
        
        if "ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ" in issues:
            improvements.append("ë” ìƒì„¸í•œ ì„¤ëª…ê³¼ ì˜ˆì œë¥¼ ì¶”ê°€í•˜ì„¸ìš”")
        
        if "í•œê¸€ ì‘ë‹µ ë¹„ìœ¨ ë¶€ì¡±" in issues:
            improvements.append("í•œê¸€ë¡œ ë” ë§ì´ ì„¤ëª…í•˜ì„¸ìš”")
        
        if "ì½”ë“œ í’ˆì§ˆ ë¶€ì¡±" in issues:
            improvements.append("ì£¼ì„ì´ í¬í•¨ëœ ì™„ì „í•œ ì½”ë“œ ì˜ˆì œë¥¼ ì œê³µí•˜ì„¸ìš”")
        
        if "ì„¤ëª… í’ˆì§ˆ ë¶€ì¡±" in issues:
            improvements.append("'ì™œëƒí•˜ë©´', 'ì˜ˆë¥¼ ë“¤ì–´' ë“±ì„ ì‚¬ìš©í•´ ë” ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”")
        
        return improvements
    
    def get_custom_prompt(self, model_name: str, question_type: str) -> str:
        """ëª¨ë¸ë³„ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        model_control = self.model_controls.get(model_name)
        if not model_control:
            return "Answer the question accurately and completely."
        
        base_prompt = model_control.custom_prompts.get("system", "")
        
        if question_type in ["example", "error", "optimize"]:
            code_prompt = model_control.custom_prompts.get("code_request", "")
            return f"{base_prompt}\n{code_prompt}"
        elif question_type == "translate":
            korean_prompt = model_control.custom_prompts.get("korean_terms", "")
            return f"{base_prompt}\n{korean_prompt}"
        
        return base_prompt
    
    def get_parameter_overrides(self, model_name: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ"""
        model_control = self.model_controls.get(model_name)
        return model_control.parameter_overrides if model_control else {}
    
    def should_retry(self, quality: ResponseQuality, model_name: str, attempt: int) -> bool:
        """ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
        model_control = self.model_controls.get(model_name)
        max_attempts = model_control.max_attempts if model_control else 2
        
        return not quality.is_acceptable and attempt < max_attempts
    
    def log_response_quality(self, question: Dict[str, Any], response: str, quality: ResponseQuality, model_name: str):
        """ì‘ë‹µ í’ˆì§ˆ ë¡œê¹…"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model": model_name,
            "question_id": question.get("id"),
            "question_type": question.get("type"),
            "language": question.get("language"),
            "response_length": len(response),
            "quality_score": quality.score,
            "is_acceptable": quality.is_acceptable,
            "issues": quality.issues,
            "improvements": quality.improvements
        }
        
        self.response_history.append(log_entry)
        
        # ì‹¤ì‹œê°„ ë¡œê¹…
        if quality.is_acceptable:
            logger.info(f"âœ… í’ˆì§ˆ í†µê³¼: {model_name} (ì ìˆ˜: {quality.score:.2f})")
        else:
            logger.warning(f"âŒ í’ˆì§ˆ ì‹¤íŒ¨: {model_name} (ì ìˆ˜: {quality.score:.2f}) - {', '.join(quality.issues)}")
    
    def get_quality_report(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ê´€ë¦¬ ë¦¬í¬íŠ¸"""
        if not self.response_history:
            return {"message": "ì•„ì§ ì‘ë‹µ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        total_responses = len(self.response_history)
        acceptable_responses = sum(1 for entry in self.response_history if entry["is_acceptable"])
        avg_score = sum(entry["quality_score"] for entry in self.response_history) / total_responses
        
        model_stats = {}
        for entry in self.response_history:
            model = entry["model"]
            if model not in model_stats:
                model_stats[model] = {"total": 0, "acceptable": 0, "avg_score": 0}
            
            model_stats[model]["total"] += 1
            if entry["is_acceptable"]:
                model_stats[model]["acceptable"] += 1
            model_stats[model]["avg_score"] += entry["quality_score"]
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        for model in model_stats:
            model_stats[model]["avg_score"] /= model_stats[model]["total"]
            model_stats[model]["success_rate"] = model_stats[model]["acceptable"] / model_stats[model]["total"]
        
        return {
            "total_responses": total_responses,
            "overall_success_rate": acceptable_responses / total_responses,
            "average_quality_score": avg_score,
            "model_performance": model_stats,
            "generated_at": datetime.now().isoformat()
        }
    
    def force_quality_response(self, question: Dict[str, Any], model_name: str, max_attempts: int = 5) -> Tuple[str, ResponseQuality]:
        """í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•  ë•Œê¹Œì§€ ê°•ì œë¡œ ì¬ì‹œë„"""
        logger.info(f"ğŸ¯ í’ˆì§ˆ ì‘ë‹µ ê°•ì œ ì‹¤í–‰: {model_name}")
        
        for attempt in range(1, max_attempts + 1):
            # ì‹œë®¬ë ˆì´ì…˜: ì‹¤ì œë¡œëŠ” ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ë¶€ë¶„
            response = f"[ëª¨ë¸ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ - ì‹œë„ {attempt}]"
            quality = self.evaluate_response_quality(question, response, model_name)
            
            if quality.is_acceptable:
                logger.info(f"âœ… ì‹œë„ {attempt}: í’ˆì§ˆ í†µê³¼ (ì ìˆ˜: {quality.score:.2f})")
                return response, quality
            else:
                logger.warning(f"âŒ ì‹œë„ {attempt}: í’ˆì§ˆ ì‹¤íŒ¨ (ì ìˆ˜: {quality.score:.2f})")
                logger.info(f"   ë¬¸ì œì : {', '.join(quality.issues)}")
                
                # ë‹¤ìŒ ì‹œë„ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”
                if attempt < max_attempts:
                    logger.info("   â†’ í”„ë¡¬í”„íŠ¸ ê°•í™” í›„ ì¬ì‹œë„...")
        
        logger.error(f"âš ï¸ {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬")
        return response, quality
    
    def get_model_control_status(self) -> Dict[str, Any]:
        """AI ëª¨ë¸ ì œì–´ ìƒíƒœ ì¢…í•© ë³´ê³ """
        status = {
            "control_system": "ACTIVE",
            "quality_standards": {
                "min_length": self.quality_standards["min_length"],
                "max_length": self.quality_standards["max_length"],
                "korean_ratio": self.quality_standards["required_korean_ratio"],
                "forbidden_phrases": len(self.quality_standards["forbidden_phrases"])
            },
            "controlled_models": {},
            "recent_performance": {}
        }
        
        # ì œì–´ ì¤‘ì¸ ëª¨ë¸ ì •ë³´
        for model_name, control in self.model_controls.items():
            status["controlled_models"][model_name] = {
                "quality_threshold": control.quality_threshold,
                "max_attempts": control.max_attempts,
                "custom_prompts": len(control.custom_prompts),
                "parameter_overrides": len(control.parameter_overrides)
            }
        
        # ìµœê·¼ ì„±ëŠ¥ í†µê³„
        if self.response_history:
            recent_history = self.response_history[-100:]  # ìµœê·¼ 100ê°œ
            
            for entry in recent_history:
                model = entry["model"]
                if model not in status["recent_performance"]:
                    status["recent_performance"][model] = {
                        "attempts": 0,
                        "successes": 0,
                        "avg_score": 0
                    }
                
                status["recent_performance"][model]["attempts"] += 1
                if entry["is_acceptable"]:
                    status["recent_performance"][model]["successes"] += 1
                status["recent_performance"][model]["avg_score"] += entry["quality_score"]
            
            # í‰ê·  ê³„ì‚°
            for model in status["recent_performance"]:
                attempts = status["recent_performance"][model]["attempts"]
                status["recent_performance"][model]["avg_score"] /= attempts
                status["recent_performance"][model]["success_rate"] = (
                    status["recent_performance"][model]["successes"] / attempts
                )
        
        status["total_models_controlled"] = len(self.model_controls)
        status["control_message"] = "ğŸ¯ ìš°ë¦¬ê°€ AI ëª¨ë¸ì˜ ì™„ì „í•œ ì¡°ì¢…ê¶Œì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤!"
        
        return status 