"""
AutoCI ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ ì‹œìŠ¤í…œ
autoci fixê°€ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ autoci learnê³¼ autoci createê°€ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í†µí•© ì‹œìŠ¤í…œ
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import asyncio
from collections import defaultdict

logger = logging.getLogger(__name__)

class SharedKnowledgeBase:
    """ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ - ëª¨ë“  AutoCI ì»´í¬ë„ŒíŠ¸ê°€ ì§€ì‹ì„ ê³µìœ """
    
    def __init__(self):
        # ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬
        self.knowledge_base_dir = Path("experiences/knowledge_base")
        self.knowledge_base_dir.mkdir(parents=True, exist_ok=True)
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.search_cache_dir = self.knowledge_base_dir / "search_results"
        self.search_cache_dir.mkdir(exist_ok=True)
        
        # ì†”ë£¨ì…˜ ë°ì´í„°ë² ì´ìŠ¤
        self.solutions_dir = self.knowledge_base_dir / "solutions"
        self.solutions_dir.mkdir(exist_ok=True)
        
        # ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
        self.best_practices_dir = self.knowledge_base_dir / "best_practices"
        self.best_practices_dir.mkdir(exist_ok=True)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼ìš©)
        self.memory_cache = {
            "search_results": {},
            "solutions": {},
            "best_practices": {},
            "last_update": datetime.now()
        }
        
        # ì´ˆê¸° ë¡œë“œ
        self._load_existing_knowledge()
        
        logger.info("ğŸ“š ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_existing_knowledge(self):
        """ê¸°ì¡´ ì§€ì‹ ë¡œë“œ"""
        try:
            # ìµœê·¼ 7ì¼ê°„ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ
            cutoff_date = datetime.now() - timedelta(days=7)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ
            for search_file in self.search_cache_dir.glob("*.json"):
                try:
                    file_time = datetime.fromtimestamp(search_file.stat().st_mtime)
                    if file_time > cutoff_date:
                        with open(search_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            keyword = data.get("keyword", "")
                            if keyword:
                                self.memory_cache["search_results"][keyword] = data
                except:
                    continue
            
            logger.info(f"ğŸ“š {len(self.memory_cache['search_results'])}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œë¨")
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    async def get_cached_search(self, keyword: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜"""
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if keyword in self.memory_cache["search_results"]:
            result = self.memory_cache["search_results"][keyword]
            # 24ì‹œê°„ ì´ë‚´ ê²°ê³¼ë§Œ ìœ íš¨
            if "timestamp" in result:
                result_time = datetime.fromisoformat(result["timestamp"])
                if datetime.now() - result_time < timedelta(hours=24):
                    logger.info(f"ğŸ“š ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©: {keyword}")
                    return result
        
        # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ í™•ì¸
        search_file = self.search_cache_dir / f"search_{keyword.replace(' ', '_')}_*.json"
        matching_files = list(self.search_cache_dir.glob(f"search_{keyword.replace(' ', '_')}_*.json"))
        
        if matching_files:
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
            latest_file = max(matching_files, key=lambda p: p.stat().st_mtime)
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ë©”ëª¨ë¦¬ ìºì‹œì— ì¶”ê°€
                    self.memory_cache["search_results"][keyword] = data
                    return data
            except:
                pass
        
        return None
    
    async def save_search_result(self, keyword: str, result: Dict[str, Any]):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (autoci fixê°€ ì‚¬ìš©)"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            result["timestamp"] = datetime.now().isoformat()
            result["keyword"] = keyword
            
            # íŒŒì¼ë¡œ ì €ì¥
            filename = f"search_{keyword.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            file_path = self.search_cache_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
            self.memory_cache["search_results"][keyword] = result
            
            logger.info(f"ğŸ“š ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ë¨: {keyword}")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def get_solution_for_error(self, error_type: str, error_message: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ì˜¤ë¥˜ì— ëŒ€í•œ ì†”ë£¨ì…˜ ë°˜í™˜"""
        # ì†”ë£¨ì…˜ ìºì‹œ í™•ì¸
        solution_key = f"{error_type}_{hash(error_message) % 10000}"
        
        if solution_key in self.memory_cache["solutions"]:
            return self.memory_cache["solutions"][solution_key]
        
        # íŒŒì¼ì—ì„œ í™•ì¸
        solution_file = self.solutions_dir / f"{error_type}_solutions.json"
        if solution_file.exists():
            try:
                with open(solution_file, 'r', encoding='utf-8') as f:
                    solutions = json.load(f)
                    for solution in solutions:
                        if error_message in solution.get("error_patterns", []):
                            self.memory_cache["solutions"][solution_key] = solution
                            return solution
            except:
                pass
        
        return None
    
    async def save_solution(self, error_type: str, error_message: str, solution: str, success: bool = True):
        """í•´ê²° ë°©ë²• ì €ì¥"""
        try:
            solution_file = self.solutions_dir / f"{error_type}_solutions.json"
            
            # ê¸°ì¡´ ì†”ë£¨ì…˜ ë¡œë“œ
            solutions = []
            if solution_file.exists():
                try:
                    with open(solution_file, 'r', encoding='utf-8') as f:
                        solutions = json.load(f)
                except:
                    solutions = []
            
            # ìƒˆ ì†”ë£¨ì…˜ ì¶”ê°€
            new_solution = {
                "timestamp": datetime.now().isoformat(),
                "error_patterns": [error_message],
                "solution": solution,
                "success": success,
                "usage_count": 0
            }
            
            solutions.append(new_solution)
            
            # ì €ì¥
            with open(solution_file, 'w', encoding='utf-8') as f:
                json.dump(solutions, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“š ì†”ë£¨ì…˜ ì €ì¥ë¨: {error_type}")
            
        except Exception as e:
            logger.error(f"ì†”ë£¨ì…˜ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def get_best_practices(self, topic: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì£¼ì œì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë°˜í™˜"""
        practices = []
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if topic in self.memory_cache["best_practices"]:
            return self.memory_cache["best_practices"][topic]
        
        # íŒŒì¼ì—ì„œ ë¡œë“œ
        practice_file = self.best_practices_dir / f"{topic.replace(' ', '_')}_practices.json"
        if practice_file.exists():
            try:
                with open(practice_file, 'r', encoding='utf-8') as f:
                    practices = json.load(f)
                    self.memory_cache["best_practices"][topic] = practices
            except:
                pass
        
        return practices
    
    async def save_best_practice(self, topic: str, practice: Dict[str, Any]):
        """ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì €ì¥"""
        try:
            practice_file = self.best_practices_dir / f"{topic.replace(' ', '_')}_practices.json"
            
            # ê¸°ì¡´ í”„ë™í‹°ìŠ¤ ë¡œë“œ
            practices = []
            if practice_file.exists():
                try:
                    with open(practice_file, 'r', encoding='utf-8') as f:
                        practices = json.load(f)
                except:
                    practices = []
            
            # ìƒˆ í”„ë™í‹°ìŠ¤ ì¶”ê°€
            practice["timestamp"] = datetime.now().isoformat()
            practice["topic"] = topic
            practices.append(practice)
            
            # ì €ì¥
            with open(practice_file, 'w', encoding='utf-8') as f:
                json.dump(practices, f, indent=2, ensure_ascii=False)
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
            self.memory_cache["best_practices"][topic] = practices
            
            logger.info(f"ğŸ“š ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì €ì¥ë¨: {topic}")
            
        except Exception as e:
            logger.error(f"ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def get_knowledge_stats(self) -> Dict[str, int]:
        """ì§€ì‹ ë² ì´ìŠ¤ í†µê³„"""
        stats = {
            "total_searches": len(list(self.search_cache_dir.glob("*.json"))),
            "cached_searches": len(self.memory_cache["search_results"]),
            "total_solutions": len(list(self.solutions_dir.glob("*.json"))),
            "total_practices": len(list(self.best_practices_dir.glob("*.json"))),
            "memory_cache_size": sum(len(v) for v in self.memory_cache.values() if isinstance(v, dict))
        }
        return stats
    
    async def cleanup_old_data(self, days: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_count = 0
        
        # ëª¨ë“  ë””ë ‰í† ë¦¬ì˜ ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
        for directory in [self.search_cache_dir, self.solutions_dir, self.best_practices_dir]:
            for file_path in directory.glob("*.json"):
                try:
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_date:
                        file_path.unlink()
                        cleaned_count += 1
                except:
                    continue
        
        logger.info(f"ğŸ§¹ {cleaned_count}ê°œì˜ ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ë¨")
        
        # ë©”ëª¨ë¦¬ ìºì‹œë„ ì •ë¦¬
        self.memory_cache["search_results"].clear()
        self.memory_cache["solutions"].clear()
        self.memory_cache["best_practices"].clear()
        self._load_existing_knowledge()

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_shared_kb = None

def get_shared_knowledge_base() -> SharedKnowledgeBase:
    """ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _shared_kb
    if _shared_kb is None:
        _shared_kb = SharedKnowledgeBase()
    return _shared_kb