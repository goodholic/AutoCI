"""
AutoCI ê²©ì°¨ ë³´ì™„ ì§€ëŠ¥ ì‹œìŠ¤í…œ
autoci learnê³¼ autoci createì—ì„œ ë¶€ì¡±í•œ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ë©”ê¿”ì£¼ëŠ” í•µì‹¬ ëª¨ë“ˆ
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass
try:
    import numpy as np
except ImportError:
    # numpyê°€ ì—†ì„ ë•Œ ëŒ€ì²´ êµ¬í˜„
    class np:
        @staticmethod
        def random():
            import random
            class Random:
                @staticmethod
                def randint(a, b):
                    return random.randint(a, b)
                @staticmethod
                def uniform(a, b):
                    return random.uniform(a, b)
            return Random()
        
        @staticmethod
        def var(data):
            if not data:
                return 0
            mean = sum(data) / len(data)
            return sum((x - mean) ** 2 for x in data) / len(data)
from collections import defaultdict, Counter
import re

logger = logging.getLogger(__name__)

@dataclass
class KnowledgeGap:
    """ì§€ì‹ ê²©ì°¨ ì •ë³´"""
    category: str
    severity: float  # 0.0 ~ 1.0
    description: str
    suggested_actions: List[str]
    search_keywords: List[str]
    priority: str  # 'low', 'medium', 'high', 'critical'
    detected_at: datetime
    auto_fix_possible: bool

@dataclass
class LearningDeficiency:
    """í•™ìŠµ ë¶€ì¡± ë¶€ë¶„"""
    skill_area: str
    deficiency_score: float
    evidence: List[str]
    recommended_resources: List[str]
    estimated_learning_time: int  # minutes
    
class GapFillingIntelligence:
    """ê²©ì°¨ ë³´ì™„ ì§€ëŠ¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.gaps_dir = Path("experiences/knowledge_gaps")
        self.gaps_dir.mkdir(parents=True, exist_ok=True)
        
        self.filled_gaps_dir = Path("experiences/filled_gaps")
        self.filled_gaps_dir.mkdir(parents=True, exist_ok=True)
        
        # ì§€ì‹ ì˜ì—­ ì •ì˜
        self.knowledge_areas = {
            "csharp_basics": {
                "keywords": ["class", "method", "property", "namespace", "using"],
                "min_threshold": 10,
                "importance": 0.9
            },
            "csharp_advanced": {
                "keywords": ["async", "await", "generic", "linq", "delegate"],
                "min_threshold": 5,
                "importance": 0.8
            },
            "godot_basics": {
                "keywords": ["node", "scene", "signal", "script", "_ready"],
                "min_threshold": 15,
                "importance": 0.9
            },
            "godot_advanced": {
                "keywords": ["shader", "gdnative", "multiplayer", "physics", "animation"],
                "min_threshold": 8,
                "importance": 0.7
            },
            "socketio_networking": {
                "keywords": ["socket", "emit", "on", "connect", "disconnect"],
                "min_threshold": 6,
                "importance": 0.6
            },
            "pytorch_ai": {
                "keywords": ["tensor", "model", "train", "loss", "optimizer"],
                "min_threshold": 5,
                "importance": 0.7
            },
            "game_design": {
                "keywords": ["gameplay", "mechanics", "level", "ui", "player"],
                "min_threshold": 12,
                "importance": 0.8
            }
        }
        
        # ìë™ ìˆ˜ì • ê°€ëŠ¥í•œ ë¬¸ì œ íŒ¨í„´
        self.auto_fixable_patterns = {
            "missing_using_statements": {
                "pattern": r"The type or namespace name '(\w+)' could not be found",
                "fix_template": "using {namespace};"
            },
            "missing_async_await": {
                "pattern": r"Cannot await|async method",
                "fix_template": "async/await íŒ¨í„´ ì ìš© í•„ìš”"
            },
            "godot_node_access": {
                "pattern": r"GetNode.*null|Node not found",
                "fix_template": "ë…¸ë“œ ê²½ë¡œ í™•ì¸ ë° null ì²´í¬ ì¶”ê°€"
            }
        }
        
        logger.info("ğŸ¯ ê²©ì°¨ ë³´ì™„ ì§€ëŠ¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def analyze_comprehensive_gaps(self) -> List[KnowledgeGap]:
        """ì¢…í•©ì ì¸ ì§€ì‹ ê²©ì°¨ ë¶„ì„"""
        logger.info("ğŸ” ì¢…í•©ì ì¸ ì§€ì‹ ê²©ì°¨ ë¶„ì„ ì‹œì‘")
        
        gaps = []
        
        # 1. í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ê²©ì°¨ ë¶„ì„
        learning_gaps = await self._analyze_learning_data_gaps()
        gaps.extend(learning_gaps)
        
        # 2. í”„ë¡œì íŠ¸ ìƒì„± ê²°ê³¼ ê¸°ë°˜ ê²©ì°¨ ë¶„ì„
        project_gaps = await self._analyze_project_creation_gaps()
        gaps.extend(project_gaps)
        
        # 3. ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ê²©ì°¨ ë¶„ì„
        error_gaps = await self._analyze_error_pattern_gaps()
        gaps.extend(error_gaps)
        
        # 4. ì‹œê°„ íš¨ìœ¨ì„± ê¸°ë°˜ ê²©ì°¨ ë¶„ì„
        efficiency_gaps = await self._analyze_efficiency_gaps()
        gaps.extend(efficiency_gaps)
        
        # ê²©ì°¨ ìš°ì„ ìˆœìœ„ ì •ë ¬
        gaps.sort(key=lambda g: (g.severity, g.priority), reverse=True)
        
        logger.info(f"ğŸ¯ ì´ {len(gaps)}ê°œì˜ ì§€ì‹ ê²©ì°¨ ê°ì§€ë¨")
        
        # ê²©ì°¨ ì •ë³´ ì €ì¥
        await self._save_gap_analysis(gaps)
        
        return gaps
    
    async def _analyze_learning_data_gaps(self) -> List[KnowledgeGap]:
        """í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ê²©ì°¨ ë¶„ì„"""
        gaps = []
        
        try:
            # ì—°ì† í•™ìŠµ ë°ì´í„° ë¶„ì„
            continuous_learning_dir = Path("continuous_learning")
            if not continuous_learning_dir.exists():
                return gaps
            
            # ê° ì§€ì‹ ì˜ì—­ë³„ ë°ì´í„° ìˆ˜ì§‘
            area_data = defaultdict(list)
            
            for file_path in continuous_learning_dir.rglob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        content = str(data).lower()
                        
                        # ê° ì§€ì‹ ì˜ì—­ì— ë§¤ì¹­
                        for area, config in self.knowledge_areas.items():
                            keyword_count = sum(1 for keyword in config["keywords"] 
                                             if keyword in content)
                            if keyword_count > 0:
                                area_data[area].append({
                                    'file': str(file_path),
                                    'keyword_count': keyword_count,
                                    'content_length': len(content)
                                })
                except:
                    continue
            
            # ê²©ì°¨ ë¶„ì„
            for area, config in self.knowledge_areas.items():
                data_points = area_data.get(area, [])
                total_keywords = sum(d['keyword_count'] for d in data_points)
                
                if total_keywords < config["min_threshold"]:
                    severity = 1.0 - (total_keywords / config["min_threshold"])
                    severity = min(severity * config["importance"], 1.0)
                    
                    gap = KnowledgeGap(
                        category=f"learning_data_{area}",
                        severity=severity,
                        description=f"{area} ì˜ì—­ì˜ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (í˜„ì¬: {total_keywords}, í•„ìš”: {config['min_threshold']})",
                        suggested_actions=[
                            f"{area} ê´€ë ¨ autoci learn ì„¸ì…˜ ì¦ê°€",
                            f"{area} ì „ìš© í•™ìŠµ ìë£Œ ê²€ìƒ‰",
                            f"{area} ì‹¤ìŠµ í”„ë¡œì íŠ¸ ìƒì„±"
                        ],
                        search_keywords=config["keywords"],
                        priority="high" if severity > 0.7 else "medium" if severity > 0.4 else "low",
                        detected_at=datetime.now(),
                        auto_fix_possible=True
                    )
                    gaps.append(gap)
        
        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ê²©ì°¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return gaps
    
    async def _analyze_project_creation_gaps(self) -> List[KnowledgeGap]:
        """í”„ë¡œì íŠ¸ ìƒì„± ê²°ê³¼ ê¸°ë°˜ ê²©ì°¨ ë¶„ì„"""
        gaps = []
        
        try:
            game_projects_dir = Path("game_projects")
            if not game_projects_dir.exists():
                return gaps
            
            # ìµœê·¼ í”„ë¡œì íŠ¸ ë¶„ì„
            recent_projects = []
            one_week_ago = datetime.now() - timedelta(days=7)
            
            for project_dir in game_projects_dir.iterdir():
                if project_dir.is_dir():
                    creation_time = datetime.fromtimestamp(project_dir.stat().st_ctime)
                    if creation_time > one_week_ago:
                        recent_projects.append(project_dir)
            
            if len(recent_projects) < 3:
                gap = KnowledgeGap(
                    category="project_creation_frequency",
                    severity=0.8,
                    description="ìµœê·¼ 1ì£¼ì¼ê°„ ìƒì„±ëœ í”„ë¡œì íŠ¸ê°€ ë¶€ì¡±í•¨",
                    suggested_actions=[
                        "ë” ìì£¼ autoci create ì‹¤í–‰",
                        "ë‹¤ì–‘í•œ ê²Œì„ íƒ€ì… ì‹¤í—˜",
                        "í”„ë¡œì íŠ¸ ìƒì„± ìë™í™” ê³ ë ¤"
                    ],
                    search_keywords=["game development", "project templates", "rapid prototyping"],
                    priority="high",
                    detected_at=datetime.now(),
                    auto_fix_possible=True
                )
                gaps.append(gap)
            
            # í”„ë¡œì íŠ¸ í’ˆì§ˆ ë¶„ì„
            failed_projects = 0
            for project_dir in recent_projects:
                config_file = project_dir / "config.json"
                if config_file.exists():
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                            if config.get('status') == 'failed' or config.get('quality_score', 0) < 0.5:
                                failed_projects += 1
                    except:
                        failed_projects += 1
                else:
                    failed_projects += 1
            
            if failed_projects > len(recent_projects) * 0.5:  # 50% ì´ìƒ ì‹¤íŒ¨
                gap = KnowledgeGap(
                    category="project_quality",
                    severity=0.9,
                    description=f"í”„ë¡œì íŠ¸ ì‹¤íŒ¨ìœ¨ì´ ë†’ìŒ ({failed_projects}/{len(recent_projects)})",
                    suggested_actions=[
                        "ê¸°ì´ˆ í•™ìŠµ ê°•í™”",
                        "ë‹¨ìˆœí•œ í”„ë¡œì íŠ¸ë¶€í„° ì‹œì‘",
                        "ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ë° í•´ê²°"
                    ],
                    search_keywords=["game development best practices", "common errors", "debugging"],
                    priority="critical",
                    detected_at=datetime.now(),
                    auto_fix_possible=False
                )
                gaps.append(gap)
        
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ê²©ì°¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return gaps
    
    async def _analyze_error_pattern_gaps(self) -> List[KnowledgeGap]:
        """ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ê²©ì°¨ ë¶„ì„"""
        gaps = []
        
        try:
            # ê²½í—˜ ë°ì´í„°ì—ì„œ ì˜¤ë¥˜ íŒ¨í„´ ìˆ˜ì§‘
            experiences_dir = Path("experiences")
            error_patterns = Counter()
            
            for file_path in experiences_dir.rglob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        content = str(data).lower()
                        
                        # ì˜¤ë¥˜ í‚¤ì›Œë“œ ê²€ìƒ‰
                        if any(keyword in content for keyword in ['error', 'exception', 'failed', 'null']):
                            # êµ¬ì²´ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ ì‹ë³„
                            if 'null reference' in content or 'nullreferenceexception' in content:
                                error_patterns['null_reference'] += 1
                            if 'compile' in content and 'error' in content:
                                error_patterns['compilation'] += 1
                            if 'socket' in content and 'error' in content:
                                error_patterns['networking'] += 1
                            if 'godot' in content and 'error' in content:
                                error_patterns['godot_api'] += 1
                            if 'async' in content and 'error' in content:
                                error_patterns['async_await'] += 1
                except:
                    continue
            
            # ë¹ˆë²ˆí•œ ì˜¤ë¥˜ì— ëŒ€í•œ ê²©ì°¨ ìƒì„±
            for error_type, count in error_patterns.items():
                if count >= 3:  # 3íšŒ ì´ìƒ ë°œìƒí•œ ì˜¤ë¥˜
                    severity = min(count / 10.0, 1.0)  # ìµœëŒ€ 10íšŒë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
                    
                    gap = KnowledgeGap(
                        category=f"error_pattern_{error_type}",
                        severity=severity,
                        description=f"{error_type} ì˜¤ë¥˜ê°€ ë¹ˆë²ˆíˆ ë°œìƒí•¨ ({count}íšŒ)",
                        suggested_actions=[
                            f"{error_type} ì˜¤ë¥˜ í•´ê²° ë°©ë²• í•™ìŠµ",
                            f"{error_type} ì˜ˆë°© íŒ¨í„´ ìŠµë“",
                            f"{error_type} ê´€ë ¨ ëª¨ë²” ì‚¬ë¡€ ê²€ìƒ‰"
                        ],
                        search_keywords=[error_type, "solution", "best practices"],
                        priority="high" if count >= 5 else "medium",
                        detected_at=datetime.now(),
                        auto_fix_possible=error_type in [p.split('_')[0] for p in self.auto_fixable_patterns.keys()]
                    )
                    gaps.append(gap)
        
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ íŒ¨í„´ ê²©ì°¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return gaps
    
    async def _analyze_efficiency_gaps(self) -> List[KnowledgeGap]:
        """ì‹œê°„ íš¨ìœ¨ì„± ê¸°ë°˜ ê²©ì°¨ ë¶„ì„"""
        gaps = []
        
        try:
            # í•™ìŠµ ì„¸ì…˜ ì‹œê°„ ë¶„ì„
            continuous_learning_dir = Path("continuous_learning")
            if not continuous_learning_dir.exists():
                return gaps
            
            session_times = []
            recent_files = []
            
            # ìµœê·¼ ì„¸ì…˜ë“¤ì˜ ì‹œê°„ ë¶„ì„
            for file_path in continuous_learning_dir.rglob("*.json"):
                try:
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time > datetime.now() - timedelta(days=3):  # ìµœê·¼ 3ì¼
                        recent_files.append((file_path, file_time))
                except:
                    continue
            
            # ì„¸ì…˜ ê°„ê²© ë¶„ì„
            recent_files.sort(key=lambda x: x[1])
            
            if len(recent_files) >= 2:
                intervals = []
                for i in range(1, len(recent_files)):
                    interval = (recent_files[i][1] - recent_files[i-1][1]).total_seconds() / 3600  # hours
                    intervals.append(interval)
                
                avg_interval = sum(intervals) / len(intervals)
                
                # í•™ìŠµ ê°„ê²©ì´ ë„ˆë¬´ ê¸¸ë©´ ê²©ì°¨ë¡œ ê°„ì£¼
                if avg_interval > 24:  # 24ì‹œê°„ ì´ìƒ
                    gap = KnowledgeGap(
                        category="learning_frequency",
                        severity=min(avg_interval / 48, 1.0),  # 48ì‹œê°„ì„ ìµœëŒ€ë¡œ ì •ê·œí™”
                        description=f"í•™ìŠµ ì„¸ì…˜ ê°„ê²©ì´ ë„ˆë¬´ ê¹€ (í‰ê·  {avg_interval:.1f}ì‹œê°„)",
                        suggested_actions=[
                            "ë” ìì£¼ autoci learn ì‹¤í–‰",
                            "ìë™ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ í™œì„±í™”",
                            "ì§§ì€ ì„¸ì…˜ìœ¼ë¡œ í•™ìŠµ ë¹ˆë„ ì¦ê°€"
                        ],
                        search_keywords=["continuous learning", "spaced repetition", "learning schedule"],
                        priority="medium",
                        detected_at=datetime.now(),
                        auto_fix_possible=True
                    )
                    gaps.append(gap)
                
                # í•™ìŠµ ì‹œê°„ ì¼ê´€ì„± ë¶„ì„
                if len(intervals) >= 3:
                    interval_variance = np.var(intervals)
                    if interval_variance > 100:  # ë†’ì€ ë¶„ì‚°
                        gap = KnowledgeGap(
                            category="learning_consistency",
                            severity=min(interval_variance / 200, 1.0),
                            description="í•™ìŠµ íŒ¨í„´ì´ ì¼ê´€ì„±ì´ ì—†ìŒ",
                            suggested_actions=[
                                "ì •ê¸°ì ì¸ í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •",
                                "ì•Œë¦¼ ì‹œìŠ¤í…œ í™œìš©",
                                "í•™ìŠµ ë£¨í‹´ ê°œë°œ"
                            ],
                            search_keywords=["learning habits", "consistent study", "routine"],
                            priority="low",
                            detected_at=datetime.now(),
                            auto_fix_possible=True
                        )
                        gaps.append(gap)
        
        except Exception as e:
            logger.error(f"íš¨ìœ¨ì„± ê²©ì°¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return gaps
    
    async def auto_fill_gaps(self, gaps: List[KnowledgeGap]) -> Dict[str, Any]:
        """ìë™ìœ¼ë¡œ ë©”ê¿€ ìˆ˜ ìˆëŠ” ê²©ì°¨ë“¤ì„ ì²˜ë¦¬"""
        logger.info("ğŸ”§ ìë™ ê²©ì°¨ ë³´ì™„ ì‹œì‘")
        
        filled_count = 0
        auto_actions = []
        manual_actions = []
        
        for gap in gaps:
            if gap.auto_fix_possible:
                try:
                    success = await self._attempt_auto_fix(gap)
                    if success:
                        filled_count += 1
                        auto_actions.append({
                            'gap': gap.category,
                            'action': 'auto_fixed',
                            'timestamp': datetime.now().isoformat()
                        })
                        logger.info(f"âœ… ìë™ ë³´ì™„ ì„±ê³µ: {gap.category}")
                    else:
                        manual_actions.append({
                            'gap': gap.category,
                            'reason': 'auto_fix_failed',
                            'suggested_actions': gap.suggested_actions
                        })
                except Exception as e:
                    logger.error(f"ìë™ ë³´ì™„ ì‹¤íŒ¨ {gap.category}: {e}")
                    manual_actions.append({
                        'gap': gap.category,
                        'reason': 'exception',
                        'error': str(e)
                    })
            else:
                manual_actions.append({
                    'gap': gap.category,
                    'reason': 'manual_intervention_required',
                    'suggested_actions': gap.suggested_actions
                })
        
        result = {
            'total_gaps': len(gaps),
            'auto_filled': filled_count,
            'auto_actions': auto_actions,
            'manual_actions': manual_actions,
            'success_rate': filled_count / len(gaps) if gaps else 0.0,
            'timestamp': datetime.now().isoformat()
        }
        
        # ê²°ê³¼ ì €ì¥
        result_file = self.filled_gaps_dir / f"auto_fill_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ¯ ìë™ ê²©ì°¨ ë³´ì™„ ì™„ë£Œ: {filled_count}/{len(gaps)} ì„±ê³µ")
        
        return result
    
    async def _attempt_auto_fix(self, gap: KnowledgeGap) -> bool:
        """ê°œë³„ ê²©ì°¨ ìë™ ë³´ì™„ ì‹œë„"""
        try:
            if gap.category.startswith('learning_data_'):
                # í•™ìŠµ ë°ì´í„° ë¶€ì¡± -> ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ê°€
                from modules.intelligent_guardian_system import get_guardian_system
                guardian = get_guardian_system()
                
                for keyword in gap.search_keywords:
                    await guardian.search_queue.put(f"{keyword} ê³ ê¸‰ ê¸°ë²•")
                
                return True
                
            elif gap.category == 'project_creation_frequency':
                # í”„ë¡œì íŠ¸ ìƒì„± ë¹ˆë„ ë¶€ì¡± -> ìë™ ìƒì„± ì œì•ˆ
                suggestion_file = self.filled_gaps_dir / "auto_create_suggestion.json"
                suggestion = {
                    'timestamp': datetime.now().isoformat(),
                    'suggestion': 'autoci create ìë™ ì‹¤í–‰ ê³ ë ¤',
                    'recommended_types': ['platformer', 'puzzle', 'rpg'],
                    'schedule': 'daily'
                }
                
                with open(suggestion_file, 'w', encoding='utf-8') as f:
                    json.dump(suggestion, f, indent=2, ensure_ascii=False)
                
                return True
                
            elif gap.category == 'learning_frequency':
                # í•™ìŠµ ë¹ˆë„ ë¶€ì¡± -> ìŠ¤ì¼€ì¤„ë§ ì œì•ˆ
                schedule_file = self.filled_gaps_dir / "learning_schedule.json"
                schedule = {
                    'timestamp': datetime.now().isoformat(),
                    'recommended_interval': '12 hours',
                    'auto_reminder': True,
                    'suggested_times': ['09:00', '21:00']
                }
                
                with open(schedule_file, 'w', encoding='utf-8') as f:
                    json.dump(schedule, f, indent=2, ensure_ascii=False)
                
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"ìë™ ë³´ì™„ ì‹œë„ ì˜¤ë¥˜: {e}")
            return False
    
    async def _save_gap_analysis(self, gaps: List[KnowledgeGap]):
        """ê²©ì°¨ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'total_gaps': len(gaps),
            'critical_gaps': len([g for g in gaps if g.priority == 'critical']),
            'high_priority_gaps': len([g for g in gaps if g.priority == 'high']),
            'auto_fixable_gaps': len([g for g in gaps if g.auto_fix_possible]),
            'gaps': [
                {
                    'category': gap.category,
                    'severity': gap.severity,
                    'description': gap.description,
                    'suggested_actions': gap.suggested_actions,
                    'search_keywords': gap.search_keywords,
                    'priority': gap.priority,
                    'detected_at': gap.detected_at.isoformat(),
                    'auto_fix_possible': gap.auto_fix_possible
                }
                for gap in gaps
            ]
        }
        
        analysis_file = self.gaps_dir / f"gap_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š ê²©ì°¨ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_file}")
    
    async def generate_learning_recommendations(self, gaps: List[KnowledgeGap]) -> Dict[str, Any]:
        """ê²©ì°¨ ê¸°ë°˜ í•™ìŠµ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = {
            'timestamp': datetime.now().isoformat(),
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_goals': [],
            'resource_suggestions': [],
            'priority_order': []
        }
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        critical_gaps = [g for g in gaps if g.priority == 'critical']
        high_gaps = [g for g in gaps if g.priority == 'high']
        medium_gaps = [g for g in gaps if g.priority == 'medium']
        
        # ì¦‰ì‹œ ì¡°ì¹˜ (Critical)
        for gap in critical_gaps:
            recommendations['immediate_actions'].extend(gap.suggested_actions)
        
        # ë‹¨ê¸° ëª©í‘œ (High)
        for gap in high_gaps:
            recommendations['short_term_goals'].append({
                'goal': f"{gap.category} ê°œì„ ",
                'actions': gap.suggested_actions,
                'keywords': gap.search_keywords
            })
        
        # ì¥ê¸° ëª©í‘œ (Medium)
        for gap in medium_gaps:
            recommendations['long_term_goals'].append({
                'goal': f"{gap.category} ì™„ì„±",
                'description': gap.description
            })
        
        # ë¦¬ì†ŒìŠ¤ ì œì•ˆ
        all_keywords = set()
        for gap in gaps:
            all_keywords.update(gap.search_keywords)
        
        recommendations['resource_suggestions'] = [
            f"{keyword} íŠœí† ë¦¬ì–¼" for keyword in list(all_keywords)[:10]
        ]
        
        # ìš°ì„ ìˆœìœ„ ìˆœì„œ
        recommendations['priority_order'] = [
            f"{gap.priority}: {gap.category}" for gap in gaps[:10]
        ]
        
        return recommendations

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_gap_intelligence = None

def get_gap_filling_intelligence() -> GapFillingIntelligence:
    """ê²©ì°¨ ë³´ì™„ ì§€ëŠ¥ ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _gap_intelligence
    if _gap_intelligence is None:
        _gap_intelligence = GapFillingIntelligence()
    return _gap_intelligence