"""
AutoCI Í≤©Ï∞® Î≥¥ÏôÑ ÏßÄÎä• ÏãúÏä§ÌÖú
autoci learnÍ≥º autoci createÏóêÏÑú Î∂ÄÏ°±Ìïú Î∂ÄÎ∂ÑÏùÑ ÏûêÎèôÏúºÎ°ú Í∞êÏßÄÌïòÍ≥† Î©îÍøîÏ£ºÎäî ÌïµÏã¨ Î™®Îìà
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass
try:
    import numpy as np
except ImportError:
    # numpyÍ∞Ä ÏóÜÏùÑ Îïå ÎåÄÏ≤¥ Íµ¨ÌòÑ
    class np:
        @staticmethod
        def random():
            import random
            class Random:
                @staticmethod
                def randint(a, b):
                    return random.randint(a, b)
                @staticmethod
                def uniform(a, b):
                    return random.uniform(a, b)
            return Random()
        
        @staticmethod
        def var(data):
            if not data:
                return 0
            mean = sum(data) / len(data)
            return sum((x - mean) ** 2 for x in data) / len(data)
from collections import defaultdict, Counter
import re

logger = logging.getLogger(__name__)

@dataclass
class KnowledgeGap:
    """ÏßÄÏãù Í≤©Ï∞® Ï†ïÎ≥¥"""
    category: str
    severity: float  # 0.0 ~ 1.0
    description: str
    suggested_actions: List[str]
    search_keywords: List[str]
    priority: str  # 'low', 'medium', 'high', 'critical'
    detected_at: datetime
    auto_fix_possible: bool

@dataclass
class LearningDeficiency:
    """ÌïôÏäµ Î∂ÄÏ°± Î∂ÄÎ∂Ñ"""
    skill_area: str
    deficiency_score: float
    evidence: List[str]
    recommended_resources: List[str]
    estimated_learning_time: int  # minutes
    
class GapFillingIntelligence:
    """Í≤©Ï∞® Î≥¥ÏôÑ ÏßÄÎä• ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        self.gaps_dir = Path("experiences/knowledge_gaps")
        self.gaps_dir.mkdir(parents=True, exist_ok=True)
        
        self.filled_gaps_dir = Path("experiences/filled_gaps")
        self.filled_gaps_dir.mkdir(parents=True, exist_ok=True)
        
        # ÏßÄÏãù ÏòÅÏó≠ Ï†ïÏùò
        self.knowledge_areas = {
            "csharp_basics": {
                "keywords": ["class", "method", "property", "namespace", "using"],
                "min_threshold": 10,
                "importance": 0.9
            },
            "csharp_advanced": {
                "keywords": ["async", "await", "generic", "linq", "delegate"],
                "min_threshold": 5,
                "importance": 0.8
            },
            "godot_basics": {
                "keywords": ["node", "scene", "signal", "script", "_ready"],
                "min_threshold": 15,
                "importance": 0.9
            },
            "godot_advanced": {
                "keywords": ["shader", "gdnative", "multiplayer", "physics", "animation"],
                "min_threshold": 8,
                "importance": 0.7
            },
            "socketio_networking": {
                "keywords": ["socket", "emit", "on", "connect", "disconnect"],
                "min_threshold": 6,
                "importance": 0.6
            },
            "pytorch_ai": {
                "keywords": ["tensor", "model", "train", "loss", "optimizer"],
                "min_threshold": 5,
                "importance": 0.7
            },
            "game_design": {
                "keywords": ["gameplay", "mechanics", "level", "ui", "player"],
                "min_threshold": 12,
                "importance": 0.8
            }
        }
        
        # ÏûêÎèô ÏàòÏ†ï Í∞ÄÎä•Ìïú Î¨∏Ï†ú Ìå®ÌÑ¥
        self.auto_fixable_patterns = {
            "missing_using_statements": {
                "pattern": r"The type or namespace name '(\w+)' could not be found",
                "fix_template": "using {namespace};"
            },
            "missing_async_await": {
                "pattern": r"Cannot await|async method",
                "fix_template": "async/await Ìå®ÌÑ¥ Ï†ÅÏö© ÌïÑÏöî"
            },
            "godot_node_access": {
                "pattern": r"GetNode.*null|Node not found",
                "fix_template": "ÎÖ∏Îìú Í≤ΩÎ°ú ÌôïÏù∏ Î∞è null Ï≤¥ÌÅ¨ Ï∂îÍ∞Ä"
            }
        }
        
        logger.info("üéØ Í≤©Ï∞® Î≥¥ÏôÑ ÏßÄÎä• ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    async def analyze_comprehensive_gaps(self) -> List[KnowledgeGap]:
        """Ï¢ÖÌï©Ï†ÅÏù∏ ÏßÄÏãù Í≤©Ï∞® Î∂ÑÏÑù"""
        logger.info("üîç Ï¢ÖÌï©Ï†ÅÏù∏ ÏßÄÏãù Í≤©Ï∞® Î∂ÑÏÑù ÏãúÏûë")
        
        gaps = []
        
        # 1. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù
        learning_gaps = await self._analyze_learning_data_gaps()
        gaps.extend(learning_gaps)
        
        # 2. ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ± Í≤∞Í≥º Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù
        project_gaps = await self._analyze_project_creation_gaps()
        gaps.extend(project_gaps)
        
        # 3. Ïò§Î•ò Ìå®ÌÑ¥ Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù
        error_gaps = await self._analyze_error_pattern_gaps()
        gaps.extend(error_gaps)
        
        # 4. ÏãúÍ∞Ñ Ìö®Ïú®ÏÑ± Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù
        efficiency_gaps = await self._analyze_efficiency_gaps()
        gaps.extend(efficiency_gaps)
        
        # Í≤©Ï∞® Ïö∞ÏÑ†ÏàúÏúÑ Ï†ïÎ†¨
        gaps.sort(key=lambda g: (g.severity, g.priority), reverse=True)
        
        logger.info(f"üéØ Ï¥ù {len(gaps)}Í∞úÏùò ÏßÄÏãù Í≤©Ï∞® Í∞êÏßÄÎê®")
        
        # Í≤©Ï∞® Ï†ïÎ≥¥ Ï†ÄÏû•
        await self._save_gap_analysis(gaps)
        
        return gaps
    
    async def _analyze_learning_data_gaps(self) -> List[KnowledgeGap]:
        """ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù"""
        gaps = []
        
        try:
            # Ïó∞ÏÜç ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù
            continuous_learning_dir = Path("continuous_learning")
            if not continuous_learning_dir.exists():
                return gaps
            
            # Í∞Å ÏßÄÏãù ÏòÅÏó≠Î≥Ñ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            area_data = defaultdict(list)
            
            for file_path in continuous_learning_dir.rglob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        content = str(data).lower()
                        
                        # Í∞Å ÏßÄÏãù ÏòÅÏó≠Ïóê Îß§Ïπ≠
                        for area, config in self.knowledge_areas.items():
                            keyword_count = sum(1 for keyword in config["keywords"] 
                                             if keyword in content)
                            if keyword_count > 0:
                                area_data[area].append({
                                    'file': str(file_path),
                                    'keyword_count': keyword_count,
                                    'content_length': len(content)
                                })
                except:
                    continue
            
            # Í≤©Ï∞® Î∂ÑÏÑù
            for area, config in self.knowledge_areas.items():
                data_points = area_data.get(area, [])
                total_keywords = sum(d['keyword_count'] for d in data_points)
                
                if total_keywords < config["min_threshold"]:
                    severity = 1.0 - (total_keywords / config["min_threshold"])
                    severity = min(severity * config["importance"], 1.0)
                    
                    gap = KnowledgeGap(
                        category=f"learning_data_{area}",
                        severity=severity,
                        description=f"{area} ÏòÅÏó≠Ïùò ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°± (ÌòÑÏû¨: {total_keywords}, ÌïÑÏöî: {config['min_threshold']})",
                        suggested_actions=[
                            f"{area} Í¥ÄÎ†® autoci learn ÏÑ∏ÏÖò Ï¶ùÍ∞Ä",
                            f"{area} Ï†ÑÏö© ÌïôÏäµ ÏûêÎ£å Í≤ÄÏÉâ",
                            f"{area} Ïã§Ïäµ ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ±"
                        ],
                        search_keywords=config["keywords"],
                        priority="high" if severity > 0.7 else "medium" if severity > 0.4 else "low",
                        detected_at=datetime.now(),
                        auto_fix_possible=True
                    )
                    gaps.append(gap)
        
        except Exception as e:
            logger.error(f"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í≤©Ï∞® Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return gaps
    
    async def _analyze_project_creation_gaps(self) -> List[KnowledgeGap]:
        """ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ± Í≤∞Í≥º Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù"""
        gaps = []
        
        try:
            game_projects_dir = Path("game_projects")
            if not game_projects_dir.exists():
                return gaps
            
            # ÏµúÍ∑º ÌîÑÎ°úÏ†ùÌä∏ Î∂ÑÏÑù
            recent_projects = []
            one_week_ago = datetime.now() - timedelta(days=7)
            
            for project_dir in game_projects_dir.iterdir():
                if project_dir.is_dir():
                    creation_time = datetime.fromtimestamp(project_dir.stat().st_ctime)
                    if creation_time > one_week_ago:
                        recent_projects.append(project_dir)
            
            if len(recent_projects) < 3:
                gap = KnowledgeGap(
                    category="project_creation_frequency",
                    severity=0.8,
                    description="ÏµúÍ∑º 1Ï£ºÏùºÍ∞Ñ ÏÉùÏÑ±Îêú ÌîÑÎ°úÏ†ùÌä∏Í∞Ä Î∂ÄÏ°±Ìï®",
                    suggested_actions=[
                        "Îçî ÏûêÏ£º autoci create Ïã§Ìñâ",
                        "Îã§ÏñëÌïú Í≤åÏûÑ ÌÉÄÏûÖ Ïã§Ìóò",
                        "ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ± ÏûêÎèôÌôî Í≥†Î†§"
                    ],
                    search_keywords=["game development", "project templates", "rapid prototyping"],
                    priority="high",
                    detected_at=datetime.now(),
                    auto_fix_possible=True
                )
                gaps.append(gap)
            
            # ÌîÑÎ°úÏ†ùÌä∏ ÌíàÏßà Î∂ÑÏÑù
            failed_projects = 0
            for project_dir in recent_projects:
                config_file = project_dir / "config.json"
                if config_file.exists():
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                            if config.get('status') == 'failed' or config.get('quality_score', 0) < 0.5:
                                failed_projects += 1
                    except:
                        failed_projects += 1
                else:
                    failed_projects += 1
            
            if failed_projects > len(recent_projects) * 0.5:  # 50% Ïù¥ÏÉÅ Ïã§Ìå®
                gap = KnowledgeGap(
                    category="project_quality",
                    severity=0.9,
                    description=f"ÌîÑÎ°úÏ†ùÌä∏ Ïã§Ìå®Ïú®Ïù¥ ÎÜíÏùå ({failed_projects}/{len(recent_projects)})",
                    suggested_actions=[
                        "Í∏∞Ï¥à ÌïôÏäµ Í∞ïÌôî",
                        "Îã®ÏàúÌïú ÌîÑÎ°úÏ†ùÌä∏Î∂ÄÌÑ∞ ÏãúÏûë",
                        "Ïò§Î•ò Ìå®ÌÑ¥ Î∂ÑÏÑù Î∞è Ìï¥Í≤∞"
                    ],
                    search_keywords=["game development best practices", "common errors", "debugging"],
                    priority="critical",
                    detected_at=datetime.now(),
                    auto_fix_possible=False
                )
                gaps.append(gap)
        
        except Exception as e:
            logger.error(f"ÌîÑÎ°úÏ†ùÌä∏ Í≤©Ï∞® Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return gaps
    
    async def _analyze_error_pattern_gaps(self) -> List[KnowledgeGap]:
        """Ïò§Î•ò Ìå®ÌÑ¥ Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù"""
        gaps = []
        
        try:
            # Í≤ΩÌóò Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïò§Î•ò Ìå®ÌÑ¥ ÏàòÏßë
            experiences_dir = Path("experiences")
            error_patterns = Counter()
            
            for file_path in experiences_dir.rglob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        content = str(data).lower()
                        
                        # Ïò§Î•ò ÌÇ§ÏõåÎìú Í≤ÄÏÉâ
                        if any(keyword in content for keyword in ['error', 'exception', 'failed', 'null']):
                            # Íµ¨Ï≤¥Ï†ÅÏù∏ Ïò§Î•ò Ìå®ÌÑ¥ ÏãùÎ≥Ñ
                            if 'null reference' in content or 'nullreferenceexception' in content:
                                error_patterns['null_reference'] += 1
                            if 'compile' in content and 'error' in content:
                                error_patterns['compilation'] += 1
                            if 'socket' in content and 'error' in content:
                                error_patterns['networking'] += 1
                            if 'godot' in content and 'error' in content:
                                error_patterns['godot_api'] += 1
                            if 'async' in content and 'error' in content:
                                error_patterns['async_await'] += 1
                except:
                    continue
            
            # ÎπàÎ≤àÌïú Ïò§Î•òÏóê ÎåÄÌïú Í≤©Ï∞® ÏÉùÏÑ±
            for error_type, count in error_patterns.items():
                if count >= 3:  # 3Ìöå Ïù¥ÏÉÅ Î∞úÏÉùÌïú Ïò§Î•ò
                    severity = min(count / 10.0, 1.0)  # ÏµúÎåÄ 10ÌöåÎ•º 1.0ÏúºÎ°ú Ï†ïÍ∑úÌôî
                    
                    gap = KnowledgeGap(
                        category=f"error_pattern_{error_type}",
                        severity=severity,
                        description=f"{error_type} Ïò§Î•òÍ∞Ä ÎπàÎ≤àÌûà Î∞úÏÉùÌï® ({count}Ìöå)",
                        suggested_actions=[
                            f"{error_type} Ïò§Î•ò Ìï¥Í≤∞ Î∞©Î≤ï ÌïôÏäµ",
                            f"{error_type} ÏòàÎ∞© Ìå®ÌÑ¥ ÏäµÎìù",
                            f"{error_type} Í¥ÄÎ†® Î™®Î≤î ÏÇ¨Î°Ä Í≤ÄÏÉâ"
                        ],
                        search_keywords=[error_type, "solution", "best practices"],
                        priority="high" if count >= 5 else "medium",
                        detected_at=datetime.now(),
                        auto_fix_possible=error_type in [p.split('_')[0] for p in self.auto_fixable_patterns.keys()]
                    )
                    gaps.append(gap)
        
        except Exception as e:
            logger.error(f"Ïò§Î•ò Ìå®ÌÑ¥ Í≤©Ï∞® Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return gaps
    
    async def _analyze_efficiency_gaps(self) -> List[KnowledgeGap]:
        """ÏãúÍ∞Ñ Ìö®Ïú®ÏÑ± Í∏∞Î∞ò Í≤©Ï∞® Î∂ÑÏÑù"""
        gaps = []
        
        try:
            # ÌïôÏäµ ÏÑ∏ÏÖò ÏãúÍ∞Ñ Î∂ÑÏÑù
            continuous_learning_dir = Path("continuous_learning")
            if not continuous_learning_dir.exists():
                return gaps
            
            session_times = []
            recent_files = []
            
            # ÏµúÍ∑º ÏÑ∏ÏÖòÎì§Ïùò ÏãúÍ∞Ñ Î∂ÑÏÑù
            for file_path in continuous_learning_dir.rglob("*.json"):
                try:
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time > datetime.now() - timedelta(days=3):  # ÏµúÍ∑º 3Ïùº
                        recent_files.append((file_path, file_time))
                except:
                    continue
            
            # ÏÑ∏ÏÖò Í∞ÑÍ≤© Î∂ÑÏÑù
            recent_files.sort(key=lambda x: x[1])
            
            if len(recent_files) >= 2:
                intervals = []
                for i in range(1, len(recent_files)):
                    interval = (recent_files[i][1] - recent_files[i-1][1]).total_seconds() / 3600  # hours
                    intervals.append(interval)
                
                avg_interval = sum(intervals) / len(intervals)
                
                # ÌïôÏäµ Í∞ÑÍ≤©Ïù¥ ÎÑàÎ¨¥ Í∏∏Î©¥ Í≤©Ï∞®Î°ú Í∞ÑÏ£º
                if avg_interval > 24:  # 24ÏãúÍ∞Ñ Ïù¥ÏÉÅ
                    gap = KnowledgeGap(
                        category="learning_frequency",
                        severity=min(avg_interval / 48, 1.0),  # 48ÏãúÍ∞ÑÏùÑ ÏµúÎåÄÎ°ú Ï†ïÍ∑úÌôî
                        description=f"ÌïôÏäµ ÏÑ∏ÏÖò Í∞ÑÍ≤©Ïù¥ ÎÑàÎ¨¥ ÍπÄ (ÌèâÍ∑† {avg_interval:.1f}ÏãúÍ∞Ñ)",
                        suggested_actions=[
                            "Îçî ÏûêÏ£º autoci learn Ïã§Ìñâ",
                            "ÏûêÎèô ÌïôÏäµ Ïä§ÏºÄÏ§ÑÎßÅ ÌôúÏÑ±Ìôî",
                            "ÏßßÏùÄ ÏÑ∏ÏÖòÏúºÎ°ú ÌïôÏäµ ÎπàÎèÑ Ï¶ùÍ∞Ä"
                        ],
                        search_keywords=["continuous learning", "spaced repetition", "learning schedule"],
                        priority="medium",
                        detected_at=datetime.now(),
                        auto_fix_possible=True
                    )
                    gaps.append(gap)
                
                # ÌïôÏäµ ÏãúÍ∞Ñ ÏùºÍ¥ÄÏÑ± Î∂ÑÏÑù
                if len(intervals) >= 3:
                    interval_variance = np.var(intervals)
                    if interval_variance > 100:  # ÎÜíÏùÄ Î∂ÑÏÇ∞
                        gap = KnowledgeGap(
                            category="learning_consistency",
                            severity=min(interval_variance / 200, 1.0),
                            description="ÌïôÏäµ Ìå®ÌÑ¥Ïù¥ ÏùºÍ¥ÄÏÑ±Ïù¥ ÏóÜÏùå",
                            suggested_actions=[
                                "Ï†ïÍ∏∞Ï†ÅÏù∏ ÌïôÏäµ Ïä§ÏºÄÏ§Ñ ÏÑ§Ï†ï",
                                "ÏïåÎ¶º ÏãúÏä§ÌÖú ÌôúÏö©",
                                "ÌïôÏäµ Î£®Ìã¥ Í∞úÎ∞ú"
                            ],
                            search_keywords=["learning habits", "consistent study", "routine"],
                            priority="low",
                            detected_at=datetime.now(),
                            auto_fix_possible=True
                        )
                        gaps.append(gap)
        
        except Exception as e:
            logger.error(f"Ìö®Ïú®ÏÑ± Í≤©Ï∞® Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return gaps
    
    async def auto_fill_gaps(self, gaps: List[KnowledgeGap]) -> Dict[str, Any]:
        """ÏûêÎèôÏúºÎ°ú Î©îÍøÄ Ïàò ÏûàÎäî Í≤©Ï∞®Îì§ÏùÑ Ï≤òÎ¶¨"""
        logger.info("üîß ÏûêÎèô Í≤©Ï∞® Î≥¥ÏôÑ ÏãúÏûë")
        
        filled_count = 0
        auto_actions = []
        manual_actions = []
        
        for gap in gaps:
            if gap.auto_fix_possible:
                try:
                    success = await self._attempt_auto_fix(gap)
                    if success:
                        filled_count += 1
                        auto_actions.append({
                            'gap': gap.category,
                            'action': 'auto_fixed',
                            'timestamp': datetime.now().isoformat()
                        })
                        logger.info(f"‚úÖ ÏûêÎèô Î≥¥ÏôÑ ÏÑ±Í≥µ: {gap.category}")
                    else:
                        manual_actions.append({
                            'gap': gap.category,
                            'reason': 'auto_fix_failed',
                            'suggested_actions': gap.suggested_actions
                        })
                except Exception as e:
                    logger.error(f"ÏûêÎèô Î≥¥ÏôÑ Ïã§Ìå® {gap.category}: {e}")
                    manual_actions.append({
                        'gap': gap.category,
                        'reason': 'exception',
                        'error': str(e)
                    })
            else:
                manual_actions.append({
                    'gap': gap.category,
                    'reason': 'manual_intervention_required',
                    'suggested_actions': gap.suggested_actions
                })
        
        result = {
            'total_gaps': len(gaps),
            'auto_filled': filled_count,
            'auto_actions': auto_actions,
            'manual_actions': manual_actions,
            'success_rate': filled_count / len(gaps) if gaps else 0.0,
            'timestamp': datetime.now().isoformat()
        }
        
        # Í≤∞Í≥º Ï†ÄÏû•
        result_file = self.filled_gaps_dir / f"auto_fill_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üéØ ÏûêÎèô Í≤©Ï∞® Î≥¥ÏôÑ ÏôÑÎ£å: {filled_count}/{len(gaps)} ÏÑ±Í≥µ")
        
        return result
    
    async def _attempt_auto_fix(self, gap: KnowledgeGap) -> bool:
        """Í∞úÎ≥Ñ Í≤©Ï∞® ÏûêÎèô Î≥¥ÏôÑ ÏãúÎèÑ"""
        try:
            if gap.category.startswith('learning_data_'):
                # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°± -> Í≤ÄÏÉâ ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä
                from modules.intelligent_guardian_system import get_guardian_system
                guardian = get_guardian_system()
                
                for keyword in gap.search_keywords:
                    await guardian.search_queue.put(f"{keyword} Í≥†Í∏â Í∏∞Î≤ï")
                
                return True
                
            elif gap.category == 'project_creation_frequency':
                # ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ± ÎπàÎèÑ Î∂ÄÏ°± -> ÏûêÎèô ÏÉùÏÑ± Ï†úÏïà
                suggestion_file = self.filled_gaps_dir / "auto_create_suggestion.json"
                suggestion = {
                    'timestamp': datetime.now().isoformat(),
                    'suggestion': 'autoci create ÏûêÎèô Ïã§Ìñâ Í≥†Î†§',
                    'recommended_types': ['platformer', 'puzzle', 'rpg'],
                    'schedule': 'daily'
                }
                
                with open(suggestion_file, 'w', encoding='utf-8') as f:
                    json.dump(suggestion, f, indent=2, ensure_ascii=False)
                
                return True
                
            elif gap.category == 'learning_frequency':
                # ÌïôÏäµ ÎπàÎèÑ Î∂ÄÏ°± -> Ïä§ÏºÄÏ§ÑÎßÅ Ï†úÏïà
                schedule_file = self.filled_gaps_dir / "learning_schedule.json"
                schedule = {
                    'timestamp': datetime.now().isoformat(),
                    'recommended_interval': '12 hours',
                    'auto_reminder': True,
                    'suggested_times': ['09:00', '21:00']
                }
                
                with open(schedule_file, 'w', encoding='utf-8') as f:
                    json.dump(schedule, f, indent=2, ensure_ascii=False)
                
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"ÏûêÎèô Î≥¥ÏôÑ ÏãúÎèÑ Ïò§Î•ò: {e}")
            return False
    
    async def _save_gap_analysis(self, gaps: List[KnowledgeGap]):
        """Í≤©Ï∞® Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•"""
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'total_gaps': len(gaps),
            'critical_gaps': len([g for g in gaps if g.priority == 'critical']),
            'high_priority_gaps': len([g for g in gaps if g.priority == 'high']),
            'auto_fixable_gaps': len([g for g in gaps if g.auto_fix_possible]),
            'gaps': [
                {
                    'category': gap.category,
                    'severity': gap.severity,
                    'description': gap.description,
                    'suggested_actions': gap.suggested_actions,
                    'search_keywords': gap.search_keywords,
                    'priority': gap.priority,
                    'detected_at': gap.detected_at.isoformat(),
                    'auto_fix_possible': gap.auto_fix_possible
                }
                for gap in gaps
            ]
        }
        
        analysis_file = self.gaps_dir / f"gap_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìä Í≤©Ï∞® Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•: {analysis_file}")
    
    async def generate_learning_recommendations(self, gaps: List[KnowledgeGap]) -> Dict[str, Any]:
        """Í≤©Ï∞® Í∏∞Î∞ò ÌïôÏäµ Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = {
            'timestamp': datetime.now().isoformat(),
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_goals': [],
            'resource_suggestions': [],
            'priority_order': []
        }
        
        # Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Î∂ÑÎ•ò
        critical_gaps = [g for g in gaps if g.priority == 'critical']
        high_gaps = [g for g in gaps if g.priority == 'high']
        medium_gaps = [g for g in gaps if g.priority == 'medium']
        
        # Ï¶âÏãú Ï°∞Ïπò (Critical)
        for gap in critical_gaps:
            recommendations['immediate_actions'].extend(gap.suggested_actions)
        
        # Îã®Í∏∞ Î™©Ìëú (High)
        for gap in high_gaps:
            recommendations['short_term_goals'].append({
                'goal': f"{gap.category} Í∞úÏÑ†",
                'actions': gap.suggested_actions,
                'keywords': gap.search_keywords
            })
        
        # Ïû•Í∏∞ Î™©Ìëú (Medium)
        for gap in medium_gaps:
            recommendations['long_term_goals'].append({
                'goal': f"{gap.category} ÏôÑÏÑ±",
                'description': gap.description
            })
        
        # Î¶¨ÏÜåÏä§ Ï†úÏïà
        all_keywords = set()
        for gap in gaps:
            all_keywords.update(gap.search_keywords)
        
        recommendations['resource_suggestions'] = [
            f"{keyword} ÌäúÌÜ†Î¶¨Ïñº" for keyword in list(all_keywords)[:10]
        ]
        
        # Ïö∞ÏÑ†ÏàúÏúÑ ÏàúÏÑú
        recommendations['priority_order'] = [
            f"{gap.priority}: {gap.category}" for gap in gaps[:10]
        ]
        
        return recommendations

# Ïã±Í∏ÄÌÜ§ Ïù∏Ïä§ÌÑ¥Ïä§
_gap_intelligence = None

def get_gap_filling_intelligence() -> GapFillingIntelligence:
    """Í≤©Ï∞® Î≥¥ÏôÑ ÏßÄÎä• ÏãúÏä§ÌÖú Ïã±Í∏ÄÌÜ§ Î∞òÌôò"""
    global _gap_intelligence
    if _gap_intelligence is None:
        _gap_intelligence = GapFillingIntelligence()
    return _gap_intelligence