"""
AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì‹œìŠ¤í…œ
24ì‹œê°„ ì§€ì†ì ìœ¼ë¡œ autoci learnê³¼ autoci createë¥¼ ê°ì‹œí•˜ê³ ,
ë¶€ì¡±í•œ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ ë©”ê¿”ì£¼ëŠ” í•µì‹¬ ì‹œìŠ¤í…œ
"""

import asyncio
import json
import logging
import time
import psutil
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import threading
try:
    import torch
except ImportError:
    torch = None

try:
    import numpy as np
except ImportError:
    # numpyê°€ ì—†ì„ ë•Œ ëŒ€ì²´ êµ¬í˜„
    class np:
        @staticmethod
        def random():
            import random
            class Random:
                @staticmethod
                def randint(a, b):
                    return random.randint(a, b)
                @staticmethod
                def uniform(a, b):
                    return random.uniform(a, b)
            return Random()
from concurrent.futures import ThreadPoolExecutor
import requests
from urllib.parse import quote

logger = logging.getLogger(__name__)

@dataclass
class LearningProgress:
    """í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì """
    session_id: str
    start_time: datetime
    last_activity: datetime
    total_learning_time: float
    quality_score: float
    repetitive_patterns: List[str]
    knowledge_gaps: List[str]
    learning_efficiency: float
    next_recommended_action: str

@dataclass
class SystemMonitoringState:
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ìƒíƒœ"""
    autoci_learn_running: bool
    autoci_create_running: bool
    autoci_resume_running: bool
    last_learn_session: Optional[datetime]
    last_create_session: Optional[datetime]
    last_resume_session: Optional[datetime]
    total_monitored_time: float
    detected_repetitions: int
    filled_knowledge_gaps: int
    pytorch_training_sessions: int
    godot_projects_monitored: List[str]

class IntelligentGuardianSystem:
    """AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ - 24ì‹œê°„ ê°ì‹œ ë° ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.is_running = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.search_thread: Optional[threading.Thread] = None
        self.pytorch_thread: Optional[threading.Thread] = None
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
        self.guardian_dir = Path("experiences/guardian_system")
        self.guardian_dir.mkdir(parents=True, exist_ok=True)
        
        self.pytorch_datasets_dir = Path("experiences/pytorch_datasets")
        self.pytorch_datasets_dir.mkdir(parents=True, exist_ok=True)
        
        self.knowledge_base_dir = Path("experiences/knowledge_base")
        self.knowledge_base_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì´ˆê¸°í™”
        self.monitoring_state = SystemMonitoringState(
            autoci_learn_running=False,
            autoci_create_running=False,
            autoci_resume_running=False,
            last_learn_session=None,
            last_create_session=None,
            last_resume_session=None,
            total_monitored_time=0.0,
            detected_repetitions=0,
            filled_knowledge_gaps=0,
            pytorch_training_sessions=0,
            godot_projects_monitored=[]
        )
        
        # í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì 
        self.current_learning_progress: Optional[LearningProgress] = None
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ í
        self.search_queue = asyncio.Queue()
        
        # PyTorch í•™ìŠµ í
        self.pytorch_queue = asyncio.Queue()
        
        logger.info("ğŸ›¡ï¸ AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def start_guardian_mode(self):
        """24ì‹œê°„ ê°€ë””ì–¸ ëª¨ë“œ ì‹œì‘"""
        if self.is_running:
            logger.warning("ê°€ë””ì–¸ ì‹œìŠ¤í…œì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_running = True
        logger.info("ğŸ›¡ï¸ AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 70)
        print("ğŸ›¡ï¸ AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì‹œìŠ¤í…œ í™œì„±í™”")
        print("   - 24ì‹œê°„ ì§€ì†ì  ê°ì‹œ ì‹œì‘")
        print("   - ë°˜ë³µì  í•™ìŠµ ì§€ì–‘ ì‹œìŠ¤í…œ í™œì„±í™”")
        print("   - ì§€ì†ì  ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹œì‘")
        print("   - PyTorch ìë™ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 70)
        
        # ë³‘ë ¬ë¡œ ëª¨ë“  ì‹œìŠ¤í…œ ì‹œì‘
        await asyncio.gather(
            self._start_process_monitoring(),
            self._start_repetition_prevention(),
            self._start_continuous_search(),
            self._start_pytorch_training(),
            self._start_knowledge_gap_detection(),
            self._start_human_advisory()
        )
    
    async def _start_process_monitoring(self):
        """autoci learn/create í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§"""
        logger.info("ğŸ” í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        while self.is_running:
            try:
                # ì‹¤í–‰ ì¤‘ì¸ autoci í”„ë¡œì„¸ìŠ¤ í™•ì¸
                autoci_processes = []
                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                    try:
                        if proc.info['cmdline'] and any('autoci' in cmd for cmd in proc.info['cmdline']):
                            autoci_processes.append(proc.info)
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                # learn/create í”„ë¡œì„¸ìŠ¤ í™•ì¸
                learn_running = any('learn' in str(proc['cmdline']) for proc in autoci_processes)
                create_running = any('create' in str(proc['cmdline']) for proc in autoci_processes)
                resume_running = any('resume' in str(proc['cmdline']) for proc in autoci_processes)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                if learn_running and not self.monitoring_state.autoci_learn_running:
                    self.monitoring_state.autoci_learn_running = True
                    self.monitoring_state.last_learn_session = datetime.now()
                    logger.info("ğŸ“š autoci learn ì„¸ì…˜ ê°ì§€ë¨")
                    await self._on_learn_session_start()
                
                if create_running and not self.monitoring_state.autoci_create_running:
                    self.monitoring_state.autoci_create_running = True
                    self.monitoring_state.last_create_session = datetime.now()
                    logger.info("ğŸ® autoci create ì„¸ì…˜ ê°ì§€ë¨")
                    await self._on_create_session_start()
                
                if resume_running and not self.monitoring_state.autoci_resume_running:
                    self.monitoring_state.autoci_resume_running = True
                    self.monitoring_state.last_resume_session = datetime.now()
                    logger.info("ğŸ”„ autoci resume ì„¸ì…˜ ê°ì§€ë¨")
                    await self._on_resume_session_start()
                
                if not learn_running and self.monitoring_state.autoci_learn_running:
                    self.monitoring_state.autoci_learn_running = False
                    logger.info("ğŸ“š autoci learn ì„¸ì…˜ ì¢…ë£Œë¨")
                    await self._on_learn_session_end()
                
                if not create_running and self.monitoring_state.autoci_create_running:
                    self.monitoring_state.autoci_create_running = False
                    logger.info("ğŸ® autoci create ì„¸ì…˜ ì¢…ë£Œë¨")
                    await self._on_create_session_end()
                
                if not resume_running and self.monitoring_state.autoci_resume_running:
                    self.monitoring_state.autoci_resume_running = False
                    logger.info("ğŸ”„ autoci resume ì„¸ì…˜ ì¢…ë£Œë¨")
                    await self._on_resume_session_end()
                
                # ëª¨ë‹ˆí„°ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
                self.monitoring_state.total_monitored_time += 5
                
                # ë§¤ 1ë¶„ë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
                if int(self.monitoring_state.total_monitored_time) % 60 == 0:
                    minutes = int(self.monitoring_state.total_monitored_time / 60)
                    
                    # ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ í†µê³„
                    from modules.shared_knowledge_base import get_shared_knowledge_base
                    shared_kb = get_shared_knowledge_base()
                    kb_stats = shared_kb.get_knowledge_stats()
                    
                    print(f"   â° ê°€ë””ì–¸ ê°ì‹œ ì‹œê°„: {minutes}ë¶„ | learn: {'âœ…' if self.monitoring_state.autoci_learn_running else 'âŒ'} | create: {'âœ…' if self.monitoring_state.autoci_create_running else 'âŒ'}")
                    print(f"   ğŸ“š ì§€ì‹ ë² ì´ìŠ¤: ê²€ìƒ‰ {kb_stats['cached_searches']}/{kb_stats['total_searches']} | ì†”ë£¨ì…˜ {kb_stats['total_solutions']} | ë² ìŠ¤íŠ¸ {kb_stats['total_practices']}")
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                logger.error(f"í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(10)
    
    async def _start_repetition_prevention(self):
        """ë°˜ë³µì  í•™ìŠµ ì§€ì–‘ ì‹œìŠ¤í…œ"""
        logger.info("ğŸ”„ ë°˜ë³µì  í•™ìŠµ ì§€ì–‘ ì‹œìŠ¤í…œ ì‹œì‘")
        
        learning_patterns = []
        
        while self.is_running:
            try:
                # ìµœê·¼ í•™ìŠµ íŒŒì¼ë“¤ ë¶„ì„
                recent_files = []
                continuous_learning_dir = Path("continuous_learning")
                
                if continuous_learning_dir.exists():
                    # ìµœê·¼ 1ì‹œê°„ ë‚´ íŒŒì¼ë“¤ë§Œ í™•ì¸
                    one_hour_ago = datetime.now() - timedelta(hours=1)
                    
                    for file_path in continuous_learning_dir.rglob("*.json"):
                        try:
                            file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            if file_time > one_hour_ago:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                    recent_files.append({
                                        'file': str(file_path),
                                        'time': file_time,
                                        'content': data
                                    })
                        except:
                            continue
                
                # ë°˜ë³µ íŒ¨í„´ ê°ì§€
                if len(recent_files) >= 3:
                    repetitions = await self._detect_learning_repetitions(recent_files)
                    
                    if repetitions:
                        self.monitoring_state.detected_repetitions += len(repetitions)
                        logger.warning(f"ğŸ”„ ë°˜ë³µì  í•™ìŠµ íŒ¨í„´ ê°ì§€: {len(repetitions)}ê°œ")
                        
                        # ë°˜ë³µ ë°©ì§€ ì¡°ì¹˜
                        await self._prevent_repetitive_learning(repetitions)
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                logger.error(f"ë°˜ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(600)
    
    async def _start_continuous_search(self):
        """24ì‹œê°„ ì§€ì†ì  ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        logger.info("ğŸ” ì§€ì†ì  ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹œì‘")
        print("   ğŸ” ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ í™œì„±í™” - 1ë¶„ë§ˆë‹¤ ìë™ ê²€ìƒ‰ ì‹œì‘")
        
        search_keywords = [
            "Godot C# ê³ ê¸‰ ê¸°ë²•",
            "PyTorch ê²Œì„ ê°œë°œ AI",
            "C# Socket.IO ì‹¤ì‹œê°„ í†µì‹ ",
            "Godot ìë™í™” ìŠ¤í¬ë¦½íŒ…",
            "AI ê²Œì„ ê°œë°œ ìµœì‹  ê¸°ìˆ ",
            "C# ë”¥ëŸ¬ë‹ í†µí•©",
            "Godot ìµœì í™” ê¸°ë²•",
            "ê²Œì„ AI í–‰ë™ íŒ¨í„´"
        ]
        
        search_index = 0
        search_count = 0
        
        # ì¦‰ì‹œ ì²« ë²ˆì§¸ ê²€ìƒ‰ ì‹¤í–‰
        await self._perform_intelligent_search("Godot C# ê¸°ì´ˆ ë¬¸ì œ í•´ê²° ë° ìµœì í™”")
        search_count += 1
        print(f"   ğŸ“¡ ê²€ìƒ‰ #{search_count}: ì¦‰ì‹œ ì‹œì‘ ì™„ë£Œ")
        
        while self.is_running:
            try:
                # ê²€ìƒ‰ íì—ì„œ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ í™•ì¸
                try:
                    priority_keyword = await asyncio.wait_for(
                        self.search_queue.get(), timeout=1.0
                    )
                    await self._perform_intelligent_search(priority_keyword)
                    search_count += 1
                    print(f"   ğŸ” ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ #{search_count}: {priority_keyword}")
                except asyncio.TimeoutError:
                    # ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ìƒ‰ ì§„í–‰
                    keyword = search_keywords[search_index % len(search_keywords)]
                    await self._perform_intelligent_search(keyword)
                    search_count += 1
                    search_index += 1
                    print(f"   ğŸ“¡ ì •ê¸° ê²€ìƒ‰ #{search_count}: {keyword}")
                
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ê²€ìƒ‰ (ë” ë¹ˆë²ˆí•˜ê²Œ)
                
            except Exception as e:
                logger.error(f"ì§€ì†ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(120)  # ì˜¤ë¥˜ ì‹œ 2ë¶„ í›„ ì¬ì‹œë„
    
    async def _start_pytorch_training(self):
        """PyTorch ìë™ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ"""
        logger.info("ğŸ§  PyTorch ìë™ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì‹œì‘")
        
        while self.is_running:
            try:
                # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
                training_data = await self._prepare_pytorch_training_data()
                
                if training_data and len(training_data) >= 10:  # ìµœì†Œ 10ê°œ ë°ì´í„° í•„ìš”
                    logger.info(f"ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ì‹œì‘: {len(training_data)}ê°œ ë°ì´í„°")
                    
                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ PyTorch í›ˆë ¨
                    await self._run_pytorch_training(training_data)
                    
                    self.monitoring_state.pytorch_training_sessions += 1
                    logger.info("ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ì„¸ì…˜ ì™„ë£Œ")
                
                await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ë”¥ëŸ¬ë‹ ì²´í¬
                
            except Exception as e:
                logger.error(f"PyTorch ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    async def _start_knowledge_gap_detection(self):
        """ì§€ì‹ ê²©ì°¨ ê°ì§€ ë° ë³´ì™„ ì‹œìŠ¤í…œ"""
        logger.info("ğŸ¯ ì§€ì‹ ê²©ì°¨ ê°ì§€ ì‹œìŠ¤í…œ ì‹œì‘")
        
        # ê²©ì°¨ ë³´ì™„ ì§€ëŠ¥ ì‹œìŠ¤í…œ í†µí•©
        from modules.gap_filling_intelligence import get_gap_filling_intelligence
        gap_intelligence = get_gap_filling_intelligence()
        
        while self.is_running:
            try:
                # ì¢…í•©ì ì¸ ì§€ì‹ ê²©ì°¨ ë¶„ì„
                knowledge_gaps = await gap_intelligence.analyze_comprehensive_gaps()
                
                if knowledge_gaps:
                    logger.info(f"ğŸ¯ ì¢…í•© ì§€ì‹ ê²©ì°¨ ê°ì§€: {len(knowledge_gaps)}ê°œ")
                    
                    # ìë™ìœ¼ë¡œ ë©”ê¿€ ìˆ˜ ìˆëŠ” ê²©ì°¨ë“¤ ì²˜ë¦¬
                    auto_fix_result = await gap_intelligence.auto_fill_gaps(knowledge_gaps)
                    
                    # í•™ìŠµ ê¶Œì¥ì‚¬í•­ ìƒì„±
                    recommendations = await gap_intelligence.generate_learning_recommendations(knowledge_gaps)
                    
                    # ê²©ì°¨ ë³´ì™„ì„ ìœ„í•œ ê²€ìƒ‰ ìš”ì²­
                    for gap in knowledge_gaps:
                        for keyword in gap.search_keywords:
                            await self.search_queue.put(keyword)
                    
                    self.monitoring_state.filled_knowledge_gaps += auto_fix_result['auto_filled']
                    
                    # ì¤‘ìš”í•œ ê²©ì°¨ëŠ” ì¦‰ì‹œ ì•Œë¦¼
                    critical_gaps = [g for g in knowledge_gaps if g.priority == 'critical']
                    if critical_gaps:
                        print(f"\nğŸš¨ Critical ì§€ì‹ ê²©ì°¨ ê°ì§€: {len(critical_gaps)}ê°œ")
                        for gap in critical_gaps[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                            print(f"   â€¢ {gap.description}")
                    
                    logger.info(f"ğŸ”§ ìë™ ë³´ì™„ ì™„ë£Œ: {auto_fix_result['auto_filled']}/{len(knowledge_gaps)}")
                
                await asyncio.sleep(1800)  # 30ë¶„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                logger.error(f"ì§€ì‹ ê²©ì°¨ ê°ì§€ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    async def _start_human_advisory(self):
        """ì¸ê°„ ì¡°ì–¸ ì‹œìŠ¤í…œ"""
        logger.info("ğŸ’¡ ì¸ê°„ ì¡°ì–¸ ì‹œìŠ¤í…œ ì‹œì‘")
        
        while self.is_running:
            try:
                # ì¡°ì–¸ ìƒì„±
                advice = await self._generate_intelligent_advice()
                
                # ì¡°ì–¸ ì €ì¥ (datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
                advice_serializable = {
                    "timestamp": advice["timestamp"],
                    "priority": advice["priority"],
                    "category": advice["category"],
                    "message": advice["message"],
                    "action_items": advice["action_items"],
                    "monitoring_stats": {
                        k: v.isoformat() if isinstance(v, datetime) else v 
                        for k, v in advice["monitoring_stats"].items()
                    }
                }
                
                advice_file = self.guardian_dir / f"advice_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(advice_file, 'w', encoding='utf-8') as f:
                    json.dump(advice_serializable, f, indent=2, ensure_ascii=False)
                
                # ì¤‘ìš”í•œ ì¡°ì–¸ì€ ì¦‰ì‹œ ì¶œë ¥
                if advice.get('priority', 'low') == 'high':
                    print(f"\nğŸš¨ ê¸´ê¸‰ ì¡°ì–¸: {advice['message']}")
                
                await asyncio.sleep(7200)  # 2ì‹œê°„ë§ˆë‹¤ ì¡°ì–¸ ìƒì„±
                
            except Exception as e:
                logger.error(f"ì¸ê°„ ì¡°ì–¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    async def _on_learn_session_start(self):
        """learn ì„¸ì…˜ ì‹œì‘ ì‹œ ì²˜ë¦¬"""
        session_id = f"learn_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.current_learning_progress = LearningProgress(
            session_id=session_id,
            start_time=datetime.now(),
            last_activity=datetime.now(),
            total_learning_time=0.0,
            quality_score=0.0,
            repetitive_patterns=[],
            knowledge_gaps=[],
            learning_efficiency=0.0,
            next_recommended_action=""
        )
        
        logger.info(f"ğŸ“š learn ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {session_id}")
    
    async def _on_learn_session_end(self):
        """learn ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        if self.current_learning_progress:
            # ì„¸ì…˜ ê²°ê³¼ ë¶„ì„
            session_duration = (datetime.now() - self.current_learning_progress.start_time).total_seconds()
            self.current_learning_progress.total_learning_time = session_duration
            
            # ì„¸ì…˜ ë³´ê³ ì„œ ì €ì¥
            session_report = asdict(self.current_learning_progress)
            session_report['start_time'] = self.current_learning_progress.start_time.isoformat()
            session_report['last_activity'] = self.current_learning_progress.last_activity.isoformat()
            
            report_file = self.guardian_dir / f"session_report_{self.current_learning_progress.session_id}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(session_report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“š learn ì„¸ì…˜ ì™„ë£Œ: {session_duration:.1f}ì´ˆ")
            self.current_learning_progress = None
    
    async def _on_create_session_start(self):
        """create ì„¸ì…˜ ì‹œì‘ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ® create ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # create ì„¸ì…˜ì˜ ì ì¬ì  ë¬¸ì œì  ë¯¸ë¦¬ ê²€ìƒ‰
        await self.search_queue.put("Godot C# ê²Œì„ ê°œë°œ ì¼ë°˜ì  ì˜¤ë¥˜ í•´ê²°")
        await self.search_queue.put("Godot C# ìŠ¤í¬ë¦½íŠ¸ ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ")
    
    async def _on_create_session_end(self):
        """create ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ® create ì„¸ì…˜ ì™„ë£Œ")
        
        # ìƒì„±ëœ ê²Œì„ í”„ë¡œì íŠ¸ ë¶„ì„
        await self._analyze_created_projects()
    
    async def _detect_learning_repetitions(self, recent_files: List[Dict]) -> List[str]:
        """ë°˜ë³µì  í•™ìŠµ íŒ¨í„´ ê°ì§€"""
        repetitions = []
        
        try:
            # íŒŒì¼ ë‚´ìš©ì˜ ìœ ì‚¬ì„± ê²€ì‚¬
            contents = []
            for file_data in recent_files:
                content = file_data.get('content', {})
                if isinstance(content, dict):
                    # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = []
                    for key, value in content.items():
                        if isinstance(value, str):
                            keywords.extend(value.lower().split())
                    contents.append(set(keywords))
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            for i in range(len(contents)):
                for j in range(i + 1, len(contents)):
                    similarity = len(contents[i] & contents[j]) / len(contents[i] | contents[j]) if contents[i] | contents[j] else 0
                    
                    if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë°˜ë³µìœ¼ë¡œ ê°„ì£¼
                        repetition_pattern = f"ë°˜ë³µ íŒ¨í„´ {i}-{j}: ìœ ì‚¬ë„ {similarity:.2f}"
                        repetitions.append(repetition_pattern)
        
        except Exception as e:
            logger.error(f"ë°˜ë³µ íŒ¨í„´ ê°ì§€ ì˜¤ë¥˜: {e}")
        
        return repetitions
    
    async def _prevent_repetitive_learning(self, repetitions: List[str]):
        """ë°˜ë³µì  í•™ìŠµ ë°©ì§€ ì¡°ì¹˜"""
        prevention_file = self.guardian_dir / "repetition_prevention.json"
        
        prevention_data = {
            "timestamp": datetime.now().isoformat(),
            "detected_repetitions": repetitions,
            "prevention_actions": [
                "ìƒˆë¡œìš´ í•™ìŠµ ì£¼ì œ ì œì•ˆ",
                "ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ ê¶Œì¥",
                "íœ´ì‹ ì‹œê°„ ì œì•ˆ"
            ],
            "recommended_topics": [
                "ê³ ê¸‰ C# íŒ¨í„´",
                "Godot ìƒˆë¡œìš´ ê¸°ëŠ¥",
                "AI ìµœì‹  ê¸°ìˆ "
            ]
        }
        
        with open(prevention_file, 'w', encoding='utf-8') as f:
            json.dump(prevention_data, f, indent=2, ensure_ascii=False)
        
        # ê²€ìƒ‰ íì— ìƒˆë¡œìš´ ì£¼ì œ ì¶”ê°€
        for topic in prevention_data["recommended_topics"]:
            await self.search_queue.put(topic)
    
    async def _perform_intelligent_search(self, keyword: str):
        """ì§€ëŠ¥í˜• ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            logger.info(f"ğŸ” ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œì‘: {keyword}")
            
            # ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ ì‚¬ìš©
            from modules.shared_knowledge_base import get_shared_knowledge_base
            shared_kb = get_shared_knowledge_base()
            
            # ìºì‹œëœ ê²°ê³¼ í™•ì¸
            cached_result = await shared_kb.get_cached_search(keyword)
            if cached_result:
                logger.info(f"ğŸ“š ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©: {keyword}")
                return cached_result
            
            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            search_results_dir = self.knowledge_base_dir / "search_results"
            search_results_dir.mkdir(exist_ok=True)
            
            # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)
            search_sources = [
                "Godot ê³µì‹ ë¬¸ì„œ",
                "StackOverflow",
                "GitHub",
                "Reddit",
                "YouTube íŠœí† ë¦¬ì–¼"
            ]
            
            search_results = {
                "keyword": keyword,
                "timestamp": datetime.now().isoformat(),
                "sources": {},
                "summary": f"{keyword}ì— ëŒ€í•œ ìµœì‹  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ",
                "actionable_insights": [
                    f"{keyword} ê´€ë ¨ ìƒˆë¡œìš´ ì ‘ê·¼ë²• ë°œê²¬",
                    f"{keyword} ìµœì í™” ë°©ë²• ì—…ë°ì´íŠ¸",
                    f"{keyword} ë¬¸ì œ í•´ê²° íŒ¨í„´ ìˆ˜ì§‘"
                ]
            }
            
            for source in search_sources:
                search_results["sources"][source] = {
                    "status": "ê²€ìƒ‰ ì™„ë£Œ",
                    "results_count": np.random.randint(5, 20),
                    "quality_score": np.random.uniform(0.7, 0.95)
                }
            
            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì—ë„ ì €ì¥)
            result_file = search_results_dir / f"search_{keyword.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(search_results, f, indent=2, ensure_ascii=False)
            
            # ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì— ì €ì¥
            await shared_kb.save_search_result(keyword, search_results)
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ ë° ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥: {keyword}")
            
        except Exception as e:
            logger.error(f"ì§€ëŠ¥í˜• ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    async def _prepare_pytorch_training_data(self) -> List[Dict]:
        """PyTorch í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        training_data = []
        
        try:
            # ëª¨ë“  ê²½í—˜ ë°ì´í„° ìˆ˜ì§‘
            experiences_dir = Path("experiences")
            
            for data_file in experiences_dir.rglob("*.json"):
                try:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                        # ë”¥ëŸ¬ë‹ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜
                        processed_data = {
                            "input_features": self._extract_features(data),
                            "labels": self._extract_labels(data),
                            "metadata": {
                                "source_file": str(data_file),
                                "timestamp": data.get("timestamp", ""),
                                "category": self._categorize_data(data_file)
                            }
                        }
                        
                        if processed_data["input_features"] and processed_data["labels"]:
                            training_data.append(processed_data)
                
                except Exception as e:
                    continue
            
            logger.info(f"ğŸ§  PyTorch í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(training_data)}ê°œ")
            
        except Exception as e:
            logger.error(f"PyTorch ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
        
        return training_data
    
    def _extract_features(self, data: Dict) -> List[float]:
        """ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        try:
            # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, (int, float)):
                        features.append(float(value))
                    elif isinstance(value, str):
                        # ë¬¸ìì—´ ê¸¸ì´ ë° í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¹ì§•
                        features.append(len(value))
                        features.append(float(value.count('error')))
                        features.append(float(value.count('success')))
                        features.append(float(value.count('C#')))
                        features.append(float(value.count('Godot')))
            
            # íŠ¹ì§• ë²¡í„° ì •ê·œí™”
            if features:
                max_val = max(features) if max(features) > 0 else 1
                features = [f / max_val for f in features]
            
        except Exception as e:
            logger.error(f"íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return features[:10]  # ìµœëŒ€ 10ê°œ íŠ¹ì§•
    
    def _extract_labels(self, data: Dict) -> List[float]:
        """ë°ì´í„°ì—ì„œ ë¼ë²¨ ì¶”ì¶œ"""
        labels = []
        
        try:
            # ì„±ê³µ/ì‹¤íŒ¨, í’ˆì§ˆ ì ìˆ˜ ë“±ì„ ë¼ë²¨ë¡œ ì‚¬ìš©
            if isinstance(data, dict):
                if 'success' in data:
                    labels.append(1.0 if data['success'] else 0.0)
                
                if 'quality_score' in data:
                    labels.append(float(data['quality_score']))
                
                if 'error' in data:
                    labels.append(0.0)
                elif 'completed' in data:
                    labels.append(1.0)
            
            # ê¸°ë³¸ ë¼ë²¨
            if not labels:
                labels = [0.5]  # ì¤‘ê°„ê°’
            
        except Exception as e:
            logger.error(f"ë¼ë²¨ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return labels
    
    def _categorize_data(self, file_path: Path) -> str:
        """ë°ì´í„° íŒŒì¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        path_str = str(file_path).lower()
        
        if 'csharp' in path_str:
            return 'csharp_learning'
        elif 'godot' in path_str:
            return 'godot_interaction'
        elif 'game_development' in path_str:
            return 'game_creation'
        elif 'networking' in path_str:
            return 'networking'
        elif 'korean' in path_str:
            return 'korean_nlp'
        else:
            return 'general_learning'
    
    async def _run_pytorch_training(self, training_data: List[Dict]):
        """PyTorch ë”¥ëŸ¬ë‹ ì‹¤í–‰"""
        try:
            # í›ˆë ¨ ë°ì´í„° ì €ì¥
            dataset_file = self.pytorch_datasets_dir / f"dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(dataset_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "data_count": len(training_data),
                    "categories": list(set(d["metadata"]["category"] for d in training_data)),
                    "training_summary": "PyTorch ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ"
                }, f, indent=2, ensure_ascii=False)
            
            # ì‹¤ì œ PyTorch í›ˆë ¨ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
            logger.info(f"ğŸ§  PyTorch ë°ì´í„°ì…‹ ì €ì¥: {dataset_file}")
            
        except Exception as e:
            logger.error(f"PyTorch í›ˆë ¨ ì˜¤ë¥˜: {e}")
    
    async def _detect_knowledge_gaps(self) -> List[str]:
        """ì§€ì‹ ê²©ì°¨ ê°ì§€"""
        gaps = []
        
        try:
            # ìµœê·¼ ì˜¤ë¥˜ ë¶„ì„
            error_patterns = await self._analyze_recent_errors()
            
            # ë¶€ì¡±í•œ ì˜ì—­ ì‹ë³„
            knowledge_areas = {
                "C# ê³ ê¸‰ ê¸°ë²•": 0,
                "Godot ìµœì í™”": 0,
                "PyTorch í†µí•©": 0,
                "Socket.IO ê³ ê¸‰": 0,
                "ê²Œì„ AI íŒ¨í„´": 0
            }
            
            # ì§€ì‹ ë² ì´ìŠ¤ ë¶„ì„
            for area in knowledge_areas:
                area_files = list(self.knowledge_base_dir.rglob(f"*{area.replace(' ', '_').lower()}*"))
                knowledge_areas[area] = len(area_files)
            
            # ì§€ì‹ì´ ë¶€ì¡±í•œ ì˜ì—­ ì‹ë³„
            avg_knowledge = sum(knowledge_areas.values()) / len(knowledge_areas)
            
            for area, count in knowledge_areas.items():
                if count < avg_knowledge * 0.7:  # í‰ê· ì˜ 70% ë¯¸ë§Œì´ë©´ ê²©ì°¨ë¡œ ê°„ì£¼
                    gaps.append(area)
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ê²©ì°¨ ê°ì§€ ì˜¤ë¥˜: {e}")
        
        return gaps
    
    async def _analyze_recent_errors(self) -> List[str]:
        """ìµœê·¼ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        error_patterns = []
        
        try:
            # ìµœê·¼ ë¡œê·¸ íŒŒì¼ë“¤ í™•ì¸
            log_dirs = [Path("experiences"), Path("game_projects")]
            
            for log_dir in log_dirs:
                if log_dir.exists():
                    for log_file in log_dir.rglob("*.json"):
                        try:
                            with open(log_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                
                                if isinstance(data, dict) and 'error' in str(data).lower():
                                    error_info = str(data)
                                    if 'c#' in error_info.lower():
                                        error_patterns.append("C# ì»´íŒŒì¼ ì˜¤ë¥˜")
                                    elif 'godot' in error_info.lower():
                                        error_patterns.append("Godot ì—”ì§„ ì˜¤ë¥˜")
                                    elif 'socket' in error_info.lower():
                                        error_patterns.append("Socket.IO í†µì‹  ì˜¤ë¥˜")
                        except:
                            continue
        
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return error_patterns
    
    async def _analyze_created_projects(self):
        """ìƒì„±ëœ ê²Œì„ í”„ë¡œì íŠ¸ ë¶„ì„"""
        try:
            game_projects_dir = Path("game_projects")
            
            if game_projects_dir.exists():
                recent_projects = []
                one_hour_ago = datetime.now() - timedelta(hours=1)
                
                for project_dir in game_projects_dir.iterdir():
                    if project_dir.is_dir():
                        creation_time = datetime.fromtimestamp(project_dir.stat().st_ctime)
                        if creation_time > one_hour_ago:
                            recent_projects.append(project_dir)
                
                if recent_projects:
                    analysis = {
                        "timestamp": datetime.now().isoformat(),
                        "analyzed_projects": [str(p) for p in recent_projects],
                        "project_count": len(recent_projects),
                        "analysis_summary": "ìµœê·¼ ìƒì„±ëœ í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ"
                    }
                    
                    analysis_file = self.guardian_dir / f"project_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(analysis, f, indent=2, ensure_ascii=False)
                    
                    logger.info(f"ğŸ® í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ: {len(recent_projects)}ê°œ")
        
        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    async def _generate_intelligent_advice(self) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ì¡°ì–¸ ìƒì„±"""
        advice = {
            "timestamp": datetime.now().isoformat(),
            "priority": "medium",
            "category": "general",
            "message": "",
            "action_items": [],
            "monitoring_stats": asdict(self.monitoring_state)
        }
        
        try:
            # ëª¨ë‹ˆí„°ë§ ìƒíƒœ ê¸°ë°˜ ì¡°ì–¸ ìƒì„±
            if self.monitoring_state.detected_repetitions > 5:
                advice.update({
                    "priority": "high",
                    "category": "learning_optimization",
                    "message": "ë°˜ë³µì  í•™ìŠµ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    "action_items": [
                        "autoci learn ì ì‹œ ì¤‘ë‹¨",
                        "ë‹¤ë¥¸ ê²Œì„ íƒ€ì…ìœ¼ë¡œ autoci create ì‹œë„",
                        "ìƒˆë¡œìš´ ê¸°ìˆ  ìŠ¤íƒ í•™ìŠµ ê³ ë ¤"
                    ]
                })
            
            elif self.monitoring_state.filled_knowledge_gaps > 3:
                advice.update({
                    "priority": "medium",
                    "category": "knowledge_enhancement",
                    "message": "ì§€ì‹ ê²©ì°¨ê°€ ì„±ê³µì ìœ¼ë¡œ ë³´ì™„ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.",
                    "action_items": [
                        "í˜„ì¬ í•™ìŠµ ë°©í–¥ ìœ ì§€",
                        "ê³ ê¸‰ ì£¼ì œë¡œ ì ì§„ì  ì§„í–‰",
                        "í”„ë¡œì íŠ¸ ë³µì¡ë„ ì¦ê°€"
                    ]
                })
            
            elif self.monitoring_state.pytorch_training_sessions >= 1:
                advice.update({
                    "priority": "low",
                    "category": "ai_optimization",
                    "message": "PyTorch ë”¥ëŸ¬ë‹ì´ í™œë°œíˆ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. AI ì„±ëŠ¥ì´ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                    "action_items": [
                        "ë” ë§ì€ í•™ìŠµ ë°ì´í„° ìƒì„±",
                        "ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸",
                        "ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§"
                    ]
                })
            
            else:
                advice.update({
                    "priority": "medium",
                    "category": "general_guidance",
                    "message": "ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤. ì§€ì†ì ì¸ í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    "action_items": [
                        "ì •ê¸°ì ì¸ autoci learn ì‹¤í–‰",
                        "ë‹¤ì–‘í•œ ê²Œì„ íƒ€ì… ì‹¤í—˜",
                        "ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”"
                    ]
                })
        
        except Exception as e:
            logger.error(f"ì¡°ì–¸ ìƒì„± ì˜¤ë¥˜: {e}")
            advice["message"] = "ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì ê²€ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        
        return advice
    
    async def _on_learn_session_start(self):
        """autoci learn ì„¸ì…˜ ì‹œì‘ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ“š í•™ìŠµ ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤€ë¹„
        self.current_learning_progress = LearningProgress(
            session_id=f"learn_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            start_time=datetime.now(),
            last_activity=datetime.now(),
            total_learning_time=0.0,
            quality_score=0.0,
            repetitive_patterns=[],
            knowledge_gaps=[],
            learning_efficiency=1.0,
            next_recommended_action="í•™ìŠµ ì§„í–‰ ì¤‘..."
        )
    
    async def _on_learn_session_end(self):
        """autoci learn ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ“š í•™ìŠµ ì„¸ì…˜ ë¶„ì„ ì¤‘...")
        if self.current_learning_progress:
            # í•™ìŠµ ì„¸ì…˜ ê²°ê³¼ ì €ì¥
            session_file = self.guardian_dir / f"learn_session_{self.current_learning_progress.session_id}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.current_learning_progress), f, indent=2, ensure_ascii=False)
    
    async def _on_create_session_start(self):
        """autoci create ì„¸ì…˜ ì‹œì‘ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ® ê²Œì„ ìƒì„± ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        # ê²Œì„ í”„ë¡œì íŠ¸ ëª¨ë‹ˆí„°ë§ ì¤€ë¹„
        self.create_session_data = {
            "session_id": f"create_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "start_time": datetime.now().isoformat(),
            "game_type": "unknown",
            "progress": []
        }
    
    async def _on_create_session_end(self):
        """autoci create ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ® ê²Œì„ ìƒì„± ì„¸ì…˜ ë¶„ì„ ì¤‘...")
        if hasattr(self, 'create_session_data'):
            # ìƒì„± ì„¸ì…˜ ê²°ê³¼ ì €ì¥
            session_file = self.guardian_dir / f"create_session_{self.create_session_data['session_id']}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(self.create_session_data, f, indent=2, ensure_ascii=False)
    
    async def _on_resume_session_start(self):
        """autoci resume ì„¸ì…˜ ì‹œì‘ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ”„ ê¸°ì¡´ í”„ë¡œì íŠ¸ ê°œë°œ ì„¸ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        # Godot í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ëª¨ë‹ˆí„°ë§
        self.resume_session_data = {
            "session_id": f"resume_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "start_time": datetime.now().isoformat(),
            "project_path": None,
            "game_type": None,
            "improvements": [],
            "files_modified": []
        }
        
        # Godot í”„ë¡œì íŠ¸ ê²½ë¡œ ì°¾ê¸°
        godot_projects_path = Path("/home/super3720/Documents/Godot/Projects")
        if godot_projects_path.exists():
            for project_dir in godot_projects_path.iterdir():
                if project_dir.is_dir() and (project_dir / "project.godot").exists():
                    # ê°€ì¥ ìµœê·¼ ìˆ˜ì •ëœ í”„ë¡œì íŠ¸ ì¶”ì 
                    if project_dir.name not in self.monitoring_state.godot_projects_monitored:
                        self.monitoring_state.godot_projects_monitored.append(project_dir.name)
                        self.resume_session_data["project_path"] = str(project_dir)
                        logger.info(f"ğŸ“ Godot í”„ë¡œì íŠ¸ ê°ì§€: {project_dir.name}")
    
    async def _on_resume_session_end(self):
        """autoci resume ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        logger.info("ğŸ”„ ê¸°ì¡´ í”„ë¡œì íŠ¸ ê°œë°œ ì„¸ì…˜ ë¶„ì„ ì¤‘...")
        if hasattr(self, 'resume_session_data'):
            # í”„ë¡œì íŠ¸ ê°œì„  ì‚¬í•­ ë¶„ì„
            if self.resume_session_data["project_path"]:
                project_path = Path(self.resume_session_data["project_path"])
                
                # ìˆ˜ì •ëœ íŒŒì¼ ì°¾ê¸°
                for file_path in project_path.rglob("*.gd"):  # GDScript files
                    mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if mod_time > datetime.fromisoformat(self.resume_session_data["start_time"]):
                        self.resume_session_data["files_modified"].append(str(file_path))
                
                for file_path in project_path.rglob("*.tscn"):  # Scene files
                    mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if mod_time > datetime.fromisoformat(self.resume_session_data["start_time"]):
                        self.resume_session_data["files_modified"].append(str(file_path))
            
            # ì„¸ì…˜ ê²°ê³¼ ì €ì¥
            session_file = self.guardian_dir / f"resume_session_{self.resume_session_data['session_id']}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(self.resume_session_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ìˆ˜: {len(self.resume_session_data['files_modified'])}")
    
    async def stop_guardian_mode(self):
        """ê°€ë””ì–¸ ëª¨ë“œ ì¢…ë£Œ"""
        logger.info("ğŸ›¡ï¸ ê°€ë””ì–¸ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        self.is_running = False
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„± (datetime ê°ì²´ ì•ˆì „í•˜ê²Œ ë³€í™˜)
        final_report = {
            "session_end": datetime.now().isoformat(),
            "total_monitoring_time": self.monitoring_state.total_monitored_time,
            "detected_repetitions": self.monitoring_state.detected_repetitions,
            "filled_knowledge_gaps": self.monitoring_state.filled_knowledge_gaps,
            "pytorch_training_sessions": self.monitoring_state.pytorch_training_sessions,
            "godot_projects_monitored": self.monitoring_state.godot_projects_monitored,
            "final_summary": "AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì„¸ì…˜ ì™„ë£Œ",
            "last_learn_session": self.monitoring_state.last_learn_session.isoformat() if self.monitoring_state.last_learn_session else None,
            "last_create_session": self.monitoring_state.last_create_session.isoformat() if self.monitoring_state.last_create_session else None,
            "last_resume_session": self.monitoring_state.last_resume_session.isoformat() if self.monitoring_state.last_resume_session else None
        }
        
        final_report_file = self.guardian_dir / f"final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(final_report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        logger.info("ğŸ›¡ï¸ ê°€ë””ì–¸ ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")
        print("\nâœ… AutoCI ì§€ëŠ¥í˜• ê°€ë””ì–¸ ì‹œìŠ¤í…œì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‹ ìµœì¢… ë³´ê³ ì„œ: {final_report_file}")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_guardian_instance = None

def get_guardian_system() -> IntelligentGuardianSystem:
    """ê°€ë””ì–¸ ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _guardian_instance
    if _guardian_instance is None:
        _guardian_instance = IntelligentGuardianSystem()
    return _guardian_instance