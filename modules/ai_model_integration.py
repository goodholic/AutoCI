#!/usr/bin/env python3
"""
AI ëª¨ë¸ í†µí•© ëª¨ë“ˆ - ì‹¬ì¸µ êµ¬í˜„
ë©”ëª¨ë¦¬ì— ë”°ë¼ Qwen2.5-Coder-32B, CodeLlama-13B, Llama-3.1-8B ì¤‘ ì„ íƒ
"""

import os
import sys
import asyncio
import logging
import psutil
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

try:
    import torch
except ImportError:
    torch = None  # Handle missing torch gracefully
    
    # Mock torch for development
    class MockTorch:
        class cuda:
            @staticmethod
            def is_available():
                return False
            
            @staticmethod
            def memory_allocated(*args):
                return 0
            
            @staticmethod
            def max_memory_allocated(*args):
                return 1
        
        class backends:
            class mps:
                @staticmethod
                def is_available():
                    return False
    
    torch = MockTorch()

class ModelType(Enum):
    """ì§€ì›í•˜ëŠ” AI ëª¨ë¸ íƒ€ì…"""
    QWEN_32B = "Qwen2.5-Coder-32B"
    CODELLAMA_13B = "CodeLlama-13B"
    LLAMA_8B = "Llama-3.1-8B"

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    name: str
    model_id: str
    memory_required: int  # GB
    max_tokens: int
    temperature: float
    quantization: Optional[str] = None
    device_map: str = "auto"

class AIModelIntegration:
    """AI ëª¨ë¸ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        self.model_type = None
        self.device = self._get_device()
        
        # ëª¨ë¸ ì„¤ì •
        self.model_configs = {
            ModelType.QWEN_32B: ModelConfig(
                name="Qwen2.5-Coder-32B",
                model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
                memory_required=64,  # ì–‘ìí™” ì‹œ 32GB
                max_tokens=32768,
                temperature=0.7,
                quantization="4bit"
            ),
            ModelType.CODELLAMA_13B: ModelConfig(
                name="CodeLlama-13B",
                model_id="codellama/CodeLlama-13b-Instruct-hf",
                memory_required=26,  # ì–‘ìí™” ì‹œ 13GB
                max_tokens=16384,
                temperature=0.8,
                quantization="8bit"
            ),
            ModelType.LLAMA_8B: ModelConfig(
                name="Llama-3.1-8B",
                model_id="meta-llama/Llama-3.1-8B-Instruct",
                memory_required=16,  # ì–‘ìí™” ì‹œ 8GB
                max_tokens=8192,
                temperature=0.8,
                quantization="8bit"
            )
        }
        
        # ì½”ë“œ ìƒì„± ì „ë¬¸ í”„ë¡¬í”„íŠ¸
        self.system_prompts = {
            "game_dev": """You are an expert game developer AI assistant specializing in Godot Engine and C#.
Your role is to:
1. Generate high-quality, production-ready game code
2. Follow Godot best practices and C# conventions
3. Optimize for performance and maintainability
4. Include appropriate comments in Korean
5. Handle edge cases and errors gracefully""",
            
            "code_review": """You are a senior code reviewer specializing in game development.
Analyze the provided code for:
1. Performance bottlenecks
2. Memory leaks
3. Security vulnerabilities
4. Code style violations
5. Godot-specific anti-patterns
Provide specific, actionable feedback.""",
            
            "bug_fix": """You are a debugging expert for game development.
Your task is to:
1. Identify the root cause of bugs
2. Suggest minimal, safe fixes
3. Prevent regression
4. Add appropriate error handling
5. Document the fix clearly""",
            
            "optimization": """You are a performance optimization specialist.
Focus on:
1. Reducing draw calls and physics calculations
2. Optimizing memory usage
3. Improving frame rates
4. Efficient resource loading
5. Godot-specific optimizations"""
        }
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.initialize_model()
    
    def _get_device(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ í™•ì¸"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    
    def select_model_based_on_memory(self) -> ModelType:
        """ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ"""
        try:
            available_memory = psutil.virtual_memory().available / (1024**3)  # GB
            gpu_memory = 0
            
            if self.device == "cuda":
                # GPU ë©”ëª¨ë¦¬ í™•ì¸
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=memory.free", "--format=csv,nounits,noheader"],
                    capture_output=True,
                    text=True
                )
                if result.returncode == 0:
                    gpu_memory = int(result.stdout.strip()) / 1024  # GB
            
            total_memory = available_memory + gpu_memory
            
            self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {total_memory:.1f}GB (RAM: {available_memory:.1f}GB, GPU: {gpu_memory:.1f}GB)")
            
            # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
            if total_memory >= 32:
                return ModelType.QWEN_32B
            elif total_memory >= 16:
                return ModelType.CODELLAMA_13B
            else:
                return ModelType.LLAMA_8B
                
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
            return ModelType.LLAMA_8B  # ê¸°ë³¸ê°’
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model_type = self.select_model_based_on_memory()
        config = self.model_configs[self.model_type]
        
        self.logger.info(f"ğŸ¤– AI ëª¨ë¸ ì´ˆê¸°í™”: {config.name}")
        
        try:
            # ëª¨ë¸ì´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            model_path = Path("models") / config.name
            
            if model_path.exists():
                self.logger.info(f"ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {model_path}")
                self._load_local_model(model_path, config)
            else:
                self.logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”: {config.model_id}")
                # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ
                # self._download_and_load_model(config)
                
                # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ë”ë¯¸ ì´ˆê¸°í™”
                self._initialize_dummy_model(config)
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._initialize_dummy_model(config)
    
    def _initialize_dummy_model(self, config: ModelConfig):
        """ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ëª¨ë¸"""
        self.logger.info("ë”ë¯¸ ëª¨ë¸ ëª¨ë“œë¡œ ì‹¤í–‰")
        self.model = DummyModel(config)
        self.tokenizer = DummyTokenizer()
    
    async def generate_code(self, prompt: str, context: Dict[str, Any], 
                          task_type: str = "game_dev") -> Dict[str, Any]:
        """AIë¥¼ ì‚¬ìš©í•œ ì½”ë“œ ìƒì„±"""
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            system_prompt = self.system_prompts.get(task_type, self.system_prompts["game_dev"])
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            context_info = self._build_context_info(context)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"""{system_prompt}

Context:
{context_info}

Task: {prompt}

Please generate high-quality code that follows best practices."""
            
            # ì½”ë“œ ìƒì„±
            response = await self._generate_response(full_prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            code, explanation = self._parse_code_response(response)
            
            # ì½”ë“œ ê²€ì¦
            validation_result = await self.validate_generated_code(code, context)
            
            return {
                "success": True,
                "code": code,
                "explanation": explanation,
                "model": self.model_type.value,
                "validation": validation_result,
                "tokens_used": len(self.tokenizer.encode(full_prompt)) + len(self.tokenizer.encode(response))
            }
            
        except Exception as e:
            self.logger.error(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "fallback_code": self._generate_fallback_code(prompt, context)
            }
    
    async def _generate_response(self, prompt: str) -> str:
        """ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
        if isinstance(self.model, DummyModel):
            # ë”ë¯¸ ëª¨ë¸ì¸ ê²½ìš°
            return self.model.generate(prompt)
        
        # ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=2048,
                temperature=0.7,
                do_sample=True,
                top_p=0.95,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(prompt, "").strip()
    
    def _build_context_info(self, context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±"""
        info_parts = []
        
        if "game_type" in context:
            info_parts.append(f"Game Type: {context['game_type']}")
        
        if "current_features" in context:
            info_parts.append(f"Current Features: {', '.join(context['current_features'])}")
        
        if "target_feature" in context:
            info_parts.append(f"Target Feature: {context['target_feature']}")
        
        if "language" in context:
            info_parts.append(f"Language: {context['language']} (Godot 4.x)")
        
        if "constraints" in context:
            info_parts.append(f"Constraints: {', '.join(context['constraints'])}")
        
        return "\n".join(info_parts)
    
    def _parse_code_response(self, response: str) -> tuple[str, str]:
        """AI ì‘ë‹µì—ì„œ ì½”ë“œì™€ ì„¤ëª… ë¶„ë¦¬"""
        # ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
        code_blocks = []
        explanation_parts = []
        
        lines = response.split('\n')
        in_code_block = False
        current_code = []
        
        for line in lines:
            if line.strip().startswith('```'):
                if in_code_block:
                    # ì½”ë“œ ë¸”ë¡ ì¢…ë£Œ
                    code_blocks.append('\n'.join(current_code))
                    current_code = []
                in_code_block = not in_code_block
            elif in_code_block:
                current_code.append(line)
            else:
                explanation_parts.append(line)
        
        # ë§ˆì§€ë§‰ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
        if current_code:
            code_blocks.append('\n'.join(current_code))
        
        code = '\n\n'.join(code_blocks) if code_blocks else response
        explanation = '\n'.join(explanation_parts).strip()
        
        return code, explanation
    
    async def validate_generated_code(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒì„±ëœ ì½”ë“œ ê²€ì¦"""
        validation_results = {
            "syntax_valid": True,
            "godot_compatible": True,
            "security_issues": [],
            "performance_concerns": [],
            "suggestions": []
        }
        
        # ë¬¸ë²• ê²€ì¦
        if context.get("language") == "GDScript":
            validation_results["syntax_valid"] = self._validate_gdscript_syntax(code)
        elif context.get("language") == "C#":
            validation_results["syntax_valid"] = self._validate_csharp_syntax(code)
        
        # Godot í˜¸í™˜ì„± ê²€ì¦
        godot_issues = self._check_godot_compatibility(code)
        if godot_issues:
            validation_results["godot_compatible"] = False
            validation_results["suggestions"].extend(godot_issues)
        
        # ë³´ì•ˆ ê²€ì¦
        security_issues = self._check_security_issues(code)
        if security_issues:
            validation_results["security_issues"] = security_issues
        
        # ì„±ëŠ¥ ê²€ì¦
        performance_concerns = self._check_performance_issues(code)
        if performance_concerns:
            validation_results["performance_concerns"] = performance_concerns
        
        return validation_results
    
    def _validate_gdscript_syntax(self, code: str) -> bool:
        """GDScript ë¬¸ë²• ê²€ì¦"""
        # ê¸°ë³¸ì ì¸ ë¬¸ë²• ì²´í¬
        required_keywords = ["extends", "func"]
        return any(keyword in code for keyword in required_keywords)
    
    def _validate_csharp_syntax(self, code: str) -> bool:
        """C# ë¬¸ë²• ê²€ì¦"""
        # ê¸°ë³¸ì ì¸ C# ë¬¸ë²• ì²´í¬
        required_keywords = ["using", "class", "public"]
        return any(keyword in code for keyword in required_keywords)
    
    def _check_godot_compatibility(self, code: str) -> List[str]:
        """Godot í˜¸í™˜ì„± ê²€ì¦"""
        issues = []
        
        # Godot 4.x API ì²´í¬
        if "move_and_slide()" in code and "velocity" not in code:
            issues.append("Godot 4.xì—ì„œëŠ” move_and_slide()ì— velocity ë§¤ê°œë³€ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        if "instance()" in code:
            issues.append("Godot 4.xì—ì„œëŠ” instance() ëŒ€ì‹  instantiate()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        if "PoolStringArray" in code:
            issues.append("Godot 4.xì—ì„œëŠ” PoolStringArray ëŒ€ì‹  PackedStringArrayë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        return issues
    
    def _check_security_issues(self, code: str) -> List[str]:
        """ë³´ì•ˆ ë¬¸ì œ ê²€ì¦"""
        issues = []
        
        # ìœ„í—˜í•œ í•¨ìˆ˜ ì‚¬ìš© ì²´í¬
        dangerous_patterns = [
            ("OS.execute", "OS.execute() ì‚¬ìš© ì‹œ ì…ë ¥ ê²€ì¦ í•„ìš”"),
            ("File.open.*WRITE", "íŒŒì¼ ì“°ê¸° ì‘ì—… ì‹œ ê²½ë¡œ ê²€ì¦ í•„ìš”"),
            ("HTTPRequest", "ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹œ SSL ì¸ì¦ì„œ ê²€ì¦ í•„ìš”"),
            ("load\\(.*user://", "ì‚¬ìš©ì ì…ë ¥ ê²½ë¡œ ë¡œë“œ ì‹œ ê²€ì¦ í•„ìš”")
        ]
        
        import re
        for pattern, message in dangerous_patterns:
            if re.search(pattern, code):
                issues.append(message)
        
        return issues
    
    def _check_performance_issues(self, code: str) -> List[str]:
        """ì„±ëŠ¥ ë¬¸ì œ ê²€ì¦"""
        concerns = []
        
        # ì„±ëŠ¥ ê´€ë ¨ íŒ¨í„´ ì²´í¬
        if "_process" in code and "get_node" in code.lower():
            concerns.append("_processì—ì„œ get_node í˜¸ì¶œì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. _readyì—ì„œ ìºì‹±í•˜ì„¸ìš”.")
        
        if "for" in code and "instance" in code:
            concerns.append("ë£¨í”„ ë‚´ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë¸Œì íŠ¸ í’€ë§ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if code.count("await") > 5:
            concerns.append("ê³¼ë„í•œ await ì‚¬ìš©ì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return concerns
    
    def _generate_fallback_code(self, prompt: str, context: Dict[str, Any]) -> str:
        """í´ë°± ì½”ë“œ ìƒì„±"""
        game_type = context.get("game_type", "generic")
        feature = context.get("target_feature", "basic")
        
        return f"""# {feature} implementation for {game_type}
# Generated by fallback system

extends Node2D

func _ready():
    print("Initializing {feature}")
    # TODO: Implement {feature} logic
    pass

func _process(delta):
    # Update logic here
    pass
"""
    
    async def analyze_code(self, code: str, analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """ì½”ë“œ ë¶„ì„"""
        prompt = f"""Analyze the following code for {analysis_type} review:

```
{code}
```

Provide detailed analysis including:
1. Code quality score (0-100)
2. Potential bugs
3. Performance issues
4. Security concerns
5. Improvement suggestions
6. Best practices violations"""
        
        response = await self._generate_response(prompt)
        
        # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
        return self._parse_analysis_response(response)
    
    def _parse_analysis_response(self, response: str) -> Dict[str, Any]:
        """ë¶„ì„ ì‘ë‹µ íŒŒì‹±"""
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§
        analysis = {
            "quality_score": 75,  # ê¸°ë³¸ê°’
            "bugs": [],
            "performance_issues": [],
            "security_concerns": [],
            "suggestions": [],
            "best_practices": []
        }
        
        # ì‘ë‹µì—ì„œ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if "quality score" in line.lower():
                try:
                    score = int(''.join(filter(str.isdigit, line)))
                    analysis["quality_score"] = min(100, max(0, score))
                except:
                    pass
            elif "bug" in line.lower():
                current_section = "bugs"
            elif "performance" in line.lower():
                current_section = "performance_issues"
            elif "security" in line.lower():
                current_section = "security_concerns"
            elif "suggestion" in line.lower():
                current_section = "suggestions"
            elif "best practice" in line.lower():
                current_section = "best_practices"
            elif line.startswith("-") or line.startswith("*"):
                if current_section and current_section in analysis:
                    analysis[current_section].append(line[1:].strip())
        
        return analysis


class DummyModel:
    """ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ëª¨ë¸"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, str]:
        """ì½”ë“œ í…œí”Œë¦¿ ë¡œë“œ"""
        return {
            "movement": """extends CharacterBody2D

const SPEED = 300.0
const JUMP_VELOCITY = -400.0

func _physics_process(delta):
    # ì¤‘ë ¥ ì ìš©
    if not is_on_floor():
        velocity.y += get_gravity() * delta
    
    # ì í”„ ì²˜ë¦¬
    if Input.is_action_just_pressed("ui_accept") and is_on_floor():
        velocity.y = JUMP_VELOCITY
    
    # ì¢Œìš° ì´ë™
    var direction = Input.get_axis("ui_left", "ui_right")
    if direction:
        velocity.x = direction * SPEED
    else:
        velocity.x = move_toward(velocity.x, 0, SPEED * delta)
    
    move_and_slide()""",
            
            "shooting": """extends Node2D

@export var bullet_scene: PackedScene
@export var fire_rate: float = 0.3

var can_shoot = true

func _ready():
    $ShootTimer.wait_time = fire_rate
    $ShootTimer.timeout.connect(_on_shoot_timer_timeout)

func _process(_delta):
    if Input.is_action_pressed("shoot") and can_shoot:
        shoot()

func shoot():
    if bullet_scene:
        var bullet = bullet_scene.instantiate()
        bullet.global_position = $Muzzle.global_position
        bullet.rotation = global_rotation
        get_tree().root.add_child(bullet)
        
        can_shoot = false
        $ShootTimer.start()

func _on_shoot_timer_timeout():
    can_shoot = true""",
            
            "inventory": """using Godot;
using System.Collections.Generic;

public partial class Inventory : Node
{
    [Export] private int maxSlots = 20;
    private Dictionary<int, Item> items = new Dictionary<int, Item>();
    
    [Signal]
    public delegate void ItemAddedEventHandler(Item item);
    
    [Signal]
    public delegate void ItemRemovedEventHandler(int slot);
    
    public bool AddItem(Item item)
    {
        for (int i = 0; i < maxSlots; i++)
        {
            if (!items.ContainsKey(i))
            {
                items[i] = item;
                EmitSignal(SignalName.ItemAdded, item);
                return true;
            }
        }
        return false;
    }
    
    public bool RemoveItem(int slot)
    {
        if (items.ContainsKey(slot))
        {
            items.Remove(slot);
            EmitSignal(SignalName.ItemRemoved, slot);
            return true;
        }
        return false;
    }
}"""
        }
    
    def generate(self, prompt: str) -> str:
        """ë”ë¯¸ ì‘ë‹µ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        prompt_lower = prompt.lower()
        
        if "movement" in prompt_lower or "move" in prompt_lower:
            code = self.templates["movement"]
        elif "shoot" in prompt_lower or "bullet" in prompt_lower:
            code = self.templates["shooting"]
        elif "inventory" in prompt_lower:
            code = self.templates["inventory"]
        else:
            # ê¸°ë³¸ í…œí”Œë¦¿
            code = """extends Node

func _ready():
    print("AI Generated Code")
    # Implementation based on: """ + prompt[:50] + """...

func _process(delta):
    pass"""
        
        return f"""I'll help you implement that feature. Here's the code:

```gdscript
{code}
```

This implementation includes:
- Basic structure following Godot best practices
- Proper signal handling
- Error checking and validation
- Performance optimizations

You can customize this further based on your specific needs."""


class DummyTokenizer:
    """ë”ë¯¸ í† í¬ë‚˜ì´ì €"""
    
    def encode(self, text: str) -> List[int]:
        """ê°„ë‹¨í•œ í† í°í™” (ë‹¨ì–´ ìˆ˜ ê¸°ë°˜)"""
        return list(range(len(text.split())))
    
    def decode(self, tokens: List[int], skip_special_tokens: bool = True) -> str:
        """ë”ë¯¸ ë””ì½”ë”©"""
        return "Decoded text"
    
    def __call__(self, text: str, return_tensors: Optional[str] = None):
        """í† í¬ë‚˜ì´ì € í˜¸ì¶œ"""
        return {"input_ids": torch.tensor([self.encode(text)])}
    
    @property
    def eos_token_id(self) -> int:
        """EOS í† í° ID"""
        return 0


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ai_integration = None

def get_ai_integration() -> AIModelIntegration:
    """AI í†µí•© ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_integration
    if _ai_integration is None:
        _ai_integration = AIModelIntegration()
    return _ai_integration