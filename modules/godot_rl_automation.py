#!/usr/bin/env python3
"""
Godot ê°•í™”í•™ìŠµ ìë™í™” ì‹œìŠ¤í…œ
í™”ë©´ ì¸ì‹ê³¼ ê°€ìƒ ì…ë ¥ì„ í†µí•œ ì§€ëŠ¥í˜• ê²Œì„ ê°œë°œ
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import logging
from collections import deque
import threading
import queue

# PyTorch ê°•í™”í•™ìŠµ
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

# ê³ ê¸‰ ì»¨íŠ¸ë¡¤ëŸ¬
sys.path.insert(0, str(Path(__file__).parent))
from advanced_godot_controller import (
    GodotAutomationController, 
    GodotUIElement,
    UIElementInfo
)

logger = logging.getLogger(__name__)

class GodotEnvironment:
    """Godot ì—ë””í„°ë¥¼ ê°•í™”í•™ìŠµ í™˜ê²½ìœ¼ë¡œ ë˜í•‘"""
    
    def __init__(self):
        self.controller = GodotAutomationController()
        self.action_space = self._define_action_space()
        self.observation_space = 224 * 224 * 3  # ì´ë¯¸ì§€ í¬ê¸°
        
        # ìƒíƒœ ê¸°ë¡
        self.previous_state = None
        self.current_state = None
        self.episode_steps = 0
        self.max_steps_per_episode = 1000
        
        # ë³´ìƒ ì„¤ì •
        self.reward_config = {
            "node_created": 10.0,
            "property_set": 5.0,
            "script_attached": 15.0,
            "scene_saved": 20.0,
            "error_occurred": -10.0,
            "idle_penalty": -0.1,
            "invalid_action": -5.0,
            "task_completed": 100.0
        }
        
    def _define_action_space(self) -> List[Dict]:
        """ê°€ëŠ¥í•œ ì•¡ì…˜ ì •ì˜"""
        return [
            # ë§ˆìš°ìŠ¤ ì•¡ì…˜
            {"type": "click", "params": {"x_offset": 0, "y_offset": 0}},
            {"type": "double_click", "params": {}},
            {"type": "right_click", "params": {}},
            {"type": "drag", "params": {"dx": 100, "dy": 0}},
            
            # í‚¤ë³´ë“œ ì•¡ì…˜
            {"type": "key", "params": {"key": "ctrl+a"}},  # Add Node
            {"type": "key", "params": {"key": "ctrl+s"}},  # Save
            {"type": "key", "params": {"key": "f5"}},       # Play
            {"type": "key", "params": {"key": "f6"}},       # Play Scene
            {"type": "key", "params": {"key": "delete"}},   # Delete
            
            # í…ìŠ¤íŠ¸ ì…ë ¥
            {"type": "type", "params": {"text": "Node2D"}},
            {"type": "type", "params": {"text": "Sprite2D"}},
            {"type": "type", "params": {"text": "CollisionShape2D"}},
            {"type": "type", "params": {"text": "CharacterBody2D"}},
            
            # ë©”ë‰´ ë„¤ë¹„ê²Œì´ì…˜
            {"type": "menu", "params": {"path": ["Scene", "New Scene"]}},
            {"type": "menu", "params": {"path": ["Scene", "Save Scene"]}},
            {"type": "menu", "params": {"path": ["Project", "Project Settings"]}},
            
            # UI ìš”ì†Œ í´ë¦­
            {"type": "click_ui", "params": {"element": "scene_dock"}},
            {"type": "click_ui", "params": {"element": "inspector"}},
            {"type": "click_ui", "params": {"element": "file_system"}},
            
            # íŠ¹ìˆ˜ ì•¡ì…˜
            {"type": "wait", "params": {"duration": 0.5}},
            {"type": "scan_ui", "params": {}}
        ]
        
    def reset(self) -> np.ndarray:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.episode_steps = 0
        
        # UI ìƒíƒœ ìŠ¤ìº”
        self.controller.scan_ui_state()
        
        # ì´ˆê¸° ìƒíƒœ ìº¡ì²˜
        state = self._get_current_state()
        self.current_state = state
        self.previous_state = state
        
        return state
        
    def step(self, action_idx: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """ì•¡ì…˜ ìˆ˜í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        self.episode_steps += 1
        
        # ì´ì „ ìƒíƒœ ì €ì¥
        self.previous_state = self.current_state
        
        # ì•¡ì…˜ ì‹¤í–‰
        action = self.action_space[action_idx]
        reward = self._execute_action(action)
        
        # ìƒˆë¡œìš´ ìƒíƒœ ê´€ì°°
        self.current_state = self._get_current_state()
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
        done = (
            self.episode_steps >= self.max_steps_per_episode or
            self._is_task_completed() or
            self._is_error_state()
        )
        
        # ì¶”ê°€ ì •ë³´
        info = {
            "action": action,
            "ui_state": self.controller.ui_state,
            "episode_steps": self.episode_steps
        }
        
        return self.current_state, reward, done, info
        
    def _execute_action(self, action: Dict) -> float:
        """ì•¡ì…˜ ì‹¤í–‰ ë° ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        try:
            if action["type"] == "click":
                # í˜„ì¬ ë§ˆìš°ìŠ¤ ìœ„ì¹˜ì—ì„œ ì˜¤í”„ì…‹ë§Œí¼ ì´ë™ í›„ í´ë¦­
                x, y = self.controller.mouse.get_cursor_pos()
                x += action["params"]["x_offset"]
                y += action["params"]["y_offset"]
                self.controller.mouse.move_mouse_natural(x, y)
                self.controller.mouse.click_precise()
                reward = 0.1
                
            elif action["type"] == "key":
                keys = action["params"]["key"].split("+")
                self.controller.keyboard.send_key_combination(*keys)
                reward = 0.5
                
            elif action["type"] == "type":
                text = action["params"]["text"]
                self.controller.keyboard.type_with_timing(text)
                reward = 1.0
                
            elif action["type"] == "menu":
                path = action["params"]["path"]
                if self.controller.navigate_to_menu(path):
                    reward = 2.0
                else:
                    reward = self.reward_config["invalid_action"]
                    
            elif action["type"] == "click_ui":
                element_name = action["params"]["element"]
                element = self.controller.ui_state.get(element_name)
                if element:
                    x = element.position[0] + element.size[0] // 2
                    y = element.position[1] + element.size[1] // 2
                    self.controller.mouse.move_mouse_natural(x, y)
                    self.controller.mouse.click_precise()
                    reward = 1.0
                else:
                    reward = self.reward_config["invalid_action"]
                    
            elif action["type"] == "scan_ui":
                self.controller.scan_ui_state()
                reward = 0.1
                
            elif action["type"] == "wait":
                time.sleep(action["params"]["duration"])
                reward = self.reward_config["idle_penalty"]
                
            # ìƒíƒœ ë³€í™”ì— ë”°ë¥¸ ì¶”ê°€ ë³´ìƒ
            reward += self._calculate_state_change_reward()
            
        except Exception as e:
            logger.error(f"ì•¡ì…˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            reward = self.reward_config["error_occurred"]
            
        return reward
        
    def _get_current_state(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ (í™”ë©´) ìº¡ì²˜"""
        screenshot = self.controller.vision.capture_screen_fast()
        
        # í¬ê¸° ì¡°ì • ë° ì •ê·œí™”
        import cv2
        screenshot = cv2.resize(screenshot, (224, 224))
        screenshot = screenshot.astype(np.float32) / 255.0
        
        return screenshot.flatten()
        
    def _calculate_state_change_reward(self) -> float:
        """ìƒíƒœ ë³€í™”ì— ë”°ë¥¸ ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        # UI ìš”ì†Œ ë³€í™” ê°ì§€
        current_elements = set(self.controller.ui_state.keys())
        
        # ë…¸ë“œê°€ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        scene_dock = self.controller.ui_state.get(GodotUIElement.SCENE_DOCK.value)
        if scene_dock and "Node2D" in scene_dock.text_content:
            reward += self.reward_config["node_created"] * 0.1
            
        return reward
        
    def _is_task_completed(self) -> bool:
        """ì‘ì—… ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
        # ì˜ˆ: íŠ¹ì • ì”¬ì´ ìƒì„±ë˜ê³  ì €ì¥ë˜ì—ˆëŠ”ì§€
        return False
        
    def _is_error_state(self) -> bool:
        """ì—ëŸ¬ ìƒíƒœ í™•ì¸"""
        # ì—ëŸ¬ ë‹¤ì´ì–¼ë¡œê·¸ ê°ì§€
        return False

class A3CNetwork(nn.Module):
    """Actor-Critic ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim: int, num_actions: int):
        super().__init__()
        
        # ê³µìœ  ë ˆì´ì–´
        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        
        # ê³„ì‚°ëœ íŠ¹ì§• í¬ê¸°
        conv_out_size = self._get_conv_out_size((3, 224, 224))
        
        self.fc = nn.Linear(conv_out_size, 512)
        
        # Actor (ì •ì±…)
        self.actor = nn.Linear(512, num_actions)
        
        # Critic (ê°€ì¹˜)
        self.critic = nn.Linear(512, 1)
        
    def _get_conv_out_size(self, shape):
        """Conv ë ˆì´ì–´ ì¶œë ¥ í¬ê¸° ê³„ì‚°"""
        o = torch.zeros(1, *shape)
        o = self.conv1(o)
        o = self.conv2(o)
        o = self.conv3(o)
        return int(np.prod(o.size()))
        
    def forward(self, x):
        # ì…ë ¥ reshape (flatten -> image)
        x = x.view(-1, 3, 224, 224)
        
        # Convolutional layers
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        
        # Shared FC
        x = F.relu(self.fc(x))
        
        # Actor-Critic outputs
        policy = F.softmax(self.actor(x), dim=1)
        value = self.critic(x)
        
        return policy, value

class GodotRLAgent:
    """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, env: GodotEnvironment):
        self.env = env
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ë„¤íŠ¸ì›Œí¬
        self.network = A3CNetwork(
            input_dim=env.observation_space,
            num_actions=len(env.action_space)
        ).to(self.device)
        
        self.optimizer = optim.Adam(self.network.parameters(), lr=0.0001)
        
        # ê²½í—˜ ë²„í¼
        self.memory = []
        self.gamma = 0.99
        self.tau = 0.95
        
        # í•™ìŠµ í†µê³„
        self.episode_rewards = []
        self.episode_lengths = []
        
    def select_action(self, state: np.ndarray) -> int:
        """ì•¡ì…˜ ì„ íƒ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            policy, _ = self.network(state_tensor)
            
        # í™•ë¥ ì  ì•¡ì…˜ ì„ íƒ
        dist = Categorical(policy)
        action = dist.sample()
        
        return action.item()
        
    def train_episode(self) -> float:
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        state = self.env.reset()
        episode_reward = 0
        
        while True:
            # ì•¡ì…˜ ì„ íƒ
            action = self.select_action(state)
            
            # í™˜ê²½ì—ì„œ ì‹¤í–‰
            next_state, reward, done, info = self.env.step(action)
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            self.memory.append((state, action, reward, next_state, done))
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
                
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ í•™ìŠµ
        self.update()
        
        # í†µê³„ ê¸°ë¡
        self.episode_rewards.append(episode_reward)
        self.episode_lengths.append(self.env.episode_steps)
        
        return episode_reward
        
    def update(self):
        """ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        if len(self.memory) == 0:
            return
            
        # ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        states, actions, rewards, next_states, dones = zip(*self.memory)
        
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # í˜„ì¬ ê°€ì¹˜ ê³„ì‚°
        policies, values = self.network(states)
        _, next_values = self.network(next_states)
        
        # TD íƒ€ê²Ÿ
        targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)
        
        # ì†ì‹¤ ê³„ì‚°
        value_loss = F.mse_loss(values.squeeze(), targets.detach())
        
        # ì •ì±… ì†ì‹¤ (REINFORCE with baseline)
        advantages = targets - values.squeeze()
        log_probs = torch.log(policies.gather(1, actions.unsqueeze(1)) + 1e-8)
        policy_loss = -(log_probs.squeeze() * advantages.detach()).mean()
        
        # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒìƒ‰ ì´‰ì§„)
        entropy = -(policies * torch.log(policies + 1e-8)).sum(dim=1).mean()
        
        # ì „ì²´ ì†ì‹¤
        total_loss = value_loss + policy_loss - 0.01 * entropy
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        self.optimizer.step()
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = []
        
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'network_state': self.network.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths
        }, path)
        
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.network.load_state_dict(checkpoint['network_state'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        self.episode_rewards = checkpoint.get('episode_rewards', [])
        self.episode_lengths = checkpoint.get('episode_lengths', [])

class GodotAutomationOrchestrator:
    """ìë™í™” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.env = GodotEnvironment()
        self.agent = GodotRLAgent(self.env)
        
        # ì‘ì—… í
        self.task_queue = queue.Queue()
        self.current_task = None
        
        # í•™ìŠµ ìŠ¤ë ˆë“œ
        self.training_thread = None
        self.is_training = False
        
    def add_task(self, task_type: str, params: Dict = None):
        """ì‘ì—… ì¶”ê°€"""
        task = {
            "type": task_type,
            "params": params or {},
            "status": "pending",
            "created_at": datetime.now().isoformat()
        }
        self.task_queue.put(task)
        
    def start_training(self, num_episodes: int = 1000):
        """í•™ìŠµ ì‹œì‘"""
        self.is_training = True
        self.training_thread = threading.Thread(
            target=self._training_loop,
            args=(num_episodes,)
        )
        self.training_thread.start()
        
    def _training_loop(self, num_episodes: int):
        """í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ ê°•í™”í•™ìŠµ ì‹œì‘ ({num_episodes} ì—í”¼ì†Œë“œ)")
        
        for episode in range(num_episodes):
            if not self.is_training:
                break
                
            # ì‘ì—…ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
            if not self.task_queue.empty():
                self.current_task = self.task_queue.get()
                logger.info(f"ì‘ì—… ì‹œì‘: {self.current_task['type']}")
                
            # ì—í”¼ì†Œë“œ í•™ìŠµ
            reward = self.agent.train_episode()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if episode % 10 == 0:
                avg_reward = np.mean(self.agent.episode_rewards[-10:])
                logger.info(f"Episode {episode}, Avg Reward: {avg_reward:.2f}")
                
            # ì£¼ê¸°ì  ì €ì¥
            if episode % 100 == 0:
                self.agent.save_model(f"godot_rl_model_ep{episode}.pth")
                
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ")
        
    def stop_training(self):
        """í•™ìŠµ ì¤‘ì§€"""
        self.is_training = False
        if self.training_thread:
            self.training_thread.join()
            
    def execute_learned_task(self, task_type: str):
        """í•™ìŠµëœ ì‘ì—… ì‹¤í–‰"""
        logger.info(f"ğŸ¤– í•™ìŠµëœ ì‘ì—… ì‹¤í–‰: {task_type}")
        
        state = self.env.reset()
        total_reward = 0
        
        for step in range(self.env.max_steps_per_episode):
            # ìµœì  ì•¡ì…˜ ì„ íƒ (íƒìš•ì )
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
                policy, _ = self.agent.network(state_tensor)
                action = policy.argmax().item()
                
            # ì‹¤í–‰
            state, reward, done, info = self.env.step(action)
            total_reward += reward
            
            if done:
                break
                
        logger.info(f"âœ… ì‘ì—… ì™„ë£Œ. ì´ ë³´ìƒ: {total_reward:.2f}")
        return total_reward

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í”„ë¡œì íŠ¸ ê²½ë¡œ
    project_path = "D:/MyGodotProject"
    
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    orchestrator = GodotAutomationOrchestrator(project_path)
    
    # ì‘ì—… ì¶”ê°€
    orchestrator.add_task("create_2d_platformer")
    orchestrator.add_task("create_ui_system")
    orchestrator.add_task("setup_player_controller")
    
    # í•™ìŠµ ì‹œì‘
    orchestrator.start_training(num_episodes=500)
    
    # í•™ìŠµ ì™„ë£Œ ëŒ€ê¸°
    input("í•™ìŠµ ì¤‘... Enterë¥¼ ëˆŒëŸ¬ ì¤‘ì§€: ")
    orchestrator.stop_training()
    
    # í•™ìŠµëœ ëª¨ë¸ë¡œ ì‘ì—… ì‹¤í–‰
    orchestrator.agent.load_model("godot_rl_model_ep400.pth")
    orchestrator.execute_learned_task("create_2d_platformer")