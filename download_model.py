#!/usr/bin/env python3
"""
Code Llama 7B-Instruct ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì •
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import shutil
import torch
from huggingface_hub import snapshot_download, login
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelDownloader:
    """Code Llama ëª¨ë¸ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self):
        self.base_dir = Path(".")
        self.model_name = "codellama/CodeLlama-7b-Instruct-hf"
        self.model_dir = self.base_dir / "CodeLlama-7b-Instruct-hf"
        
    def check_requirements(self):
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        logger.info("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
        
        # Python ë²„ì „ í™•ì¸
        if sys.version_info < (3, 8):
            logger.error("âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤")
            return False
        
        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        stat = shutil.disk_usage(".")
        free_gb = stat.free / (1024**3)
        
        if free_gb < 20:
            logger.warning(f"âš ï¸  ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ì´ {free_gb:.1f}GBì…ë‹ˆë‹¤. ìµœì†Œ 20GB ì´ìƒ ê¶Œì¥")
            response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                return False
        
        # PyTorch í™•ì¸
        try:
            import torch
            logger.info(f"âœ… PyTorch {torch.__version__} í™•ì¸")
            
            # CUDA í™•ì¸
            if torch.cuda.is_available():
                logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
                vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"   GPU VRAM: {vram:.1f}GB")
            else:
                logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ëŠë¦¼)")
                
        except ImportError:
            logger.error("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        return True
    
    def check_existing_model(self):
        """ê¸°ì¡´ ëª¨ë¸ í™•ì¸"""
        if self.model_dir.exists() and any(self.model_dir.iterdir()):
            logger.info(f"âœ… ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.model_dir}")
            
            # ëª¨ë¸ ë¬´ê²°ì„± í™•ì¸
            required_files = [
                "config.json",
                "tokenizer_config.json",
                "pytorch_model.bin.index.json"
            ]
            
            missing_files = []
            for file in required_files:
                if not (self.model_dir / file).exists():
                    missing_files.append(file)
            
            if missing_files:
                logger.warning(f"âš ï¸  ì¼ë¶€ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_files}")
                response = input("ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
                if response.lower() == 'y':
                    shutil.rmtree(self.model_dir)
                    return False
            
            return True
        return False
    
    def download_model(self):
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        logger.info(f"ğŸ“¥ {self.model_name} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        logger.info("   í¬ê¸°: ì•½ 13GB (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        try:
            # Hugging Face ë¡œê·¸ì¸ (ì„ íƒì‚¬í•­)
            token = os.getenv("HUGGINGFACE_TOKEN")
            if token:
                login(token=token)
                logger.info("âœ… Hugging Face í† í°ìœ¼ë¡œ ë¡œê·¸ì¸ë¨")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            logger.info("ğŸ“¦ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            snapshot_download(
                repo_id=self.model_name,
                local_dir=str(self.model_dir),
                local_dir_use_symlinks=False,
                resume_download=True,
                cache_dir=None  # ìºì‹œ ì‚¬ìš© ì•ˆ í•¨
            )
            
            logger.info("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            
            # ëª¨ë¸ í¬ê¸° í™•ì¸
            total_size = sum(f.stat().st_size for f in self.model_dir.rglob("*") if f.is_file())
            size_gb = total_size / (1024**3)
            logger.info(f"   ì´ í¬ê¸°: {size_gb:.2f}GB")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            logger.info("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            logger.info("1. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”")
            logger.info("2. Hugging Faceê°€ ì°¨ë‹¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            logger.info("3. ë””ìŠ¤í¬ ê³µê°„ì´ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”")
            logger.info("4. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì‹œë„:")
            logger.info(f"   git lfs clone https://huggingface.co/{self.model_name}")
            return False
    
    def verify_model(self):
        """ëª¨ë¸ ê²€ì¦"""
        logger.info("ğŸ” ëª¨ë¸ ê²€ì¦ ì¤‘...")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸
            logger.info("   í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸...")
            tokenizer = AutoTokenizer.from_pretrained(str(self.model_dir))
            
            # ê°„ë‹¨í•œ í† í°í™” í…ŒìŠ¤íŠ¸
            test_text = "Hello, World!"
            tokens = tokenizer.encode(test_text)
            decoded = tokenizer.decode(tokens)
            logger.info(f"   âœ… í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{test_text}' -> {len(tokens)} tokens")
            
            # ì„¤ì • íŒŒì¼ í™•ì¸
            config_path = self.model_dir / "config.json"
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            logger.info(f"   âœ… ëª¨ë¸ íƒ€ì…: {config.get('model_type', 'unknown')}")
            logger.info(f"   âœ… íˆë“  í¬ê¸°: {config.get('hidden_size', 'unknown')}")
            
            # ëª¨ë¸ íŒŒì¼ í™•ì¸
            model_files = list(self.model_dir.glob("pytorch_model*.bin"))
            if model_files:
                logger.info(f"   âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸: {len(model_files)}ê°œ ìƒ¤ë“œ")
            else:
                logger.warning("   âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def create_model_info(self):
        """ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±"""
        info = {
            "model_name": self.model_name,
            "model_path": str(self.model_dir),
            "download_date": str(Path.ctime(self.model_dir)),
            "verified": True,
            "requirements": {
                "min_ram_gb": 16,
                "recommended_ram_gb": 32,
                "gpu_vram_gb": 8
            },
            "usage": {
                "load_model": f"model = AutoModelForCausalLM.from_pretrained('{self.model_dir}')",
                "load_tokenizer": f"tokenizer = AutoTokenizer.from_pretrained('{self.model_dir}')"
            }
        }
        
        info_path = self.model_dir / "model_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“ ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}")
    
    def create_quick_test_script(self):
        """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        test_script = '''#!/usr/bin/env python3
"""Code Llama ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸ¤– Code Llama 7B-Instruct í…ŒìŠ¤íŠ¸")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
model_path = "./CodeLlama-7b-Instruct-hf"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
prompt = """### Instruction:
Write a simple C# function that calculates the factorial of a number.

### Response:
"""

print("ğŸ’­ ì½”ë“œ ìƒì„± ì¤‘...")
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    inputs.input_ids,
    max_length=200,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\\nâœ… ìƒì„±ëœ ì½”ë“œ:")
print(response)
'''
        
        test_path = self.base_dir / "test_model.py"
        with open(test_path, 'w', encoding='utf-8') as f:
            f.write(test_script)
        
        os.chmod(test_path, 0o755)
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {test_path}")
        logger.info("   ì‹¤í–‰: python test_model.py")
    
    def run(self, check_only=False):
        """ë©”ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Code Llama 7B-Instruct ëª¨ë¸ ë‹¤ìš´ë¡œë”")
        logger.info("="*50)
        
        # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not self.check_requirements():
            return False
        
        # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
        if self.check_existing_model():
            if check_only:
                logger.info("âœ… ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                return True
            
            response = input("\nëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                logger.info("âœ… ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
                return True
        
        if check_only:
            logger.info("âŒ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if not self.download_model():
            return False
        
        # ëª¨ë¸ ê²€ì¦
        if not self.verify_model():
            logger.warning("âš ï¸  ëª¨ë¸ ê²€ì¦ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
        
        # ëª¨ë¸ ì •ë³´ ìƒì„±
        self.create_model_info()
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        self.create_quick_test_script()
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ‰ ëª¨ë¸ ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("="*50)
        logger.info("\në‹¤ìŒ ë‹¨ê³„:")
        logger.info("1. ëª¨ë¸ í…ŒìŠ¤íŠ¸: python test_model.py")
        logger.info("2. ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘: python start_all.py")
        
        return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Code Llama 7B-Instruct ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸"
    )
    
    args = parser.parse_args()
    
    downloader = ModelDownloader()
    success = downloader.run(check_only=args.check_only)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()