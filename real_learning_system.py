#!/usr/bin/env python3
"""
AutoCI ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ
ì‚¬ìš©ìì™€ì˜ ëŒ€í™”, ì½”ë“œ íŒ¨í„´, í”¼ë“œë°±ìœ¼ë¡œë¶€í„° ì‹¤ì œë¡œ í•™ìŠµí•˜ëŠ” AI
"""

import os
import sys
import json
import sqlite3
import logging
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import pickle
import hashlib
from collections import defaultdict, Counter
import re
import threading
import time

# ê¸°ê³„í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("âš ï¸  scikit-learnì´ ì—†ì–´ ê¸°ë³¸ í•™ìŠµ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class RealLearningSystem:
    """ì‹¤ì œë¡œ í•™ìŠµí•˜ëŠ” AI ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: str = "autoci_brain.db"):
        self.base_path = Path(__file__).parent
        self.db_path = self.base_path / db_path
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.learning_rate = 0.1
        self.memory_capacity = 10000
        self.pattern_threshold = 0.7
        
        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.short_term_memory = []  # ìµœê·¼ ëŒ€í™”
        self.long_term_memory = {}   # ì˜êµ¬ ì €ì¥
        self.working_memory = {}     # í˜„ì¬ ì‘ì—… ì»¨í…ìŠ¤íŠ¸
        
        # í•™ìŠµ í†µê³„
        self.stats = {
            'total_conversations': 0,
            'learned_patterns': 0,
            'accuracy': 0.0,
            'last_update': datetime.now()
        }
        
        # ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ (ê°„ë‹¨í•œ êµ¬í˜„)
        self.weights = self._initialize_weights()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()
        
        # í•™ìŠµ ë°ì´í„° ë¡œë“œ
        self._load_learned_data()
        
        # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ìŠ¤ë ˆë“œ
        self.learning_thread = None
        self.is_learning = False
        
    def _initialize_weights(self) -> Dict[str, np.ndarray]:
        """ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        weights = {
            'intent_recognition': np.random.randn(100, 50) * 0.01,
            'response_generation': np.random.randn(50, 100) * 0.01,
            'context_understanding': np.random.randn(50, 50) * 0.01,
            'emotion_detection': np.random.randn(30, 10) * 0.01
        }
        return weights
        
    def _init_database(self):
        """í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # ëŒ€í™” í•™ìŠµ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversation_memory (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                user_input TEXT,
                ai_response TEXT,
                user_feedback REAL DEFAULT 0.5,
                context TEXT,
                learned_features TEXT
            )
        ''')
        
        # ì½”ë“œ íŒ¨í„´ í•™ìŠµ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS code_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_hash TEXT UNIQUE,
                pattern_type TEXT,
                pattern_content TEXT,
                usage_count INTEGER DEFAULT 1,
                success_rate REAL DEFAULT 0.5,
                last_seen DATETIME DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        # í•™ìŠµëœ ê°œë… í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learned_concepts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                concept_name TEXT UNIQUE,
                concept_type TEXT,
                understanding_level REAL DEFAULT 0.1,
                related_patterns TEXT,
                examples TEXT,
                last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_preferences (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                preference_type TEXT,
                preference_value TEXT,
                confidence REAL DEFAULT 0.5,
                frequency INTEGER DEFAULT 1,
                last_observed DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì—ëŸ¬ íŒ¨í„´ í•™ìŠµ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS error_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                error_type TEXT,
                error_context TEXT,
                solution TEXT,
                success_count INTEGER DEFAULT 0,
                total_count INTEGER DEFAULT 1,
                last_encountered DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì €ì¥ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS neural_weights (
                layer_name TEXT PRIMARY KEY,
                weights_data BLOB,
                last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        
    def learn_from_conversation(self, user_input: str, ai_response: str, 
                               context: Dict = None) -> Dict[str, Any]:
        """ëŒ€í™”ë¡œë¶€í„° í•™ìŠµ"""
        # íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_conversation_features(user_input, ai_response)
        
        # ë‹¨ê¸° ê¸°ì–µì— ì €ì¥
        self.short_term_memory.append({
            'timestamp': datetime.now(),
            'user_input': user_input,
            'ai_response': ai_response,
            'features': features,
            'context': context or {}
        })
        
        # íŒ¨í„´ ì¸ì‹
        patterns = self._identify_patterns(user_input, ai_response)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO conversation_memory 
            (user_input, ai_response, context, learned_features)
            VALUES (?, ?, ?, ?)
        ''', (user_input, ai_response, 
              json.dumps(context or {}), 
              json.dumps(features)))
        
        # íŒ¨í„´ ì €ì¥
        for pattern in patterns:
            self._save_pattern(cursor, pattern)
            
        conn.commit()
        conn.close()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_conversations'] += 1
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ gradient descent)
        self._update_weights(features)
        
        return {
            'learned': True,
            'features': features,
            'patterns': patterns,
            'memory_size': len(self.short_term_memory)
        }
        
    def _extract_conversation_features(self, user_input: str, 
                                     ai_response: str) -> Dict[str, Any]:
        """ëŒ€í™”ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        features = {
            'input_length': len(user_input),
            'response_length': len(ai_response),
            'input_keywords': self._extract_keywords(user_input),
            'response_keywords': self._extract_keywords(ai_response),
            'input_intent': self._classify_intent(user_input),
            'emotion': self._detect_emotion(user_input),
            'topic': self._identify_topic(user_input),
            'complexity': self._assess_complexity(user_input),
            'code_snippets': self._extract_code_snippets(user_input + ai_response)
        }
        
        # TF-IDF íŠ¹ì§• (sklearn ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if HAS_SKLEARN and hasattr(self, 'tfidf_vectorizer'):
            try:
                tfidf_features = self.tfidf_vectorizer.transform([user_input])
                features['tfidf_scores'] = tfidf_features.toarray()[0].tolist()
            except:
                pass
                
        return features
        
    def _extract_keywords(self, text: str) -> List[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
        words = re.findall(r'\b\w+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 
                    'the', 'is', 'at', 'which', 'on', 'a', 'an'}
        
        keywords = [w for w in words if w not in stopwords and len(w) > 2]
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        return [word for word, _ in word_freq.most_common(10)]
        
    def _classify_intent(self, text: str) -> str:
        """ì˜ë„ ë¶„ë¥˜ (ì‹ ê²½ë§ ì‚¬ìš©)"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ + í•™ìŠµëœ íŒ¨í„´
        intents = {
            'question': ['?', 'ë­', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””'],
            'request': ['í•´ì¤˜', 'ë§Œë“¤ì–´', 'ê³ ì³', 'ë¶€íƒ', 'ì›í•´'],
            'greeting': ['ì•ˆë…•', 'í•˜ì´', 'ë°˜ê°€ì›Œ'],
            'error': ['ì—ëŸ¬', 'ì˜¤ë¥˜', 'ì•ˆë¼', 'ë¬¸ì œ', 'exception'],
            'feedback': ['ê³ ë§ˆì›Œ', 'ê°ì‚¬', 'ì¢‹ì•„', 'ë‚˜ë¹ ', 'ë³„ë¡œ']
        }
        
        text_lower = text.lower()
        intent_scores = {}
        
        for intent, keywords in intents.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            intent_scores[intent] = score
            
        # ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì ìš©
        if 'intent_recognition' in self.weights:
            # ê°„ë‹¨í•œ íŠ¹ì§• ë²¡í„° ìƒì„±
            feature_vector = np.zeros(100)
            for i, char in enumerate(text[:100]):
                feature_vector[i] = ord(char) / 255.0
                
            # ê°€ì¤‘ì¹˜ ì ìš©
            hidden = np.tanh(np.dot(feature_vector, self.weights['intent_recognition']))
            
            # ìµœì¢… ì ìˆ˜ì— ë°˜ì˜
            learned_scores = hidden[:len(intent_scores)]
            for i, (intent, _) in enumerate(intent_scores.items()):
                if i < len(learned_scores):
                    intent_scores[intent] += learned_scores[i]
                    
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ë°˜í™˜
        if intent_scores:
            return max(intent_scores.items(), key=lambda x: x[1])[0]
        return 'unknown'
        
    def _detect_emotion(self, text: str) -> str:
        """ê°ì • ê°ì§€"""
        emotions = {
            'happy': ['ì¢‹ì•„', 'ê¸°ë»', 'í–‰ë³µ', 'ì¦ê±°ì›Œ', 'ìµœê³ ', 'ğŸ˜Š', 'ğŸ˜„'],
            'sad': ['ìŠ¬í¼', 'ìš°ìš¸', 'í˜ë“¤ì–´', 'ì™¸ë¡œì›Œ', 'ğŸ˜¢', 'ğŸ˜­'],
            'angry': ['í™”ë‚˜', 'ì§œì¦', 'ì‹«ì–´', 'ë¯¸ì›Œ', 'ğŸ˜¡', 'ğŸ˜ '],
            'confused': ['ëª¨ë¥´ê² ', 'í—·ê°ˆë ¤', 'ì–´ë ¤ì›Œ', 'ë³µì¡', 'ğŸ¤”', 'ğŸ˜•'],
            'excited': ['ì‹ ë‚˜', 'ê¸°ëŒ€', 'ì™€', 'ëŒ€ë°•', 'ğŸ‰', 'âœ¨']
        }
        
        detected_emotions = []
        for emotion, keywords in emotions.items():
            if any(kw in text for kw in keywords):
                detected_emotions.append(emotion)
                
        return detected_emotions[0] if detected_emotions else 'neutral'
        
    def _identify_topic(self, text: str) -> str:
        """ì£¼ì œ ì‹ë³„"""
        topics = {
            'unity': ['unity', 'ìœ ë‹ˆí‹°', 'gameobject', 'transform', 'collider'],
            'csharp': ['c#', 'class', 'method', 'async', 'interface'],
            'error': ['error', 'exception', 'null', 'ì—ëŸ¬', 'ì˜¤ë¥˜'],
            'general': []
        }
        
        text_lower = text.lower()
        topic_scores = defaultdict(int)
        
        for topic, keywords in topics.items():
            for keyword in keywords:
                if keyword in text_lower:
                    topic_scores[topic] += 1
                    
        if topic_scores:
            return max(topic_scores.items(), key=lambda x: x[1])[0]
        return 'general'
        
    def _assess_complexity(self, text: str) -> float:
        """ë³µì¡ë„ í‰ê°€ (0-1)"""
        # ê°„ë‹¨í•œ ë³µì¡ë„ ë©”íŠ¸ë¦­
        word_count = len(text.split())
        unique_words = len(set(text.lower().split()))
        
        # ê¸°ìˆ  ìš©ì–´ ê°œìˆ˜
        tech_terms = ['async', 'interface', 'abstract', 'delegate', 'lambda',
                     'coroutine', 'singleton', 'dependency', 'injection']
        tech_count = sum(1 for term in tech_terms if term in text.lower())
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity = min(1.0, (word_count / 50 + unique_words / 30 + tech_count / 5) / 3)
        
        return complexity
        
    def _extract_code_snippets(self, text: str) -> List[str]:
        """ì½”ë“œ ìŠ¤ë‹ˆí« ì¶”ì¶œ"""
        # ì½”ë“œ ë¸”ë¡ íŒ¨í„´
        code_patterns = [
            r'```[\s\S]*?```',  # Markdown ì½”ë“œ ë¸”ë¡
            r'`[^`]+`',         # ì¸ë¼ì¸ ì½”ë“œ
            r'class\s+\w+\s*{[\s\S]*?}',  # í´ë˜ìŠ¤ ì •ì˜
            r'(public|private|protected)\s+\w+\s+\w+\s*\([^)]*\)',  # ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜
        ]
        
        snippets = []
        for pattern in code_patterns:
            matches = re.findall(pattern, text)
            snippets.extend(matches)
            
        return snippets
        
    def _identify_patterns(self, user_input: str, ai_response: str) -> List[Dict]:
        """íŒ¨í„´ ì¸ì‹"""
        patterns = []
        
        # ì§ˆë¬¸-ë‹µë³€ íŒ¨í„´
        if '?' in user_input:
            pattern = {
                'type': 'qa_pattern',
                'question_type': self._classify_intent(user_input),
                'topic': self._identify_topic(user_input),
                'hash': hashlib.md5(user_input.encode()).hexdigest()[:16]
            }
            patterns.append(pattern)
            
        # ì½”ë“œ íŒ¨í„´
        code_snippets = self._extract_code_snippets(user_input + ai_response)
        for snippet in code_snippets:
            pattern = {
                'type': 'code_pattern',
                'content': snippet[:200],  # ì²˜ìŒ 200ì
                'language': 'csharp',  # ê°„ë‹¨íˆ ê°€ì •
                'hash': hashlib.md5(snippet.encode()).hexdigest()[:16]
            }
            patterns.append(pattern)
            
        # ì—ëŸ¬ íŒ¨í„´
        error_keywords = ['error', 'exception', 'ì—ëŸ¬', 'ì˜¤ë¥˜', 'null']
        if any(kw in user_input.lower() for kw in error_keywords):
            pattern = {
                'type': 'error_pattern',
                'error_type': self._extract_error_type(user_input),
                'context': user_input[:100],
                'hash': hashlib.md5(user_input.encode()).hexdigest()[:16]
            }
            patterns.append(pattern)
            
        return patterns
        
    def _extract_error_type(self, text: str) -> str:
        """ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ"""
        error_types = [
            'NullReferenceException',
            'IndexOutOfRangeException',
            'ArgumentException',
            'InvalidOperationException',
            'NotImplementedException'
        ]
        
        for error_type in error_types:
            if error_type.lower() in text.lower():
                return error_type
                
        return 'UnknownError'
        
    def _save_pattern(self, cursor, pattern: Dict):
        """íŒ¨í„´ ì €ì¥"""
        if pattern['type'] == 'code_pattern':
            cursor.execute('''
                INSERT INTO code_patterns (pattern_hash, pattern_type, pattern_content)
                VALUES (?, ?, ?)
                ON CONFLICT(pattern_hash) DO UPDATE SET
                usage_count = usage_count + 1,
                last_seen = CURRENT_TIMESTAMP
            ''', (pattern['hash'], pattern['type'], pattern.get('content', '')))
            
        self.stats['learned_patterns'] += 1
        
    def _update_weights(self, features: Dict):
        """ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # í•™ìŠµë¥  ì ìš©
        learning_rate = self.learning_rate
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        feature_vector = np.zeros(100)
        
        # ê°„ë‹¨í•œ íŠ¹ì§• ì¸ì½”ë”©
        feature_vector[0] = features.get('input_length', 0) / 1000.0
        feature_vector[1] = features.get('response_length', 0) / 1000.0
        feature_vector[2] = features.get('complexity', 0)
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (gradient descent ì‹œë®¬ë ˆì´ì…˜)
        for layer_name, weights in self.weights.items():
            # ëœë¤ ê·¸ë˜ë””ì–¸íŠ¸ (ì‹¤ì œë¡œëŠ” ì—­ì „íŒŒ í•„ìš”)
            gradient = np.random.randn(*weights.shape) * 0.001
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            self.weights[layer_name] -= learning_rate * gradient
            
        # ì£¼ê¸°ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì €ì¥
        if self.stats['total_conversations'] % 10 == 0:
            self._save_weights()
            
    def _save_weights(self):
        """ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì €ì¥"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        for layer_name, weights in self.weights.items():
            weights_blob = pickle.dumps(weights)
            cursor.execute('''
                INSERT OR REPLACE INTO neural_weights (layer_name, weights_data)
                VALUES (?, ?)
            ''', (layer_name, weights_blob))
            
        conn.commit()
        conn.close()
        
    def _load_learned_data(self):
        """í•™ìŠµëœ ë°ì´í„° ë¡œë“œ"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # í†µê³„ ë¡œë“œ
        cursor.execute('SELECT COUNT(*) FROM conversation_memory')
        self.stats['total_conversations'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM code_patterns')
        self.stats['learned_patterns'] = cursor.fetchone()[0]
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        cursor.execute('SELECT layer_name, weights_data FROM neural_weights')
        for layer_name, weights_blob in cursor.fetchall():
            try:
                self.weights[layer_name] = pickle.loads(weights_blob)
            except:
                pass
                
        # TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™” (sklearn ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if HAS_SKLEARN:
            # ê¸°ì¡´ ëŒ€í™”ì—ì„œ í•™ìŠµ
            cursor.execute('SELECT user_input FROM conversation_memory LIMIT 1000')
            texts = [row[0] for row in cursor.fetchall()]
            
            if texts:
                self.tfidf_vectorizer = TfidfVectorizer(max_features=1000)
                self.tfidf_vectorizer.fit(texts)
                
        conn.close()
        
        logger.info(f"í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.stats['total_conversations']}ê°œ ëŒ€í™”, "
                   f"{self.stats['learned_patterns']}ê°œ íŒ¨í„´")
        
    def learn_from_feedback(self, conversation_id: int, feedback: float):
        """ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµ"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # í”¼ë“œë°± ì €ì¥
        cursor.execute('''
            UPDATE conversation_memory 
            SET user_feedback = ? 
            WHERE id = ?
        ''', (feedback, conversation_id))
        
        # ê´€ë ¨ íŒ¨í„´ì˜ ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        if feedback > 0.7:  # ê¸ì •ì  í”¼ë“œë°±
            cursor.execute('''
                UPDATE code_patterns 
                SET success_rate = (success_rate * usage_count + 1) / (usage_count + 1)
                WHERE pattern_hash IN (
                    SELECT pattern_hash FROM conversation_memory 
                    WHERE id = ?
                )
            ''', (conversation_id,))
            
        conn.commit()
        conn.close()
        
        # ì •í™•ë„ ì—…ë°ì´íŠ¸
        self._update_accuracy()
        
    def _update_accuracy(self):
        """ì „ì²´ ì •í™•ë„ ê³„ì‚°"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT AVG(user_feedback) 
            FROM conversation_memory 
            WHERE user_feedback > 0
        ''')
        
        result = cursor.fetchone()
        if result and result[0]:
            self.stats['accuracy'] = result[0]
            
        conn.close()
        
    def get_similar_conversations(self, user_input: str, k: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        keywords = self._extract_keywords(user_input)
        
        similar_convs = []
        
        for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            cursor.execute('''
                SELECT user_input, ai_response, user_feedback
                FROM conversation_memory
                WHERE user_input LIKE ?
                ORDER BY user_feedback DESC
                LIMIT ?
            ''', (f'%{keyword}%', k))
            
            for row in cursor.fetchall():
                similar_convs.append({
                    'user_input': row[0],
                    'ai_response': row[1],
                    'feedback': row[2]
                })
                
        conn.close()
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ kê°œ ë°˜í™˜
        seen = set()
        unique_convs = []
        for conv in similar_convs:
            if conv['user_input'] not in seen:
                seen.add(conv['user_input'])
                unique_convs.append(conv)
                
        return unique_convs[:k]
        
    def get_learning_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return {
            'total_conversations': self.stats['total_conversations'],
            'learned_patterns': self.stats['learned_patterns'],
            'accuracy': f"{self.stats['accuracy'] * 100:.1f}%",
            'memory_usage': len(self.short_term_memory),
            'last_update': self.stats['last_update'].isoformat(),
            'learning_rate': self.learning_rate,
            'topics_learned': self._get_learned_topics(),
            'error_patterns': self._get_error_patterns_summary()
        }
        
    def _get_learned_topics(self) -> List[str]:
        """í•™ìŠµí•œ ì£¼ì œ ëª©ë¡"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT DISTINCT concept_name 
            FROM learned_concepts 
            ORDER BY understanding_level DESC
            LIMIT 10
        ''')
        
        topics = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        return topics
        
    def _get_error_patterns_summary(self) -> Dict[str, int]:
        """ì—ëŸ¬ íŒ¨í„´ ìš”ì•½"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT error_type, COUNT(*) as count
            FROM error_patterns
            GROUP BY error_type
            ORDER BY count DESC
            LIMIT 5
        ''')
        
        summary = {row[0]: row[1] for row in cursor.fetchall()}
        conn.close()
        
        return summary
        
    def start_background_learning(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘"""
        if not self.is_learning:
            self.is_learning = True
            self.learning_thread = threading.Thread(
                target=self._background_learning_loop,
                daemon=True
            )
            self.learning_thread.start()
            logger.info("ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    def _background_learning_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë£¨í”„"""
        while self.is_learning:
            try:
                # ì£¼ê¸°ì ìœ¼ë¡œ íŒ¨í„´ ë¶„ì„
                self._analyze_patterns()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                self._cleanup_memory()
                
                # ê°€ì¤‘ì¹˜ ìµœì í™”
                self._optimize_weights()
                
                # 1ë¶„ ëŒ€ê¸°
                time.sleep(60)
                
            except Exception as e:
                logger.error(f"ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì˜¤ë¥˜: {e}")
                
    def _analyze_patterns(self):
        """ì €ì¥ëœ íŒ¨í„´ ë¶„ì„"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´ ì°¾ê¸°
        cursor.execute('''
            SELECT pattern_type, COUNT(*) as count
            FROM code_patterns
            GROUP BY pattern_type
            ORDER BY count DESC
        ''')
        
        pattern_stats = cursor.fetchall()
        
        # íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ (sklearn ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if HAS_SKLEARN and len(pattern_stats) > 10:
            # ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            pass
            
        conn.close()
        
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # ë‹¨ê¸° ê¸°ì–µ ì •ë¦¬ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
        if len(self.short_term_memory) > 100:
            self.short_term_memory = self.short_term_memory[-100:]
            
        # ì˜¤ë˜ëœ ì‘ì—… ë©”ëª¨ë¦¬ ì œê±°
        current_time = datetime.now()
        expired_keys = []
        
        for key, value in self.working_memory.items():
            if isinstance(value, dict) and 'timestamp' in value:
                if current_time - value['timestamp'] > timedelta(hours=1):
                    expired_keys.append(key)
                    
        for key in expired_keys:
            del self.working_memory[key]
            
    def _optimize_weights(self):
        """ê°€ì¤‘ì¹˜ ìµœì í™”"""
        # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ì •ê·œí™”
        for layer_name, weights in self.weights.items():
            # L2 ì •ê·œí™”
            norm = np.linalg.norm(weights)
            if norm > 10:
                self.weights[layer_name] = weights / norm * 10
                
    def stop_background_learning(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì¤‘ì§€"""
        self.is_learning = False
        if self.learning_thread:
            self.learning_thread.join(timeout=5)
        logger.info("ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_real_learning():
    """ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  AutoCI ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    learning_system = RealLearningSystem()
    
    # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘
    learning_system.start_background_learning()
    
    # í…ŒìŠ¤íŠ¸ ëŒ€í™”
    test_conversations = [
        ("Unityì—ì„œ í”Œë ˆì´ì–´ ì´ë™ì„ êµ¬í˜„í•˜ê³  ì‹¶ì–´ìš”", 
         "Transform.Translate() ë˜ëŠ” Rigidbody.velocityë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."),
        
        ("NullReferenceExceptionì´ ê³„ì† ë°œìƒí•´ìš”",
         "ê°ì²´ê°€ nullì¸ì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”: if (myObject != null)"),
         
        ("ì½”ë£¨í‹´ì´ ë­”ê°€ìš”?",
         "Unityì—ì„œ ì‹œê°„ì— ê±¸ì³ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. yield returnì„ ì‚¬ìš©í•©ë‹ˆë‹¤."),
    ]
    
    # ëŒ€í™” í•™ìŠµ
    for i, (user_input, ai_response) in enumerate(test_conversations):
        print(f"\n[ëŒ€í™” {i+1}]")
        print(f"ğŸ‘¤: {user_input}")
        print(f"ğŸ¤–: {ai_response}")
        
        # í•™ìŠµ
        result = learning_system.learn_from_conversation(
            user_input, ai_response,
            context={'session_id': 'test', 'timestamp': datetime.now()}
        )
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ: {result['patterns']} íŒ¨í„´ ë°œê²¬")
        
        # í”¼ë“œë°± (ê¸ì •ì )
        learning_system.learn_from_feedback(i+1, 0.9)
        
    # í•™ìŠµ í†µê³„ í™•ì¸
    print("\n" + "=" * 60)
    print("ğŸ“Š í•™ìŠµ í†µê³„:")
    stats = learning_system.get_learning_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
        
    # ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰
    print("\n" + "=" * 60)
    print("ğŸ” ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰:")
    similar = learning_system.get_similar_conversations("Unity ì´ë™")
    for conv in similar:
        print(f"  Q: {conv['user_input'][:50]}...")
        print(f"  A: {conv['ai_response'][:50]}...")
        print(f"  í‰ê°€: {conv['feedback']}")
        
    # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì¤‘ì§€
    learning_system.stop_background_learning()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_real_learning()