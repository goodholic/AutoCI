#!/usr/bin/env python3
"""
RTX 2080 8GB ìµœì í™” LLM ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
8GB VRAM + 32GB RAM í™˜ê²½ì— ì™„ë²½í•œ ëª¨ë¸ë“¤ë§Œ ì„¤ì¹˜
"""

import os
import json
import time
import logging
from pathlib import Path
from huggingface_hub import snapshot_download

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hugging Face í† í°
HF_TOKEN = "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"

# RTX 2080 8GB ìµœì í™” ëª¨ë¸ ì„¤ì •
RTX_2080_MODELS = {
    "bitnet-b1.58-2b": {
        "model_id": "microsoft/BitNet-b1.58-2B-4T-gguf", 
        "size_gb": 0.5,
        "quantization": "1.58bit",
        "features": ["general", "korean", "efficient"],
        "vram_gb": 1,
        "description": "í˜ì‹ ì ì¸ 1.58bit ëª¨ë¸, ê·¹ë„ë¡œ ê²½ëŸ‰"
    },
    "gemma-4b": {
        "model_id": "google/gemma-4b-it",
        "size_gb": 2.5,
        "quantization": "4bit", 
        "features": ["general", "korean", "csharp"],
        "vram_gb": 4,
        "description": "Google Gemma 4B, ë›°ì–´ë‚œ ì„±ëŠ¥ ëŒ€ë¹„ í¬ê¸°"
    },
    "phi3-mini": {
        "model_id": "microsoft/Phi-3-mini-4k-instruct",
        "size_gb": 4.0,
        "quantization": "4bit",
        "features": ["reasoning", "csharp", "math"],
        "vram_gb": 6,
        "description": "Microsoft Phi-3 Mini, ì¶”ë¡  íŠ¹í™”"
    },
    "deepseek-coder-7b": {
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "size_gb": 4.2,
        "quantization": "4bit",
        "features": ["code", "csharp", "godot"],
        "vram_gb": 6,
        "description": "DeepSeek ì½”ë”© íŠ¹í™” ëª¨ë¸"
    },
    "mistral-7b": {
        "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
        "size_gb": 4.5,
        "quantization": "4bit",
        "features": ["general", "fast", "csharp"],
        "vram_gb": 7,
        "description": "Mistral 7B, ë¹ ë¥¸ ì¶”ë¡  ì†ë„"
    }
}

def install_model(model_name: str, model_info: dict, models_dir: Path):
    """RTX 2080 ìµœì í™” ëª¨ë¸ ì„¤ì¹˜"""
    model_path = models_dir / model_name
    model_path.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸš€ {model_name} ì„¤ì¹˜ ì‹œì‘...")
    logger.info(f"   ğŸ“¦ í¬ê¸°: {model_info['size_gb']}GB")
    logger.info(f"   ğŸ¯ VRAM: {model_info['vram_gb']}GB")
    logger.info(f"   ğŸ“ ì„¤ëª…: {model_info['description']}")
    
    try:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=model_info['model_id'],
            local_dir=str(model_path),
            token=HF_TOKEN,
            revision="main"
        )
        
        # ì„¤ì¹˜ ì •ë³´ ì €ì¥
        install_info = {
            "installed_at": time.strftime('%Y-%m-%dT%H:%M:%S'),
            "model_id": model_info['model_id'],
            "size_gb": model_info['size_gb'],
            "vram_gb": model_info['vram_gb'],
            "quantization": model_info['quantization'],
            "features": model_info['features'],
            "rtx_2080_optimized": True
        }
        
        with open(model_path / "install_info.json", 'w') as f:
            json.dump(install_info, f, indent=2)
            
        logger.info(f"âœ… {model_name} ì„¤ì¹˜ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {model_name} ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}")
        return False

def update_installed_models_json(models_dir: Path):
    """installed_models.json ì—…ë°ì´íŠ¸"""
    installed_models = {}
    
    for model_name, model_info in RTX_2080_MODELS.items():
        model_path = models_dir / model_name
        if (model_path / "install_info.json").exists():
            installed_models[model_name] = {
                "model_id": model_info['model_id'],
                "path": str(model_path),
                "quantization": model_info['quantization'],
                "features": model_info['features'],
                "installed_at": time.strftime('%Y-%m-%dT%H:%M:%S'),
                "size_gb": model_info['size_gb'],
                "vram_gb": model_info['vram_gb'],
                "status": "installed",
                "rtx_2080_optimized": True
            }
    
    models_info_file = models_dir / "installed_models.json"
    with open(models_info_file, 'w', encoding='utf-8') as f:
        json.dump(installed_models, f, indent=2, ensure_ascii=False)
        
    logger.info(f"ğŸ“‹ installed_models.json ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def main():
    """RTX 2080 ìµœì í™” ëª¨ë¸ ì„¤ì¹˜ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ RTX 2080 8GB + 32GB RAM ìµœì í™” LLM ëª¨ë¸ ì„¤ì¹˜")
    print("=" * 60)
    print("ğŸ’» ìµœì í™” ê¸°ì¤€:")
    print("  - GPU VRAM: 8GB ì´í•˜")
    print("  - ì‹œìŠ¤í…œ RAM: 32GB í˜¸í™˜")
    print("  - ì–‘ìí™”: 4bit ì´í•˜")
    print("  - ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥")
    print("=" * 60)
    
    # í™˜ê²½ í™•ì¸
    models_dir = Path("./models")
    models_dir.mkdir(exist_ok=True)
    
    # ê¸°ì¡´ ë¬´ê±°ìš´ ëª¨ë¸ ë°±ì—…
    backup_dir = models_dir / "backup_heavy_models"
    backup_dir.mkdir(exist_ok=True)
    
    heavy_models = ["llama-3.1-8b", "codellama-13b", "qwen2.5-coder-32b"]
    for heavy_model in heavy_models:
        heavy_path = models_dir / heavy_model
        if heavy_path.exists():
            backup_path = backup_dir / heavy_model
            if not backup_path.exists():
                logger.info(f"ğŸ“¦ ë¬´ê±°ìš´ ëª¨ë¸ ë°±ì—…: {heavy_model}")
                heavy_path.rename(backup_path)
    
    # RTX 2080 ìµœì í™” ëª¨ë¸ë“¤ ì„¤ì¹˜
    success_count = 0
    total_size = 0
    
    for model_name, model_info in RTX_2080_MODELS.items():
        print(f"\nğŸ”„ {model_name} ì„¤ì¹˜ ì¤‘...")
        if install_model(model_name, model_info, models_dir):
            success_count += 1
            total_size += model_info['size_gb']
        
        # ì„¤ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸°
        time.sleep(2)
    
    # ì„¤ì¹˜ ì™„ë£Œ í›„ ì„¤ì • ì—…ë°ì´íŠ¸
    update_installed_models_json(models_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ RTX 2080 ìµœì í™” ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}/{len(RTX_2080_MODELS)} ëª¨ë¸")
    print(f"ğŸ’¾ ì´ í¬ê¸°: {total_size:.1f}GB")
    print(f"ğŸ¯ VRAM ì‚¬ìš©ëŸ‰: ìµœëŒ€ 7GB (8GB í•œê³„ ë‚´)")
    print("\nğŸš€ ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
    print("   autoci learn low  # RTX 2080 ìµœì í™” í•™ìŠµ")
    print("=" * 60)

if __name__ == "__main__":
    main() 