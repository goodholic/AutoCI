#!/usr/bin/env python3
"""
DeepSeek-coder-v2 6.7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
RTX 2080 8GB + 32GB RAM ìµœì í™”
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path

def setup_environment():
    """ê°€ìƒí™˜ê²½ ì„¤ì • í™•ì¸"""
    venv_path = Path("autoci_env")
    if not venv_path.exists():
        print("âŒ ê°€ìƒí™˜ê²½ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ì„¸ìš”:")
        print("python -m venv autoci_env")
        return False
    
    # Windowsì—ì„œëŠ” Scripts, Linuxì—ì„œëŠ” bin
    activate_script = venv_path / "Scripts" / "activate" if os.name == 'nt' else venv_path / "bin" / "activate"
    if not activate_script.exists():
        print("âŒ ê°€ìƒí™˜ê²½ í™œì„±í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    print("âœ… ê°€ìƒí™˜ê²½ í™•ì¸ ì™„ë£Œ")
    return True

def check_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    print("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"âœ… GPU: {gpu_name} ({vram:.1f}GB VRAM)")
        
        if vram < 6:
            print("âš ï¸ ê²½ê³ : VRAMì´ 6GB ë¯¸ë§Œì…ë‹ˆë‹¤. ì–‘ìí™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # RAM í™•ì¸ (ëŒ€ëµì )
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / (1024**3)
        print(f"âœ… ì‹œìŠ¤í…œ RAM: {ram_gb:.1f}GB")
        
        if ram_gb < 16:
            print("âš ï¸ ê²½ê³ : ì‹œìŠ¤í…œ RAMì´ 16GB ë¯¸ë§Œì…ë‹ˆë‹¤.")
    except ImportError:
        print("â„¹ï¸ psutilì´ ì—†ì–´ RAM í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    free_space = os.statvfs('.').f_bavail * os.statvfs('.').f_frsize / (1024**3)
    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„: {free_space:.1f}GB")
    
    if free_space < 15:
        print("âŒ ì˜¤ë¥˜: ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (15GB ì´ìƒ í•„ìš”)")
        return False
    
    return True

def download_deepseek_coder():
    """DeepSeek-coder 6.7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    print("ğŸš€ DeepSeek-coder-v2 6.7B ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    model_id = "deepseek-ai/deepseek-coder-6.7b-instruct"
    model_dir = Path("models") / "deepseek-coder-7b"
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
    model_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True,
            cache_dir=model_dir / "cache"
        )
        
        print("ğŸ’¾ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
        tokenizer.save_pretrained(model_dir / "tokenizer")
        
        print("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
            cache_dir=model_dir / "cache"
        )
        
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        model.save_pretrained(model_dir / "model")
        
        # ì„¤ì¹˜ ì •ë³´ ì €ì¥
        install_info = {
            "model_id": model_id,
            "model_name": "deepseek-coder-7b",
            "version": "6.7b-instruct",
            "download_date": str(Path().cwd()),
            "size_gb": 14.2,
            "vram_requirement_gb": 6,
            "optimized_for": "RTX 2080 8GB",
            "quantization": "bfloat16",
            "status": "installed"
        }
        
        with open(model_dir / "install_info.json", "w", encoding="utf-8") as f:
            json.dump(install_info, f, indent=2, ensure_ascii=False)
        
        print("âœ… DeepSeek-coder-v2 6.7B ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“ ì„¤ì¹˜ ìœ„ì¹˜: {model_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def update_model_config():
    """ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
    print("âš™ï¸ ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì¤‘...")
    
    config_file = Path("models") / "installed_models.json"
    
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)
        
        # deepseek-coder-7b ì„¤ì • ì—…ë°ì´íŠ¸
        if "deepseek-coder-7b" in config["models"]:
            config["models"]["deepseek-coder-7b"].update({
                "status": "installed",
                "model_path": "models/deepseek-coder-7b/model",
                "tokenizer_path": "models/deepseek-coder-7b/tokenizer",
                "last_updated": "2025-01-03",
                "rtx_2080_optimized": True,
                "priority_score": 10,  # ìµœê³  ìš°ì„ ìˆœìœ„
                "quantization_support": ["4bit", "8bit", "bfloat16"],
                "recommended_quantization": "bfloat16"
            })
        
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print("âœ… ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_model():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        model_dir = Path("models") / "deepseek-coder-7b"
        
        print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir / "tokenizer",
            trust_remote_code=True
        )
        
        print("ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            model_dir / "model",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        test_prompt = "# C#ì—ì„œ async/awaitë¥¼ ì‚¬ìš©í•œ ì˜ˆì œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”"
        
        messages = [
            {"role": "user", "content": test_prompt}
        ]
        
        inputs = tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            return_tensors="pt"
        ).to(model.device)
        
        print("ğŸ’­ í…ŒìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        
        print("âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ì‘ë‹µ:")
        print("-" * 50)
        print(response[:300] + "..." if len(response) > 300 else response)
        print("-" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ DeepSeek-coder-v2 6.7B ì„¤ì¹˜ í”„ë¡œê·¸ë¨")
    print("=" * 50)
    
    # í™˜ê²½ í™•ì¸
    if not setup_environment():
        return False
    
    if not check_system_requirements():
        return False
    
    # ì‚¬ìš©ì í™•ì¸
    print("\nğŸ“‹ ì„¤ì¹˜ ì •ë³´:")
    print("- ëª¨ë¸: deepseek-ai/deepseek-coder-6.7b-instruct")
    print("- í¬ê¸°: ~14GB")
    print("- VRAM ìš”êµ¬ì‚¬í•­: 6GB")
    print("- ìµœì í™” ëŒ€ìƒ: RTX 2080 8GB")
    
    confirm = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ ì„¤ì¹˜ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    
    # ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
    if not download_deepseek_coder():
        return False
    
    if not update_model_config():
        return False
    
    # í…ŒìŠ¤íŠ¸
    if not test_model():
        print("âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì„¤ì¹˜ëŠ” ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\nğŸ‰ DeepSeek-coder-v2 6.7B ì„¤ì¹˜ ì™„ë£Œ!")
    print("ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print("ğŸ‘‰ autoci learn low")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 