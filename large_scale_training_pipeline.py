#!/usr/bin/env python3
"""
Large-Scale Training Data Pipeline for Neural AutoCI
ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„° íŒŒì´í”„ë¼ì¸ - ChatGPT ìˆ˜ì¤€ì˜ ë°ì´í„° ì²˜ë¦¬
"""

import os
import sys
import time
import json
import sqlite3
import threading
import logging
import hashlib
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Generator
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import random
import re

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TrainingExample:
    """ëŒ€ê·œëª¨ í•™ìŠµìš© ì˜ˆì œ ë°ì´í„°"""
    id: str
    input_text: str
    target_output: str
    context: str
    topic: str
    difficulty: str  # "beginner", "intermediate", "advanced"
    quality_score: float
    source: str
    language: str
    created_at: str
    tokens: int
    metadata: Dict[str, Any]

@dataclass
class DatasetStatistics:
    """ë°ì´í„°ì…‹ í†µê³„"""
    total_examples: int
    total_tokens: int
    avg_quality_score: float
    topic_distribution: Dict[str, int]
    difficulty_distribution: Dict[str, int]
    language_distribution: Dict[str, int]
    source_distribution: Dict[str, int]
    avg_tokens_per_example: float

class LargeScaleDataGenerator:
    """ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, target_examples: int = 1000000):
        self.target_examples = target_examples
        self.generated_examples = 0
        self.quality_threshold = 0.7
        
        # Unity/C# ì „ë¬¸ ì§€ì‹ ë² ì´ìŠ¤
        self.unity_knowledge_base = {
            "basic_concepts": [
                "GameObject", "Transform", "Component", "MonoBehaviour", 
                "Unity Engine", "Scene", "Prefab", "Inspector"
            ],
            "scripting": [
                "C# Script", "Start", "Update", "Awake", "OnEnable", 
                "Coroutine", "Invoke", "Events", "Delegates"
            ],
            "physics": [
                "Rigidbody", "Collider", "Physics", "Raycast", 
                "Trigger", "Joints", "Forces", "Gravity"
            ],
            "ui": [
                "Canvas", "UI Elements", "Button", "Text", 
                "Image", "Slider", "Toggle", "Layout"
            ],
            "animation": [
                "Animator", "Animation Clip", "State Machine", 
                "Blend Trees", "Timeline", "Playable API"
            ],
            "rendering": [
                "Camera", "Light", "Material", "Shader", 
                "Texture", "Mesh", "Renderer", "Post Processing"
            ]
        }
        
        # í•œêµ­ì–´ ì§ˆë¬¸ íŒ¨í„´
        self.korean_question_patterns = [
            "{topic}ì„/ë¥¼ ì–´ë–»ê²Œ {action}í•˜ë‚˜ìš”?",
            "{topic}ì˜ {property}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "{topic}ì„/ë¥¼ ì‚¬ìš©í•  ë•Œ ì£¼ì˜ì‚¬í•­ì´ ìˆë‚˜ìš”?",
            "{topic}ê³¼/ì™€ {related_topic}ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "{topic}ì„/ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "{topic}ì—ì„œ {issue} ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì€?",
            "{topic}ì˜ ìƒëª…ì£¼ê¸°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "{topic}ì„/ë¥¼ ì½”ë“œë¡œ ì–´ë–»ê²Œ êµ¬í˜„í•˜ë‚˜ìš”?"
        ]
        
        # ë‹µë³€ í…œí”Œë¦¿ (êµ¬ì¡°í™”ëœ í˜•íƒœ)
        self.answer_templates = {
            "explanation": [
                "{topic}ëŠ” {definition}ì…ë‹ˆë‹¤.\n\nì£¼ìš” íŠ¹ì§•:\n1. {feature1}\n2. {feature2}\n3. {feature3}\n\nì‚¬ìš© ì˜ˆì‹œ:\n```csharp\n{code_example}\n```",
                "{topic}ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n{topic}ëŠ” {purpose}ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. {detailed_explanation}\n\nê¸°ë³¸ ì‚¬ìš©ë²•:\n```csharp\n{code_example}\n```\n\nì°¸ê³ ì‚¬í•­: {note}"
            ],
            "tutorial": [
                "{topic}ì„ {action}í•˜ëŠ” ë°©ë²•:\n\në‹¨ê³„ 1: {step1}\në‹¨ê³„ 2: {step2}\në‹¨ê³„ 3: {step3}\n\nì™„ì„±ëœ ì½”ë“œ:\n```csharp\n{complete_code}\n```",
                "{topic} {action} ê°€ì´ë“œ:\n\nğŸ¯ ëª©í‘œ: {goal}\n\nğŸ“‹ ì¤€ë¹„ì‚¬í•­:\n- {requirement1}\n- {requirement2}\n\nğŸ’» êµ¬í˜„:\n```csharp\n{implementation}\n```\n\nâœ… ê²°ê³¼: {expected_result}"
            ],
            "troubleshooting": [
                "{issue} ë¬¸ì œ í•´ê²° ë°©ë²•:\n\nğŸ” ì›ì¸: {cause}\n\nğŸ’¡ í•´ê²°ì±…:\n1. {solution1}\n2. {solution2}\n3. {solution3}\n\nğŸ“ ì˜ˆë°©ë²•: {prevention}",
                "{issue} ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ:\n\nì¼ë°˜ì ì¸ ì›ì¸ë“¤:\n- {common_cause1}\n- {common_cause2}\n\ní•´ê²° ì½”ë“œ:\n```csharp\n{fix_code}\n```\n\nì¶”ê°€ íŒ: {additional_tip}"
            ]
        }

    def generate_synthetic_examples(self, count: int) -> Generator[TrainingExample, None, None]:
        """í•©ì„± í•™ìŠµ ë°ì´í„° ìƒì„±"""
        logger.info(f"í•©ì„± ë°ì´í„° {count:,}ê°œ ìƒì„± ì‹œì‘")
        
        for i in range(count):
            try:
                # ì£¼ì œì™€ ì¹´í…Œê³ ë¦¬ ì„ íƒ
                category = random.choice(list(self.unity_knowledge_base.keys()))
                topic = random.choice(self.unity_knowledge_base[category])
                
                # ì§ˆë¬¸ ìƒì„±
                question_pattern = random.choice(self.korean_question_patterns)
                input_text = self._generate_question(question_pattern, topic, category)
                
                # ë‹µë³€ ìƒì„±
                answer_type = random.choice(list(self.answer_templates.keys()))
                target_output = self._generate_answer(answer_type, topic, category)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                difficulty = random.choices(
                    ["beginner", "intermediate", "advanced"],
                    weights=[0.4, 0.4, 0.2]
                )[0]
                
                quality_score = self._calculate_quality_score(input_text, target_output)
                
                if quality_score >= self.quality_threshold:
                    example = TrainingExample(
                        id=f"synthetic_{i:08d}",
                        input_text=input_text,
                        target_output=target_output,
                        context=f"Unity {category} ê´€ë ¨ ì§ˆë¬¸",
                        topic=f"unity_{category}",
                        difficulty=difficulty,
                        quality_score=quality_score,
                        source="synthetic_generation",
                        language="korean",
                        created_at=datetime.now().isoformat(),
                        tokens=len(input_text.split()) + len(target_output.split()),
                        metadata={
                            "category": category,
                            "topic": topic,
                            "answer_type": answer_type,
                            "generated_by": "LargeScaleDataGenerator"
                        }
                    )
                    
                    self.generated_examples += 1
                    yield example
                
                if (i + 1) % 10000 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {i+1:,}/{count:,} ({(i+1)/count*100:.1f}%)")
                    
            except Exception as e:
                logger.warning(f"ë°ì´í„° ìƒì„± ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {e}")
                continue
        
        logger.info(f"í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {self.generated_examples:,}ê°œ")

    def _generate_question(self, pattern: str, topic: str, category: str) -> str:
        """ì§ˆë¬¸ ìƒì„±"""
        # íŒ¨í„´ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        actions = {
            "basic_concepts": ["ì‚¬ìš©", "ìƒì„±", "ì„¤ì •", "ê´€ë¦¬"],
            "scripting": ["ì‘ì„±", "êµ¬í˜„", "í˜¸ì¶œ", "ì‚¬ìš©"],
            "physics": ["ì ìš©", "ì„¤ì •", "ê³„ì‚°", "ì²˜ë¦¬"],
            "ui": ["ìƒì„±", "ë°°ì¹˜", "ì—°ê²°", "ìŠ¤íƒ€ì¼ë§"],
            "animation": ["ì œì–´", "ìƒì„±", "ì¬ìƒ", "í¸ì§‘"],
            "rendering": ["ì„¤ì •", "ìµœì í™”", "êµ¬ì„±", "ë Œë”ë§"]
        }
        
        properties = {
            "basic_concepts": ["êµ¬ì¡°", "ìƒëª…ì£¼ê¸°", "ê³„ì¸µêµ¬ì¡°", "ì†ì„±"],
            "scripting": ["ë©”ì„œë“œ", "ì´ë²¤íŠ¸", "ë³€ìˆ˜", "ì‹¤í–‰ìˆœì„œ"],
            "physics": ["ë¬¼ë¦¬ë²•ì¹™", "ì¶©ëŒê°ì§€", "í˜", "ì†ë„"],
            "ui": ["ë ˆì´ì•„ì›ƒ", "ì´ë²¤íŠ¸", "ìŠ¤íƒ€ì¼", "ë°˜ì‘ì„±"],
            "animation": ["í‚¤í”„ë ˆì„", "ê³¡ì„ ", "ìƒíƒœ", "ì „í™˜"],
            "rendering": ["í’ˆì§ˆ", "ì„±ëŠ¥", "íš¨ê³¼", "ìµœì í™”"]
        }
        
        # ê´€ë ¨ ì£¼ì œ
        related_topics = self.unity_knowledge_base[category]
        related_topic = random.choice([t for t in related_topics if t != topic])
        
        # ì¼ë°˜ì ì¸ ì´ìŠˆë“¤
        issues = ["ì„±ëŠ¥ì €í•˜", "ì˜¤ë¥˜ë°œìƒ", "ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë™ì‘", "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜", "ë Œë”ë§ ë¬¸ì œ"]
        
        # íŒ¨í„´ ì±„ìš°ê¸°
        filled_pattern = pattern.format(
            topic=topic,
            action=random.choice(actions.get(category, ["ì‚¬ìš©"])),
            property=random.choice(properties.get(category, ["ì†ì„±"])),
            related_topic=related_topic,
            issue=random.choice(issues)
        )
        
        return filled_pattern

    def _generate_answer(self, answer_type: str, topic: str, category: str) -> str:
        """ë‹µë³€ ìƒì„±"""
        template = random.choice(self.answer_templates[answer_type])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ ì •ë³´
        definitions = {
            "GameObject": "Unityì—ì„œ ëª¨ë“  ê°ì²´ì˜ ê¸°ë³¸ì´ ë˜ëŠ” í´ë˜ìŠ¤",
            "Transform": "ê°ì²´ì˜ ìœ„ì¹˜, íšŒì „, í¬ê¸°ë¥¼ ê´€ë¦¬í•˜ëŠ” ì»´í¬ë„ŒíŠ¸",
            "MonoBehaviour": "Unity ìŠ¤í¬ë¦½íŠ¸ì˜ ê¸°ë³¸ì´ ë˜ëŠ” í´ë˜ìŠ¤",
            "Rigidbody": "ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì»´í¬ë„ŒíŠ¸"
        }
        
        code_examples = {
            "GameObject": "GameObject obj = new GameObject(\"MyObject\");\nobj.transform.position = Vector3.zero;",
            "Transform": "transform.position = new Vector3(0, 1, 0);\ntransform.Rotate(0, 90, 0);",
            "MonoBehaviour": "public class MyScript : MonoBehaviour\n{\n    void Start() { Debug.Log(\"Hello!\"); }\n}",
            "Rigidbody": "Rigidbody rb = GetComponent<Rigidbody>();\nrb.AddForce(Vector3.up * 10);"
        }
        
        # í…œí”Œë¦¿ ì±„ìš°ê¸°
        filled_template = template.format(
            topic=topic,
            definition=definitions.get(topic, f"{topic}ëŠ” Unityì˜ í•µì‹¬ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜"),
            purpose=f"{category} ê°œë°œ",
            detailed_explanation=f"{topic}ëŠ” ê²Œì„ ê°œë°œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.",
            feature1=f"{topic}ì˜ ì²« ë²ˆì§¸ íŠ¹ì§•",
            feature2=f"{topic}ì˜ ë‘ ë²ˆì§¸ íŠ¹ì§•", 
            feature3=f"{topic}ì˜ ì„¸ ë²ˆì§¸ íŠ¹ì§•",
            code_example=code_examples.get(topic, f"// {topic} ì‚¬ìš© ì˜ˆì‹œ\n// ì½”ë“œ êµ¬í˜„"),
            step1=f"{topic} ì¤€ë¹„í•˜ê¸°",
            step2=f"{topic} ì„¤ì •í•˜ê¸°",
            step3=f"{topic} í…ŒìŠ¤íŠ¸í•˜ê¸°",
            complete_code=code_examples.get(topic, f"// ì™„ì„±ëœ {topic} ì½”ë“œ"),
            goal=f"{topic} ë§ˆìŠ¤í„°í•˜ê¸°",
            requirement1=f"{topic} ê¸°ë³¸ ì§€ì‹",
            requirement2="Unity ì—ë””í„° ì‚¬ìš©ë²•",
            implementation=code_examples.get(topic, f"// {topic} êµ¬í˜„"),
            expected_result=f"{topic}ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•¨",
            issue=f"{topic} ë¬¸ì œ",
            cause=f"{topic} ì„¤ì • ì˜¤ë¥˜",
            solution1="ì„¤ì • í™•ì¸",
            solution2="ì½”ë“œ ê²€í† ",
            solution3="Unity ì¬ì‹œì‘",
            prevention="ì •ê¸°ì ì¸ í…ŒìŠ¤íŠ¸",
            common_cause1="ì˜ëª»ëœ ì„¤ì •",
            common_cause2="ë²„ì „ ì¶©ëŒ",
            fix_code=code_examples.get(topic, f"// {topic} ìˆ˜ì • ì½”ë“œ"),
            additional_tip=f"{topic} ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­",
            action="êµ¬í˜„",
            note=f"{topic} ì‚¬ìš© ì‹œ ì„±ëŠ¥ì„ ê³ ë ¤í•˜ì„¸ìš”"
        )
        
        return filled_template

    def _calculate_quality_score(self, input_text: str, output_text: str) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        if len(input_text) > 10:
            score += 0.1
        if len(output_text) > 50:
            score += 0.1
        
        # ê¸°ìˆ ì  í‚¤ì›Œë“œ í¬í•¨
        tech_keywords = ["Unity", "GameObject", "Transform", "C#", "Script", "Component"]
        if any(keyword in input_text for keyword in tech_keywords):
            score += 0.1
        if any(keyword in output_text for keyword in tech_keywords):
            score += 0.1
        
        # ì½”ë“œ ë¸”ë¡ í¬í•¨
        if "```" in output_text:
            score += 0.15
        
        # êµ¬ì¡°í™”ëœ ë‹µë³€ (ë²ˆí˜¸, ë‹¨ê³„)
        if any(marker in output_text for marker in ["1.", "ë‹¨ê³„", "ë°©ë²•:"]):
            score += 0.05
        
        return min(1.0, score)

class LargeScaleDatasetDatabase:
    """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self, db_path: str = "large_scale_training_dataset.db"):
        self.db_path = db_path
        self.batch_size = 10000
        self.init_database()
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ë©”ì¸ í•™ìŠµ ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS training_examples (
                    id TEXT PRIMARY KEY,
                    input_text TEXT NOT NULL,
                    target_output TEXT NOT NULL,
                    context TEXT,
                    topic TEXT,
                    difficulty TEXT,
                    quality_score REAL,
                    source TEXT,
                    language TEXT,
                    created_at TEXT,
                    tokens INTEGER,
                    metadata TEXT,
                    processed BOOLEAN DEFAULT FALSE,
                    training_set TEXT DEFAULT 'train'
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            indices = [
                "CREATE INDEX IF NOT EXISTS idx_topic ON training_examples(topic)",
                "CREATE INDEX IF NOT EXISTS idx_difficulty ON training_examples(difficulty)", 
                "CREATE INDEX IF NOT EXISTS idx_quality ON training_examples(quality_score)",
                "CREATE INDEX IF NOT EXISTS idx_source ON training_examples(source)",
                "CREATE INDEX IF NOT EXISTS idx_training_set ON training_examples(training_set)",
                "CREATE INDEX IF NOT EXISTS idx_processed ON training_examples(processed)"
            ]
            
            for index_sql in indices:
                cursor.execute(index_sql)
            
            # í†µê³„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS dataset_statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    total_examples INTEGER,
                    total_tokens INTEGER,
                    avg_quality_score REAL,
                    topic_distribution TEXT,
                    difficulty_distribution TEXT,
                    language_distribution TEXT,
                    source_distribution TEXT,
                    updated_at TEXT
                )
            ''')
            
            conn.commit()
            logger.info("âœ… ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def insert_examples_batch(self, examples: List[TrainingExample]) -> int:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì œ ì‚½ì…"""
        inserted_count = 0
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            try:
                batch_data = []
                for example in examples:
                    batch_data.append((
                        example.id,
                        example.input_text,
                        example.target_output,
                        example.context,
                        example.topic,
                        example.difficulty,
                        example.quality_score,
                        example.source,
                        example.language,
                        example.created_at,
                        example.tokens,
                        json.dumps(example.metadata, ensure_ascii=False),
                        False,  # processed
                        'train'  # training_set
                    ))
                
                cursor.executemany('''
                    INSERT OR REPLACE INTO training_examples 
                    (id, input_text, target_output, context, topic, difficulty, 
                     quality_score, source, language, created_at, tokens, metadata, processed, training_set)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', batch_data)
                
                inserted_count = cursor.rowcount
                conn.commit()
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì‚½ì… ì˜¤ë¥˜: {e}")
                conn.rollback()
        
        return inserted_count

    def get_dataset_statistics(self) -> DatasetStatistics:
        """ë°ì´í„°ì…‹ í†µê³„ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ê¸°ë³¸ í†µê³„
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_examples,
                    SUM(tokens) as total_tokens,
                    AVG(quality_score) as avg_quality
                FROM training_examples
            ''')
            
            basic_stats = cursor.fetchone()
            
            # ì£¼ì œë³„ ë¶„í¬
            cursor.execute('''
                SELECT topic, COUNT(*) 
                FROM training_examples 
                GROUP BY topic
            ''')
            topic_dist = dict(cursor.fetchall())
            
            # ë‚œì´ë„ë³„ ë¶„í¬
            cursor.execute('''
                SELECT difficulty, COUNT(*) 
                FROM training_examples 
                GROUP BY difficulty
            ''')
            difficulty_dist = dict(cursor.fetchall())
            
            # ì–¸ì–´ë³„ ë¶„í¬
            cursor.execute('''
                SELECT language, COUNT(*) 
                FROM training_examples 
                GROUP BY language
            ''')
            language_dist = dict(cursor.fetchall())
            
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            cursor.execute('''
                SELECT source, COUNT(*) 
                FROM training_examples 
                GROUP BY source
            ''')
            source_dist = dict(cursor.fetchall())
            
            return DatasetStatistics(
                total_examples=basic_stats[0] or 0,
                total_tokens=basic_stats[1] or 0,
                avg_quality_score=basic_stats[2] or 0.0,
                topic_distribution=topic_dist,
                difficulty_distribution=difficulty_dist,
                language_distribution=language_dist,
                source_distribution=source_dist,
                avg_tokens_per_example=(basic_stats[1] or 0) / max(basic_stats[0] or 1, 1)
            )

    def split_dataset(self, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1):
        """ë°ì´í„°ì…‹ ë¶„í•  (train/validation/test)"""
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "ë¹„ìœ¨ì˜ í•©ì´ 1ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤"
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ
            cursor.execute("SELECT COUNT(*) FROM training_examples")
            total_count = cursor.fetchone()[0]
            
            if total_count == 0:
                logger.warning("ë¶„í• í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ëœë¤ ìˆœì„œë¡œ ë°ì´í„° ì¡°íšŒ
            cursor.execute("SELECT id FROM training_examples ORDER BY RANDOM()")
            all_ids = [row[0] for row in cursor.fetchall()]
            
            # ë¶„í•  ê³„ì‚°
            train_count = int(total_count * train_ratio)
            val_count = int(total_count * val_ratio)
            
            train_ids = all_ids[:train_count]
            val_ids = all_ids[train_count:train_count + val_count]
            test_ids = all_ids[train_count + val_count:]
            
            # ë¶„í•  ì ìš©
            splits = [
                ('train', train_ids),
                ('validation', val_ids),
                ('test', test_ids)
            ]
            
            for split_name, ids in splits:
                if ids:
                    cursor.executemany(
                        "UPDATE training_examples SET training_set = ? WHERE id = ?",
                        [(split_name, id_) for id_ in ids]
                    )
            
            conn.commit()
            
            logger.info(f"ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
            logger.info(f"  Train: {len(train_ids):,} ({len(train_ids)/total_count*100:.1f}%)")
            logger.info(f"  Validation: {len(val_ids):,} ({len(val_ids)/total_count*100:.1f}%)")
            logger.info(f"  Test: {len(test_ids):,} ({len(test_ids)/total_count*100:.1f}%)")

    def get_training_batch(self, batch_size: int = 32, split: str = 'train') -> List[TrainingExample]:
        """í•™ìŠµìš© ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM training_examples 
                WHERE training_set = ? AND processed = FALSE
                ORDER BY RANDOM()
                LIMIT ?
            ''', (split, batch_size))
            
            batch_examples = []
            for row in cursor.fetchall():
                example = TrainingExample(
                    id=row[0],
                    input_text=row[1],
                    target_output=row[2],
                    context=row[3],
                    topic=row[4],
                    difficulty=row[5],
                    quality_score=row[6],
                    source=row[7],
                    language=row[8],
                    created_at=row[9],
                    tokens=row[10],
                    metadata=json.loads(row[11]) if row[11] else {}
                )
                batch_examples.append(example)
            
            return batch_examples

class LargeScaleDataPipeline:
    """ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì"""
    
    def __init__(self, target_examples: int = 1000000):
        self.target_examples = target_examples
        self.data_generator = LargeScaleDataGenerator(target_examples)
        self.database = LargeScaleDatasetDatabase()
        self.worker_threads = 4
        self.batch_size = 10000
        
    def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ğŸš€ ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ëª©í‘œ: {self.target_examples:,}ê°œ)")
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„° ìƒì„±
            self._generate_training_data()
            
            # 2. ë°ì´í„°ì…‹ ë¶„í• 
            self._split_dataset()
            
            # 3. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics()
            
            # 4. í’ˆì§ˆ ê²€ì¦
            self._validate_dataset()
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time/60:.1f}ë¶„)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False
    
    def _generate_training_data(self):
        """í•™ìŠµ ë°ì´í„° ìƒì„±"""
        logger.info("ğŸ“Š í•©ì„± í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
        
        examples_per_batch = self.batch_size
        total_batches = (self.target_examples + examples_per_batch - 1) // examples_per_batch
        
        current_batch = []
        batch_count = 0
        
        for example in self.data_generator.generate_synthetic_examples(self.target_examples):
            current_batch.append(example)
            
            if len(current_batch) >= examples_per_batch:
                # ë°°ì¹˜ ì €ì¥
                inserted = self.database.insert_examples_batch(current_batch)
                batch_count += 1
                
                logger.info(f"ë°°ì¹˜ {batch_count}/{total_batches} ì™„ë£Œ ({inserted:,}ê°œ ì‚½ì…)")
                current_batch = []
        
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if current_batch:
            inserted = self.database.insert_examples_batch(current_batch)
            logger.info(f"ìµœì¢… ë°°ì¹˜ ì™„ë£Œ ({inserted:,}ê°œ ì‚½ì…)")
    
    def _split_dataset(self):
        """ë°ì´í„°ì…‹ ë¶„í• """
        logger.info("ğŸ“‚ ë°ì´í„°ì…‹ train/validation/test ë¶„í• ...")
        self.database.split_dataset(train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)
    
    def _update_statistics(self):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„ ì—…ë°ì´íŠ¸...")
        stats = self.database.get_dataset_statistics()
        
        logger.info(f"ğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
        logger.info(f"  ì´ ì˜ˆì œ ìˆ˜: {stats.total_examples:,}")
        logger.info(f"  ì´ í† í° ìˆ˜: {stats.total_tokens:,}")
        logger.info(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {stats.avg_quality_score:.3f}")
        logger.info(f"  í‰ê·  í† í°/ì˜ˆì œ: {stats.avg_tokens_per_example:.1f}")
        
        logger.info(f"  ì£¼ì œ ë¶„í¬: {list(stats.topic_distribution.keys())[:5]}...")
        logger.info(f"  ë‚œì´ë„ ë¶„í¬: {stats.difficulty_distribution}")
    
    def _validate_dataset(self):
        """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""
        logger.info("ğŸ” ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦...")
        
        stats = self.database.get_dataset_statistics()
        
        # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
        quality_checks = []
        
        if stats.total_examples >= self.target_examples * 0.8:
            quality_checks.append("âœ… ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ëŸ‰")
        else:
            quality_checks.append("âŒ ë¶€ì¡±í•œ ë°ì´í„° ìˆ˜ëŸ‰")
        
        if stats.avg_quality_score >= 0.7:
            quality_checks.append("âœ… ë†’ì€ í‰ê·  í’ˆì§ˆ")
        else:
            quality_checks.append("âŒ ë‚®ì€ í‰ê·  í’ˆì§ˆ")
        
        if len(stats.topic_distribution) >= 5:
            quality_checks.append("âœ… ë‹¤ì–‘í•œ ì£¼ì œ ë¶„í¬")
        else:
            quality_checks.append("âŒ ì œí•œì ì¸ ì£¼ì œ ë¶„í¬")
        
        if stats.avg_tokens_per_example >= 50:
            quality_checks.append("âœ… ì ì ˆí•œ ì˜ˆì œ ê¸¸ì´")
        else:
            quality_checks.append("âŒ ì§§ì€ ì˜ˆì œ ê¸¸ì´")
        
        for check in quality_checks:
            logger.info(f"  {check}")
        
        passed_checks = sum(1 for check in quality_checks if check.startswith("âœ…"))
        total_checks = len(quality_checks)
        
        logger.info(f"í’ˆì§ˆ ê²€ì¦ ê²°ê³¼: {passed_checks}/{total_checks} í†µê³¼ ({passed_checks/total_checks*100:.1f}%)")
        
        return passed_checks >= total_checks * 0.75

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„° íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì„¤ì •
        target_examples = 100000  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ë§Œê°œ
        pipeline = LargeScaleDataPipeline(target_examples)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = pipeline.run_pipeline()
        
        if success:
            print("ğŸ‰ ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            return 0
        else:
            print("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return 1
            
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())