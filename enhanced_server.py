import os
import time
from fastapi import FastAPI
from contextlib import asynccontextmanager
import threading

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

model = None
tokenizer = None
last_model_update = 0

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer
    
    if not TRANSFORMERS_AVAILABLE:
        print("transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    try:
        model_path = "CodeLlama-7b-Instruct-hf"
        if os.path.exists(model_path):
            print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        else:
            print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

def check_model_update():
    """ëª¨ë¸ ì—…ë°ì´íŠ¸ í™•ì¸"""
    global model, tokenizer, last_model_update
    
    while True:
        if os.path.exists("model_updated.signal"):
            signal_time = os.path.getmtime("model_updated.signal")
            if signal_time > last_model_update:
                print("ğŸ”„ ìƒˆë¡œìš´ ëª¨ë¸ ê°ì§€! ë¦¬ë¡œë”©...")
                load_model()
                last_model_update = signal_time
                os.remove("model_updated.signal")
        
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘
    load_model()
    
    # ëª¨ë¸ ì—…ë°ì´íŠ¸ ì²´í¬ ìŠ¤ë ˆë“œ ì‹œì‘
    update_thread = threading.Thread(target=check_model_update, daemon=True)
    update_thread.start()
    
    yield
    # ì¢…ë£Œ

app = FastAPI(lifespan=lifespan)

@app.post("/generate")
async def generate_code(request: dict):
    """ì½”ë“œ ìƒì„± API"""
    global model, tokenizer
    
    if not TRANSFORMERS_AVAILABLE:
        return {"error": "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    if model is None or tokenizer is None:
        return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    try:
        prompt = request.get("prompt", "")
        
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        
        # ì½”ë“œ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 150,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì œê±°
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
        
        return {"generated_text": generated_text}
        
    except Exception as e:
        return {"error": f"ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

@app.get("/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "model_loaded": model is not None,
        "transformers_available": TRANSFORMERS_AVAILABLE
    }