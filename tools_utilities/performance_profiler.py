#!/usr/bin/env python3
"""
AutoCI Performance Profiler - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ
"""

import os
import sys
import time
import asyncio
import psutil
import json
import statistics
import cProfile
import pstats
import memory_profiler
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import pandas as pd

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    name: str
    duration: float
    memory_usage: float
    cpu_usage: float
    iterations: int
    timestamp: datetime
    metadata: Dict[str, Any] = None

@dataclass 
class PerformanceReport:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
    test_name: str
    results: List[BenchmarkResult]
    summary: Dict[str, float]
    recommendations: List[str]
    generated_at: datetime

class PerformanceProfiler:
    """ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""
    
    def __init__(self, output_dir: Path = None):
        self.output_dir = output_dir or Path("performance_reports")
        self.output_dir.mkdir(exist_ok=True)
        
        self.benchmark_results: List[BenchmarkResult] = []
        
        # ì„±ëŠ¥ ìž„ê³„ê°’
        self.thresholds = {
            "max_duration": 5.0,  # ì´ˆ
            "max_memory": 500.0,  # MB
            "max_cpu": 80.0,      # %
            "min_throughput": 100.0  # ops/sec
        }
    
    def benchmark(self, name: str, iterations: int = 10):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            async def async_wrapper(*args, **kwargs):
                return await self._run_async_benchmark(name, func, iterations, args, kwargs)
            
            def sync_wrapper(*args, **kwargs):
                return self._run_sync_benchmark(name, func, iterations, args, kwargs)
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator
    
    async def _run_async_benchmark(self, name: str, func: Callable, iterations: int, args, kwargs):
        """ë¹„ë™ê¸° í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for i in range(iterations):
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¸¡ì • ì‹œìž‘
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_cpu = process.cpu_percent()
            start_time = time.perf_counter()
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = await func(*args, **kwargs)
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¸¡ì • ì¢…ë£Œ
            end_time = time.perf_counter()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            end_cpu = process.cpu_percent()
            
            # ê²°ê³¼ ê¸°ë¡
            benchmark_result = BenchmarkResult(
                name=f"{name}_iteration_{i+1}",
                duration=end_time - start_time,
                memory_usage=end_memory - start_memory,
                cpu_usage=max(end_cpu - start_cpu, 0),
                iterations=1,
                timestamp=datetime.now(),
                metadata={"iteration": i+1, "total_iterations": iterations}
            )
            
            results.append(benchmark_result)
            self.benchmark_results.append(benchmark_result)
            
            # ì•½ê°„ì˜ ì¿¨ë‹¤ìš´
            await asyncio.sleep(0.1)
        
        # ìš”ì•½ í†µê³„ ìƒì„±
        self._generate_summary_stats(name, results)
        
        return result
    
    def _run_sync_benchmark(self, name: str, func: Callable, iterations: int, args, kwargs):
        """ë™ê¸° í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for i in range(iterations):
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¸¡ì • ì‹œìž‘
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_cpu = process.cpu_percent()
            start_time = time.perf_counter()
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¸¡ì • ì¢…ë£Œ
            end_time = time.perf_counter()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            end_cpu = process.cpu_percent()
            
            # ê²°ê³¼ ê¸°ë¡
            benchmark_result = BenchmarkResult(
                name=f"{name}_iteration_{i+1}",
                duration=end_time - start_time,
                memory_usage=end_memory - start_memory,
                cpu_usage=max(end_cpu - start_cpu, 0),
                iterations=1,
                timestamp=datetime.now(),
                metadata={"iteration": i+1, "total_iterations": iterations}
            )
            
            results.append(benchmark_result)
            self.benchmark_results.append(benchmark_result)
            
            # ì•½ê°„ì˜ ì¿¨ë‹¤ìš´
            time.sleep(0.1)
        
        # ìš”ì•½ í†µê³„ ìƒì„±
        self._generate_summary_stats(name, results)
        
        return result
    
    def _generate_summary_stats(self, name: str, results: List[BenchmarkResult]):
        """ìš”ì•½ í†µê³„ ìƒì„±"""
        durations = [r.duration for r in results]
        memory_usages = [r.memory_usage for r in results]
        cpu_usages = [r.cpu_usage for r in results]
        
        summary = {
            "name": name,
            "iterations": len(results),
            "duration": {
                "mean": statistics.mean(durations),
                "median": statistics.median(durations),
                "stdev": statistics.stdev(durations) if len(durations) > 1 else 0,
                "min": min(durations),
                "max": max(durations)
            },
            "memory": {
                "mean": statistics.mean(memory_usages),
                "median": statistics.median(memory_usages),
                "max": max(memory_usages)
            },
            "cpu": {
                "mean": statistics.mean(cpu_usages),
                "median": statistics.median(cpu_usages),
                "max": max(cpu_usages)
            },
            "throughput": len(results) / sum(durations) if sum(durations) > 0 else 0
        }
        
        print(f"\nðŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print(f"  í‰ê·  ì‹¤í–‰ì‹œê°„: {summary['duration']['mean']:.4f}ì´ˆ")
        print(f"  ì²˜ë¦¬ëŸ‰: {summary['throughput']:.2f} ops/sec")
        print(f"  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©: {summary['memory']['max']:.2f}MB")
        print(f"  í‰ê·  CPU ì‚¬ìš©: {summary['cpu']['mean']:.2f}%")
        
        # ì„±ëŠ¥ ê²½ê³  í™•ì¸
        self._check_performance_warnings(name, summary)
    
    def _check_performance_warnings(self, name: str, summary: Dict[str, Any]):
        """ì„±ëŠ¥ ê²½ê³  í™•ì¸"""
        warnings = []
        
        if summary['duration']['mean'] > self.thresholds['max_duration']:
            warnings.append(f"âš ï¸  í‰ê·  ì‹¤í–‰ì‹œê°„ì´ ìž„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {summary['duration']['mean']:.4f}ì´ˆ > {self.thresholds['max_duration']}ì´ˆ")
        
        if summary['memory']['max'] > self.thresholds['max_memory']:
            warnings.append(f"âš ï¸  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìž„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {summary['memory']['max']:.2f}MB > {self.thresholds['max_memory']}MB")
        
        if summary['cpu']['max'] > self.thresholds['max_cpu']:
            warnings.append(f"âš ï¸  ìµœëŒ€ CPU ì‚¬ìš©ë¥ ì´ ìž„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {summary['cpu']['max']:.2f}% > {self.thresholds['max_cpu']}%")
        
        if summary['throughput'] < self.thresholds['min_throughput']:
            warnings.append(f"âš ï¸  ì²˜ë¦¬ëŸ‰ì´ ìž„ê³„ê°’ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤: {summary['throughput']:.2f} ops/sec < {self.thresholds['min_throughput']} ops/sec")
        
        for warning in warnings:
            print(warning)
    
    def profile_memory(self, func, *args, **kwargs):
        """ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§"""
        print(f"\nðŸ§  ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§: {func.__name__}")
        
        @memory_profiler.profile
        def profiled_func():
            return func(*args, **kwargs)
        
        return profiled_func()
    
    def profile_cpu(self, func, *args, **kwargs):
        """CPU í”„ë¡œíŒŒì¼ë§"""
        print(f"\nâš¡ CPU í”„ë¡œíŒŒì¼ë§: {func.__name__}")
        
        profiler = cProfile.Profile()
        profiler.enable()
        
        result = func(*args, **kwargs)
        
        profiler.disable()
        
        # í”„ë¡œíŒŒì¼ ê²°ê³¼ ì €ìž¥
        profile_file = self.output_dir / f"cpu_profile_{func.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.prof"
        profiler.dump_stats(str(profile_file))
        
        # ìƒìœ„ 10ê°œ í•¨ìˆ˜ ì¶œë ¥
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(10)
        
        return result
    
    def run_stress_test(self, func, duration_seconds: int = 60, concurrent_tasks: int = 10):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        print(f"\nðŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: {func.__name__} ({duration_seconds}ì´ˆ, {concurrent_tasks}ê°œ ë™ì‹œ ìž‘ì—…)")
        
        async def stress_task():
            start_time = time.time()
            task_count = 0
            
            while time.time() - start_time < duration_seconds:
                if asyncio.iscoroutinefunction(func):
                    await func()
                else:
                    func()
                task_count += 1
                await asyncio.sleep(0.01)  # ì•½ê°„ì˜ ì¿¨ë‹¤ìš´
            
            return task_count
        
        async def run_stress():
            # ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì‹œìž‘
            process = psutil.Process()
            start_time = time.time()
            start_memory = process.memory_info().rss / 1024 / 1024
            
            # ë™ì‹œ ìž‘ì—… ì‹¤í–‰
            tasks = [stress_task() for _ in range(concurrent_tasks)]
            results = await asyncio.gather(*tasks)
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¸¡ì • ì¢…ë£Œ
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024
            
            # ê²°ê³¼ ë¶„ì„
            total_operations = sum(results)
            elapsed_time = end_time - start_time
            ops_per_second = total_operations / elapsed_time
            memory_increase = end_memory - start_memory
            
            print(f"  ì´ ìž‘ì—… ìˆ˜: {total_operations}")
            print(f"  ì²˜ë¦¬ëŸ‰: {ops_per_second:.2f} ops/sec")
            print(f"  ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.2f}MB")
            
            # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ìž¥
            stress_result = BenchmarkResult(
                name=f"stress_test_{func.__name__}",
                duration=elapsed_time,
                memory_usage=memory_increase,
                cpu_usage=psutil.cpu_percent(),
                iterations=total_operations,
                timestamp=datetime.now(),
                metadata={
                    "concurrent_tasks": concurrent_tasks,
                    "ops_per_second": ops_per_second,
                    "duration_seconds": duration_seconds
                }
            )
            
            self.benchmark_results.append(stress_result)
            
            return stress_result
        
        if asyncio.get_event_loop().is_running():
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
            return asyncio.create_task(run_stress())
        else:
            return asyncio.run(run_stress())
    
    def generate_performance_report(self, report_name: str = None) -> PerformanceReport:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not report_name:
            report_name = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"\nðŸ“‹ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±: {report_name}")
        
        # ì „ì²´ ìš”ì•½ í†µê³„
        all_durations = [r.duration for r in self.benchmark_results]
        all_memory = [r.memory_usage for r in self.benchmark_results]
        all_cpu = [r.cpu_usage for r in self.benchmark_results]
        
        summary = {
            "total_benchmarks": len(self.benchmark_results),
            "avg_duration": statistics.mean(all_durations) if all_durations else 0,
            "max_duration": max(all_durations) if all_durations else 0,
            "avg_memory": statistics.mean(all_memory) if all_memory else 0,
            "max_memory": max(all_memory) if all_memory else 0,
            "avg_cpu": statistics.mean(all_cpu) if all_cpu else 0,
            "max_cpu": max(all_cpu) if all_cpu else 0
        }
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(summary)
        
        # ë¦¬í¬íŠ¸ ê°ì²´ ìƒì„±
        report = PerformanceReport(
            test_name=report_name,
            results=self.benchmark_results.copy(),
            summary=summary,
            recommendations=recommendations,
            generated_at=datetime.now()
        )
        
        # ë¦¬í¬íŠ¸ ì €ìž¥
        self._save_report(report)
        
        # ì‹œê°í™” ìƒì„±
        self._generate_visualizations(report)
        
        return report
    
    def _generate_recommendations(self, summary: Dict[str, float]) -> List[str]:
        """ì„±ëŠ¥ ìµœì í™” ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if summary['max_duration'] > self.thresholds['max_duration']:
            recommendations.append("ì‹¤í–‰ ì‹œê°„ì´ ê¸´ í•¨ìˆ˜ë“¤ì— ëŒ€í•´ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            recommendations.append("ë¹„ë™ê¸° ì²˜ë¦¬ë‚˜ ë³‘ë ¬ ì²˜ë¦¬ ë„ìž…ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if summary['max_memory'] > self.thresholds['max_memory']:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ í’€ë§ì´ë‚˜ ê°ì²´ ìž¬ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
            recommendations.append("ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì„ ë„ìž…í•˜ì„¸ìš”.")
        
        if summary['max_cpu'] > self.thresholds['max_cpu']:
            recommendations.append("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ë” ìž‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì„¸ìš”.")
            recommendations.append("I/O ë°”ìš´ë“œ ìž‘ì—…ê³¼ CPU ë°”ìš´ë“œ ìž‘ì—…ì„ ë¶„ë¦¬í•˜ì„¸ìš”.")
        
        if summary['avg_duration'] > 1.0:
            recommendations.append("í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 1ì´ˆë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ìºì‹± ì „ëžµì„ ë„ìž…í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. í˜„ìž¬ ìµœì í™” ìƒíƒœë¥¼ ìœ ì§€í•˜ì„¸ìš”.")
        
        return recommendations
    
    def _save_report(self, report: PerformanceReport):
        """ë¦¬í¬íŠ¸ ì €ìž¥"""
        # JSON í˜•ì‹ìœ¼ë¡œ ì €ìž¥
        json_file = self.output_dir / f"{report.test_name}.json"
        report_dict = asdict(report)
        
        # datetime ê°ì²´ë¥¼ ë¬¸ìžì—´ë¡œ ë³€í™˜
        def serialize_datetime(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=2, default=serialize_datetime, ensure_ascii=False)
        
        # Markdown í˜•ì‹ìœ¼ë¡œ ì €ìž¥
        md_file = self.output_dir / f"{report.test_name}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# Performance Report: {report.test_name}\n\n")
            f.write(f"**Generated:** {report.generated_at.isoformat()}\n\n")
            
            f.write("## Summary\n\n")
            f.write(f"- Total Benchmarks: {report.summary['total_benchmarks']}\n")
            f.write(f"- Average Duration: {report.summary['avg_duration']:.4f}s\n")
            f.write(f"- Max Duration: {report.summary['max_duration']:.4f}s\n")
            f.write(f"- Average Memory: {report.summary['avg_memory']:.2f}MB\n")
            f.write(f"- Max Memory: {report.summary['max_memory']:.2f}MB\n")
            f.write(f"- Average CPU: {report.summary['avg_cpu']:.2f}%\n")
            f.write(f"- Max CPU: {report.summary['max_cpu']:.2f}%\n\n")
            
            f.write("## Recommendations\n\n")
            for i, rec in enumerate(report.recommendations, 1):
                f.write(f"{i}. {rec}\n")
            f.write("\n")
            
            f.write("## Detailed Results\n\n")
            for result in report.results:
                f.write(f"### {result.name}\n")
                f.write(f"- Duration: {result.duration:.4f}s\n")
                f.write(f"- Memory: {result.memory_usage:.2f}MB\n")
                f.write(f"- CPU: {result.cpu_usage:.2f}%\n")
                f.write(f"- Timestamp: {result.timestamp.isoformat()}\n\n")
        
        print(f"ðŸ“Š ë¦¬í¬íŠ¸ ì €ìž¥ ì™„ë£Œ:")
        print(f"  - JSON: {json_file}")
        print(f"  - Markdown: {md_file}")
    
    def _generate_visualizations(self, report: PerformanceReport):
        """ì‹œê°í™” ìƒì„±"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            df = pd.DataFrame([asdict(result) for result in report.results])
            
            # ì‹œê°„ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ
            plt.figure(figsize=(15, 10))
            
            # ì‹¤í–‰ ì‹œê°„ ì°¨íŠ¸
            plt.subplot(2, 2, 1)
            plt.plot(df['timestamp'], df['duration'])
            plt.title('Execution Time Over Time')
            plt.xlabel('Timestamp')
            plt.ylabel('Duration (seconds)')
            plt.xticks(rotation=45)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨íŠ¸
            plt.subplot(2, 2, 2)
            plt.plot(df['timestamp'], df['memory_usage'])
            plt.title('Memory Usage Over Time')
            plt.xlabel('Timestamp')
            plt.ylabel('Memory (MB)')
            plt.xticks(rotation=45)
            
            # CPU ì‚¬ìš©ë¥  ì°¨íŠ¸
            plt.subplot(2, 2, 3)
            plt.plot(df['timestamp'], df['cpu_usage'])
            plt.title('CPU Usage Over Time')
            plt.xlabel('Timestamp')
            plt.ylabel('CPU (%)')
            plt.xticks(rotation=45)
            
            # ì„±ëŠ¥ ë¶„í¬ ížˆìŠ¤í† ê·¸ëž¨
            plt.subplot(2, 2, 4)
            plt.hist(df['duration'], bins=20, alpha=0.7)
            plt.title('Duration Distribution')
            plt.xlabel('Duration (seconds)')
            plt.ylabel('Frequency')
            
            plt.tight_layout()
            
            # ì°¨íŠ¸ ì €ìž¥
            chart_file = self.output_dir / f"{report.test_name}_charts.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ðŸ“ˆ ì‹œê°í™” ì €ìž¥: {chart_file}")
            
        except ImportError:
            print("âš ï¸  matplotlib ë˜ëŠ” pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸  ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def compare_reports(self, report1: PerformanceReport, report2: PerformanceReport):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë¹„êµ"""
        print(f"\nðŸ” ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë¹„êµ: {report1.test_name} vs {report2.test_name}")
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ
        metrics = ['avg_duration', 'max_duration', 'avg_memory', 'max_memory', 'avg_cpu', 'max_cpu']
        
        print("ë©”íŠ¸ë¦­ ë¹„êµ:")
        for metric in metrics:
            val1 = report1.summary[metric]
            val2 = report2.summary[metric]
            change = ((val2 - val1) / val1 * 100) if val1 > 0 else 0
            
            direction = "ðŸ“ˆ" if change > 0 else "ðŸ“‰" if change < 0 else "âž¡ï¸"
            print(f"  {metric}: {val1:.4f} â†’ {val2:.4f} ({direction} {change:+.2f}%)")
    
    def clear_results(self):
        """ê²°ê³¼ ì´ˆê¸°í™”"""
        self.benchmark_results.clear()
        print("ðŸ“ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
async def example_async_function():
    """ì˜ˆì œ ë¹„ë™ê¸° í•¨ìˆ˜"""
    await asyncio.sleep(0.1)
    return "async result"

def example_sync_function():
    """ì˜ˆì œ ë™ê¸° í•¨ìˆ˜"""
    time.sleep(0.05)
    return "sync result"

def memory_intensive_function():
    """ë©”ëª¨ë¦¬ ì§‘ì•½ì  í•¨ìˆ˜"""
    data = [list(range(1000)) for _ in range(1000)]
    return len(data)

def cpu_intensive_function():
    """CPU ì§‘ì•½ì  í•¨ìˆ˜"""
    total = 0
    for i in range(1000000):
        total += i * i
    return total

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ðŸš€ AutoCI Performance Profiler")
    print("=" * 60)
    
    profiler = PerformanceProfiler()
    
    # ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    print("\nðŸ“Š ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    
    # ë™ê¸° í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬
    profiler.benchmark("sync_function", 5)(example_sync_function)()
    
    # ë¹„ë™ê¸° í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬
    async def async_test():
        await profiler.benchmark("async_function", 5)(example_async_function)()
    
    asyncio.run(async_test())
    
    # ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
    print("\nðŸ§  ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§...")
    profiler.profile_memory(memory_intensive_function)
    
    # CPU í”„ë¡œíŒŒì¼ë§  
    print("\nâš¡ CPU í”„ë¡œíŒŒì¼ë§...")
    profiler.profile_cpu(cpu_intensive_function)
    
    # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
    print("\nðŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸...")
    asyncio.run(profiler.run_stress_test(example_async_function, duration_seconds=5, concurrent_tasks=3))
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    report = profiler.generate_performance_report("autoci_performance_test")
    
    print(f"\nâœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
    print(f"ðŸ“ ê²°ê³¼ ì €ìž¥ ìœ„ì¹˜: {profiler.output_dir}")

if __name__ == "__main__":
    main()