#!/usr/bin/env python3
"""
ë²¡í„° ì„ë² ë”© ê¸°ë°˜ ê³ ê¸‰ ì¸ë±ì‹± ì‹œìŠ¤í…œ
ìˆ˜ì§‘ëœ C# ë°ì´í„°ë¥¼ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
"""

import os
import sys
import json
import sqlite3
import logging
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import hashlib
import pickle
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import faiss
from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm
import re
from collections import defaultdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('vector_indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class CodeChunk:
    """ì½”ë“œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    file_path: str
    content: str
    chunk_type: str  # 'code', 'documentation', 'qa', 'pattern'
    category: str
    metadata: Dict
    embedding: Optional[np.ndarray] = None
    quality_score: float = 0.7


class VectorIndexer:
    """ë²¡í„° ì„ë² ë”© ê¸°ë°˜ ì¸ë±ì„œ"""
    
    def __init__(self, data_dir: str = "expert_learning_data"):
        self.data_dir = Path(data_dir)
        self.index_dir = self.data_dir / "vector_index"
        self.index_dir.mkdir(exist_ok=True)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model_name = 'microsoft/codebert-base'
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # FAISS ì¸ë±ìŠ¤
        self.dimension = 768  # CodeBERT dimension
        self.index = None
        self.chunk_map = {}  # chunk_id -> CodeChunk
        
        # ë°ì´í„°ë² ì´ìŠ¤
        self.db_path = self.index_dir / "vector_index.db"
        self.init_database()
        
        # í†µê³„
        self.stats = {
            'total_chunks': 0,
            'total_files': 0,
            'categories': defaultdict(int),
            'avg_quality': 0.0
        }
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS vector_chunks (
            chunk_id TEXT PRIMARY KEY,
            file_path TEXT NOT NULL,
            content TEXT NOT NULL,
            chunk_type TEXT NOT NULL,
            category TEXT NOT NULL,
            start_line INTEGER,
            end_line INTEGER,
            quality_score REAL,
            metadata TEXT,
            embedding_id INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS semantic_clusters (
            cluster_id INTEGER PRIMARY KEY,
            cluster_name TEXT,
            cluster_description TEXT,
            centroid_embedding BLOB,
            chunk_count INTEGER,
            avg_quality REAL
        )
        ''')
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS chunk_relationships (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            chunk_id1 TEXT,
            chunk_id2 TEXT,
            relationship_type TEXT,
            similarity_score REAL,
            FOREIGN KEY (chunk_id1) REFERENCES vector_chunks(chunk_id),
            FOREIGN KEY (chunk_id2) REFERENCES vector_chunks(chunk_id)
        )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_chunk_type ON vector_chunks(chunk_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON vector_chunks(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_quality ON vector_chunks(quality_score)')
        
        conn.commit()
        conn.close()
        
    def load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        try:
            # CodeBERT ë˜ëŠ” ë‹¤ë¥¸ ì½”ë“œ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
            self.model = SentenceTransformer('microsoft/codebert-base')
            self.model.to(self.device)
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {self.device})")
            
        except Exception as e:
            logger.warning(f"CodeBERT ë¡œë“œ ì‹¤íŒ¨, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {e}")
            # ëŒ€ì²´ ëª¨ë¸
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            self.dimension = 384  # MiniLM dimension
            
    def index_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì¸ë±ì‹±"""
        logger.info("ğŸš€ ë²¡í„° ì¸ë±ì‹± ì‹œì‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model()
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.init_faiss_index()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
        categories = [
            'microsoft_docs',
            'github_samples', 
            'nuget_packages',
            'stackoverflow_advanced',
            'expert_blogs',
            'performance_guides',
            'design_patterns',
            'unity_best_practices'
        ]
        
        for category in categories:
            category_path = self.data_dir / category
            if category_path.exists():
                logger.info(f"ğŸ“ {category} ì¸ë±ì‹± ì¤‘...")
                self.index_category(category, category_path)
                
        # ì¸ë±ìŠ¤ ì €ì¥
        self.save_index()
        
        # í´ëŸ¬ìŠ¤í„°ë§
        self.perform_clustering()
        
        # ê´€ê³„ ë¶„ì„
        self.analyze_relationships()
        
        # í†µê³„ ì €ì¥
        self.save_statistics()
        
    def init_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        # IVF (Inverted File) ì¸ë±ìŠ¤ ì‚¬ìš©
        nlist = 100  # í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        # ì–‘ìí™”ê¸°
        quantizer = faiss.IndexFlatL2(self.dimension)
        
        # IVF ì¸ë±ìŠ¤
        self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        
        logger.info(f"ğŸ“Š FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (dimension: {self.dimension}, nlist: {nlist})")
        
    def index_category(self, category: str, category_path: Path):
        """ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ì‹±"""
        chunks_to_index = []
        
        for file_path in category_path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = self.extract_chunks(data, category, file_path)
                
                for chunk in chunks:
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    chunk.quality_score = self.calculate_chunk_quality(chunk)
                    
                    # ì„ë² ë”© ìƒì„±
                    chunk.embedding = self.create_embedding(chunk.content)
                    
                    # ì²­í¬ ì €ì¥
                    self.save_chunk(chunk)
                    chunks_to_index.append(chunk)
                    
                    self.stats['total_chunks'] += 1
                    self.stats['categories'][category] += 1
                    
            except Exception as e:
                logger.error(f"íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
                
        # FAISSì— ì¶”ê°€
        if chunks_to_index:
            self.add_to_faiss(chunks_to_index)
            
        self.stats['total_files'] += len(list(category_path.glob('*.json')))
        
    def extract_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """ë°ì´í„°ì—ì„œ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        if category == 'microsoft_docs':
            chunks.extend(self.extract_doc_chunks(data, category, file_path))
            
        elif category == 'github_samples':
            chunks.extend(self.extract_code_chunks(data, category, file_path))
            
        elif category == 'stackoverflow_advanced':
            chunks.extend(self.extract_qa_chunks(data, category, file_path))
            
        elif category == 'design_patterns':
            chunks.extend(self.extract_pattern_chunks(data, category, file_path))
            
        else:
            # ê¸°ë³¸ ì²­í¬ ì¶”ì¶œ
            chunks.extend(self.extract_default_chunks(data, category, file_path))
            
        return chunks
        
    def extract_doc_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """ë¬¸ì„œì—ì„œ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        # ì œëª©ê³¼ ì„¤ëª…
        if 'title' in data and 'content' in data:
            doc_chunk = CodeChunk(
                id=self.generate_chunk_id(file_path, 'doc', 0),
                file_path=str(file_path),
                content=f"{data['title']}\n\n{data.get('content', '')[:1000]}",
                chunk_type='documentation',
                category=category,
                metadata={'topic': data.get('topic', '')}
            )
            chunks.append(doc_chunk)
            
        # ì½”ë“œ ìƒ˜í”Œ
        for i, sample in enumerate(data.get('code_samples', [])):
            if 'code' in sample:
                code_chunk = CodeChunk(
                    id=self.generate_chunk_id(file_path, 'code', i),
                    file_path=str(file_path),
                    content=sample['code'],
                    chunk_type='code',
                    category=category,
                    metadata={
                        'language': sample.get('language', 'csharp'),
                        'topic': data.get('topic', '')
                    }
                )
                chunks.append(code_chunk)
                
        return chunks
        
    def extract_code_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """ì½”ë“œ íŒŒì¼ì—ì„œ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        if 'content' in data:
            # ì½”ë“œë¥¼ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• 
            code_content = data['content']
            code_blocks = self.split_code_into_blocks(code_content)
            
            for i, block in enumerate(code_blocks):
                if len(block['code']) > 50:  # ì˜ë¯¸ìˆëŠ” í¬ê¸°
                    chunk = CodeChunk(
                        id=self.generate_chunk_id(file_path, 'code', i),
                        file_path=str(file_path),
                        content=block['code'],
                        chunk_type='code',
                        category=category,
                        metadata={
                            'block_type': block['type'],
                            'file_name': data.get('file_name', ''),
                            'repo': data.get('repo', ''),
                            'start_line': block.get('start_line', 0),
                            'end_line': block.get('end_line', 0)
                        }
                    )
                    chunks.append(chunk)
                    
        return chunks
        
    def split_code_into_blocks(self, code: str) -> List[Dict]:
        """ì½”ë“œë¥¼ ì˜ë¯¸ìˆëŠ” ë¸”ë¡ìœ¼ë¡œ ë¶„í• """
        blocks = []
        lines = code.split('\n')
        
        current_block = {
            'type': 'unknown',
            'code': '',
            'start_line': 0,
            'end_line': 0
        }
        
        in_class = False
        in_method = False
        brace_count = 0
        
        for i, line in enumerate(lines):
            # í´ë˜ìŠ¤ ì‹œì‘
            if re.search(r'\bclass\s+\w+', line):
                if current_block['code']:
                    blocks.append(current_block)
                current_block = {
                    'type': 'class',
                    'code': line + '\n',
                    'start_line': i,
                    'end_line': i
                }
                in_class = True
                brace_count = 0
                
            # ë©”ì„œë“œ ì‹œì‘
            elif re.search(r'(public|private|protected|internal)\s+\w+\s+\w+\s*\(', line):
                if current_block['code'] and current_block['type'] == 'method':
                    blocks.append(current_block)
                    
                current_block = {
                    'type': 'method',
                    'code': line + '\n',
                    'start_line': i,
                    'end_line': i
                }
                in_method = True
                brace_count = 0
                
            else:
                current_block['code'] += line + '\n'
                current_block['end_line'] = i
                
            # ì¤‘ê´„í˜¸ ì¹´ìš´íŠ¸
            brace_count += line.count('{') - line.count('}')
            
            # ë¸”ë¡ ì¢…ë£Œ
            if brace_count == 0 and (in_class or in_method) and '{' in current_block['code']:
                blocks.append(current_block)
                current_block = {
                    'type': 'unknown',
                    'code': '',
                    'start_line': i + 1,
                    'end_line': i + 1
                }
                in_class = False
                in_method = False
                
        # ë§ˆì§€ë§‰ ë¸”ë¡
        if current_block['code']:
            blocks.append(current_block)
            
        return blocks
        
    def extract_qa_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """Q&Aì—ì„œ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        # ì§ˆë¬¸
        if 'title' in data and 'body' in data:
            question_chunk = CodeChunk(
                id=self.generate_chunk_id(file_path, 'question', 0),
                file_path=str(file_path),
                content=f"Q: {data['title']}\n\n{data.get('body', '')[:500]}",
                chunk_type='qa',
                category=category,
                metadata={
                    'tags': data.get('tags', []),
                    'score': data.get('score', 0)
                }
            )
            chunks.append(question_chunk)
            
        # ë‹µë³€
        for i, answer in enumerate(data.get('answers', [])):
            if 'body' in answer:
                answer_chunk = CodeChunk(
                    id=self.generate_chunk_id(file_path, 'answer', i),
                    file_path=str(file_path),
                    content=f"A: {answer['body'][:1000]}",
                    chunk_type='qa',
                    category=category,
                    metadata={
                        'score': answer.get('score', 0),
                        'is_accepted': answer.get('is_accepted', False)
                    }
                )
                chunks.append(answer_chunk)
                
        # ì½”ë“œ ìƒ˜í”Œ
        for i, sample in enumerate(data.get('code_samples', [])):
            if 'code' in sample:
                code_chunk = CodeChunk(
                    id=self.generate_chunk_id(file_path, 'qa_code', i),
                    file_path=str(file_path),
                    content=sample['code'],
                    chunk_type='code',
                    category=category,
                    metadata={'from_qa': True}
                )
                chunks.append(code_chunk)
                
        return chunks
        
    def extract_pattern_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """ë””ìì¸ íŒ¨í„´ì—ì„œ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        # íŒ¨í„´ ì„¤ëª…
        if 'name' in data:
            pattern_chunk = CodeChunk(
                id=self.generate_chunk_id(file_path, 'pattern', 0),
                file_path=str(file_path),
                content=f"Pattern: {data['name']}\n\nIntent: {data.get('intent', '')}\n\nWhen to use: {data.get('when_to_use', [])}",
                chunk_type='pattern',
                category=category,
                metadata={
                    'pattern_name': data['name'],
                    'pattern_category': data.get('category', '')
                }
            )
            chunks.append(pattern_chunk)
            
        # êµ¬í˜„ ì½”ë“œ
        if 'implementation' in data and 'csharp' in data['implementation']:
            impl_chunk = CodeChunk(
                id=self.generate_chunk_id(file_path, 'pattern_code', 0),
                file_path=str(file_path),
                content=data['implementation']['csharp'],
                chunk_type='code',
                category=category,
                metadata={
                    'pattern_name': data.get('name', ''),
                    'is_pattern_implementation': True
                }
            )
            chunks.append(impl_chunk)
            
        return chunks
        
    def extract_default_chunks(self, data: Dict, category: str, file_path: Path) -> List[CodeChunk]:
        """ê¸°ë³¸ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        
        # JSON ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ë¡œ
        content = json.dumps(data, indent=2)
        
        # í¬ê¸° ì œí•œìœ¼ë¡œ ë¶„í• 
        max_chunk_size = 1000
        for i in range(0, len(content), max_chunk_size):
            chunk_content = content[i:i+max_chunk_size]
            
            chunk = CodeChunk(
                id=self.generate_chunk_id(file_path, 'default', i // max_chunk_size),
                file_path=str(file_path),
                content=chunk_content,
                chunk_type='documentation',
                category=category,
                metadata={}
            )
            chunks.append(chunk)
            
        return chunks
        
    def generate_chunk_id(self, file_path: Path, chunk_type: str, index: int) -> str:
        """ì²­í¬ ID ìƒì„±"""
        base = f"{file_path.stem}_{chunk_type}_{index}"
        return hashlib.md5(base.encode()).hexdigest()
        
    def calculate_chunk_quality(self, chunk: CodeChunk) -> float:
        """ì²­í¬ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ì²­í¬ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        type_weights = {
            'code': 0.8,
            'pattern': 0.9,
            'qa': 0.7,
            'documentation': 0.6
        }
        quality = type_weights.get(chunk.chunk_type, 0.5)
        
        # ì½˜í…ì¸  ê¸¸ì´
        if len(chunk.content) > 200:
            quality += 0.1
            
        # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ
        if chunk.metadata.get('score', 0) > 50:  # Stack Overflow ì ìˆ˜
            quality += 0.1
        if chunk.metadata.get('is_accepted', False):
            quality += 0.1
        if chunk.metadata.get('pattern_name'):  # ë””ìì¸ íŒ¨í„´
            quality += 0.15
            
        # ì½”ë“œ í’ˆì§ˆ ì§€í‘œ
        if chunk.chunk_type == 'code':
            # ì£¼ì„ ì¡´ì¬
            if '//' in chunk.content or '/*' in chunk.content:
                quality += 0.05
            # async/await ì‚¬ìš©
            if 'async' in chunk.content and 'await' in chunk.content:
                quality += 0.05
            # LINQ ì‚¬ìš©
            if any(linq in chunk.content for linq in ['.Where(', '.Select(', '.OrderBy(']):
                quality += 0.05
                
        return min(quality, 1.0)
        
    def create_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            # ì½”ë“œ ì „ì²˜ë¦¬
            processed_text = self.preprocess_code(text)
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                embedding = self.model.encode(processed_text, convert_to_numpy=True)
                
            # ì •ê·œí™”
            embedding = embedding / np.linalg.norm(embedding)
            
            return embedding
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            # ëœë¤ ì„ë² ë”© ë°˜í™˜ (fallback)
            return np.random.randn(self.dimension).astype('float32')
            
    def preprocess_code(self, text: str) -> str:
        """ì½”ë“œ ì „ì²˜ë¦¬"""
        # ê³¼ë„í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸°
        max_length = 512
        if len(text) > max_length:
            text = text[:max_length] + "..."
            
        return text.strip()
        
    def save_chunk(self, chunk: CodeChunk):
        """ì²­í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
            INSERT OR REPLACE INTO vector_chunks
            (chunk_id, file_path, content, chunk_type, category,
             quality_score, metadata, embedding_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                chunk.id,
                chunk.file_path,
                chunk.content,
                chunk.chunk_type,
                chunk.category,
                chunk.quality_score,
                json.dumps(chunk.metadata),
                -1  # ì„ì‹œ ID, FAISS ì¶”ê°€ í›„ ì—…ë°ì´íŠ¸
            ))
            
            conn.commit()
            
            # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
            self.chunk_map[chunk.id] = chunk
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()
            
    def add_to_faiss(self, chunks: List[CodeChunk]):
        """FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        if not chunks:
            return
            
        # ì„ë² ë”© ì¶”ì¶œ
        embeddings = np.array([chunk.embedding for chunk in chunks]).astype('float32')
        
        # ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•™ìŠµ í•„ìš”
        if not self.index.is_trained:
            logger.info("ğŸ¯ FAISS ì¸ë±ìŠ¤ í•™ìŠµ ì¤‘...")
            self.index.train(embeddings)
            
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        start_id = len(self.chunk_map) - len(chunks)
        self.index.add(embeddings)
        
        # ì²­í¬ ID ì—…ë°ì´íŠ¸
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for i, chunk in enumerate(chunks):
            embedding_id = start_id + i
            cursor.execute('''
            UPDATE vector_chunks SET embedding_id = ? WHERE chunk_id = ?
            ''', (embedding_id, chunk.id))
            
        conn.commit()
        conn.close()
        
    def save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss_path = self.index_dir / "faiss_index.bin"
        faiss.write_index(self.index, str(faiss_path))
        
        # ì²­í¬ ë§µ ì €ì¥
        chunk_map_path = self.index_dir / "chunk_map.pkl"
        with open(chunk_map_path, 'wb') as f:
            pickle.dump(self.chunk_map, f)
            
        logger.info(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
        
    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_path = self.index_dir / "faiss_index.bin"
        if faiss_path.exists():
            self.index = faiss.read_index(str(faiss_path))
            
        # ì²­í¬ ë§µ ë¡œë“œ
        chunk_map_path = self.index_dir / "chunk_map.pkl"
        if chunk_map_path.exists():
            with open(chunk_map_path, 'rb') as f:
                self.chunk_map = pickle.load(f)
                
        # ëª¨ë¸ ë¡œë“œ
        if self.model is None:
            self.load_model()
            
    def search(self, query: str, k: int = 10, filters: Optional[Dict] = None) -> List[Tuple[CodeChunk, float]]:
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰"""
        # ì¸ë±ìŠ¤ ë¡œë“œ
        if self.index is None:
            self.load_index()
            
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.create_embedding(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, k * 2)  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
        
        # ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
        results = []
        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
            if idx < 0:  # ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤
                continue
                
            # ì²­í¬ ì°¾ê¸°
            chunk = None
            for chunk_id, c in self.chunk_map.items():
                if self.get_embedding_id(chunk_id) == idx:
                    chunk = c
                    break
                    
            if chunk is None:
                continue
                
            # í•„í„° ì ìš©
            if filters:
                if 'category' in filters and chunk.category != filters['category']:
                    continue
                if 'chunk_type' in filters and chunk.chunk_type != filters['chunk_type']:
                    continue
                if 'min_quality' in filters and chunk.quality_score < filters['min_quality']:
                    continue
                    
            # ìœ ì‚¬ë„ ì ìˆ˜ (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
            similarity = 1 / (1 + distance)
            
            results.append((chunk, similarity))
            
            if len(results) >= k:
                break
                
        return results
        
    def get_embedding_id(self, chunk_id: str) -> int:
        """ì²­í¬ IDë¡œ ì„ë² ë”© ID ê°€ì ¸ì˜¤ê¸°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT embedding_id FROM vector_chunks WHERE chunk_id = ?', (chunk_id,))
        result = cursor.fetchone()
        
        conn.close()
        
        return result[0] if result else -1
        
    def perform_clustering(self):
        """ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§"""
        logger.info("ğŸ”® ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        
        if len(self.chunk_map) < 100:
            logger.info("ì²­í¬ê°€ ë„ˆë¬´ ì ì–´ í´ëŸ¬ìŠ¤í„°ë§ ê±´ë„ˆëœ€")
            return
            
        # ëª¨ë“  ì„ë² ë”© ì¶”ì¶œ
        embeddings = []
        chunk_ids = []
        
        for chunk_id, chunk in self.chunk_map.items():
            if chunk.embedding is not None:
                embeddings.append(chunk.embedding)
                chunk_ids.append(chunk_id)
                
        embeddings = np.array(embeddings).astype('float32')
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        n_clusters = min(50, len(embeddings) // 20)
        kmeans = faiss.Kmeans(self.dimension, n_clusters, niter=20, verbose=True)
        kmeans.train(embeddings)
        
        # í´ëŸ¬ìŠ¤í„° í• ë‹¹
        distances, assignments = kmeans.index.search(embeddings, 1)
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥
        clusters = defaultdict(list)
        for i, (chunk_id, cluster_id) in enumerate(zip(chunk_ids, assignments[:, 0])):
            clusters[cluster_id].append({
                'chunk_id': chunk_id,
                'distance': distances[i, 0]
            })
            
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self.save_clusters(clusters, kmeans.centroids)
        
    def save_clusters(self, clusters: Dict, centroids: np.ndarray):
        """í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for cluster_id, chunk_list in clusters.items():
            # í´ëŸ¬ìŠ¤í„° í†µê³„
            chunk_count = len(chunk_list)
            avg_quality = np.mean([
                self.chunk_map[item['chunk_id']].quality_score 
                for item in chunk_list
            ])
            
            # í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± (ê°€ì¥ ë§ì€ ì¹´í…Œê³ ë¦¬)
            categories = defaultdict(int)
            for item in chunk_list:
                chunk = self.chunk_map[item['chunk_id']]
                categories[chunk.category] += 1
                
            cluster_name = max(categories.items(), key=lambda x: x[1])[0]
            
            # í´ëŸ¬ìŠ¤í„° ì„¤ëª… (ìƒ˜í”Œ ì½˜í…ì¸  ê¸°ë°˜)
            sample_chunks = [self.chunk_map[item['chunk_id']] for item in chunk_list[:3]]
            cluster_description = self.generate_cluster_description(sample_chunks)
            
            # ì €ì¥
            cursor.execute('''
            INSERT OR REPLACE INTO semantic_clusters
            (cluster_id, cluster_name, cluster_description, 
             centroid_embedding, chunk_count, avg_quality)
            VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                int(cluster_id),
                cluster_name,
                cluster_description,
                centroids[cluster_id].tobytes(),
                chunk_count,
                avg_quality
            ))
            
        conn.commit()
        conn.close()
        
    def generate_cluster_description(self, sample_chunks: List[CodeChunk]) -> str:
        """í´ëŸ¬ìŠ¤í„° ì„¤ëª… ìƒì„±"""
        # ì²­í¬ íƒ€ì…
        types = [chunk.chunk_type for chunk in sample_chunks]
        type_str = ', '.join(set(types))
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        all_content = ' '.join([chunk.content[:100] for chunk in sample_chunks])
        keywords = re.findall(r'\b[A-Z][a-zA-Z]+\b', all_content)  # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´
        keyword_str = ', '.join(set(keywords[:5]))
        
        return f"Types: {type_str}. Keywords: {keyword_str}"
        
    def analyze_relationships(self):
        """ì²­í¬ ê°„ ê´€ê³„ ë¶„ì„"""
        logger.info("ğŸ”— ì²­í¬ ê´€ê³„ ë¶„ì„ ì¤‘...")
        
        # ìƒ˜í”Œë§ (ëª¨ë“  ìŒì€ ë„ˆë¬´ ë§ìŒ)
        sample_size = min(1000, len(self.chunk_map))
        sample_chunks = list(self.chunk_map.values())[:sample_size]
        
        relationships = []
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ê³„
        for i in range(len(sample_chunks)):
            for j in range(i + 1, len(sample_chunks)):
                chunk1, chunk2 = sample_chunks[i], sample_chunks[j]
                
                # ì„ë² ë”© ìœ ì‚¬ë„
                similarity = np.dot(chunk1.embedding, chunk2.embedding)
                
                if similarity > 0.8:  # ë†’ì€ ìœ ì‚¬ë„
                    relationships.append({
                        'chunk_id1': chunk1.id,
                        'chunk_id2': chunk2.id,
                        'type': 'similar',
                        'score': float(similarity)
                    })
                    
                # ê°™ì€ íŒŒì¼ì˜ ë‹¤ë¥¸ ì²­í¬
                if chunk1.file_path == chunk2.file_path:
                    relationships.append({
                        'chunk_id1': chunk1.id,
                        'chunk_id2': chunk2.id,
                        'type': 'same_file',
                        'score': 1.0
                    })
                    
        # ê´€ê³„ ì €ì¥
        self.save_relationships(relationships)
        
    def save_relationships(self, relationships: List[Dict]):
        """ê´€ê³„ ì •ë³´ ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for rel in relationships:
            cursor.execute('''
            INSERT OR IGNORE INTO chunk_relationships
            (chunk_id1, chunk_id2, relationship_type, similarity_score)
            VALUES (?, ?, ?, ?)
            ''', (rel['chunk_id1'], rel['chunk_id2'], rel['type'], rel['score']))
            
        conn.commit()
        conn.close()
        
    def save_statistics(self):
        """í†µê³„ ì €ì¥"""
        # í‰ê·  í’ˆì§ˆ ê³„ì‚°
        all_qualities = [chunk.quality_score for chunk in self.chunk_map.values()]
        self.stats['avg_quality'] = np.mean(all_qualities) if all_qualities else 0
        
        # JSON ì €ì¥
        stats_file = self.index_dir / 'vector_index_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2)
            
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
        self.generate_report()
        
    def generate_report(self):
        """ì¸ë±ì‹± ë¦¬í¬íŠ¸ ìƒì„±"""
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ë¡œë“œ
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM semantic_clusters')
        cluster_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM chunk_relationships')
        relationship_count = cursor.fetchone()[0]
        
        conn.close()
        
        report = f"""# ë²¡í„° ì¸ë±ì‹± ë¦¬í¬íŠ¸

## ğŸ“Š ì¸ë±ì‹± í†µê³„
- **ì´ ì²­í¬ ìˆ˜**: {self.stats['total_chunks']:,}
- **ì´ íŒŒì¼ ìˆ˜**: {self.stats['total_files']:,}
- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {self.stats['avg_quality']:.3f}
- **ì¸ë±ìŠ¤ ì°¨ì›**: {self.dimension}
- **í´ëŸ¬ìŠ¤í„° ìˆ˜**: {cluster_count}
- **ê´€ê³„ ìˆ˜**: {relationship_count:,}

## ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ì²­í¬ ë¶„í¬
"""
        
        for category, count in sorted(self.stats['categories'].items(), key=lambda x: x[1], reverse=True):
            percentage = (count / self.stats['total_chunks'] * 100) if self.stats['total_chunks'] > 0 else 0
            report += f"- **{category}**: {count:,} ({percentage:.1f}%)\n"
            
        report += f"""
## ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥
- ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì§€ì›
- ì¹´í…Œê³ ë¦¬/íƒ€ì…ë³„ í•„í„°ë§
- í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ë­í‚¹
- ìœ ì‚¬ ì½”ë“œ ì°¾ê¸°

## ğŸ’¡ í™œìš© ì˜ˆì‹œ
```python
# ê²€ìƒ‰ ì˜ˆì‹œ
indexer = VectorIndexer()
results = indexer.search(
    "async await pattern", 
    k=5,
    filters={{'category': 'github_samples', 'min_quality': 0.8}}
)

for chunk, similarity in results:
    print(f"ìœ ì‚¬ë„: {{similarity:.3f}}")
    print(chunk.content[:200])
```

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„
1. `autoci dual start` - RAG + íŒŒì¸íŠœë‹ ì‹œì‘
2. `autoci enhance start /path` - 24ì‹œê°„ ìë™ ì‹œìŠ¤í…œ ì‹œì‘

---
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_file = self.index_dir / 'vector_index_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"ğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    indexer = VectorIndexer()
    
    # ì „ì²´ ì¸ë±ì‹±
    indexer.index_all_data()
    
    print("\nâœ… ë²¡í„° ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {indexer.stats['total_chunks']:,}ê°œ ì²­í¬ ì¸ë±ì‹±")
    print(f"ğŸ“ ì¸ë±ìŠ¤ ìœ„ì¹˜: {indexer.index_dir}")
    print(f"ğŸ¯ í‰ê·  í’ˆì§ˆ: {indexer.stats['avg_quality']:.3f}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    results = indexer.search("async await best practices", k=3)
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    
    for i, (chunk, similarity) in enumerate(results):
        print(f"\n[{i+1}] ìœ ì‚¬ë„: {similarity:.3f}")
        print(f"ì¹´í…Œê³ ë¦¬: {chunk.category}, íƒ€ì…: {chunk.chunk_type}")
        print(f"ë‚´ìš©: {chunk.content[:100]}...")


if __name__ == "__main__":
    main()