#!/usr/bin/env python3
"""
Distributed Training System for Neural AutoCI
ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„° ì‹ ê²½ë§ì„ ìœ„í•œ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ
"""

import os
import sys
import time
import json
import logging
import threading
import multiprocessing as mp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
import sqlite3
import queue
import signal
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, Dataset, DistributedSampler
    from torch.nn.parallel import DistributedDataParallel as DDP
    import torch.distributed as dist
    import torch.multiprocessing as torch_mp
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ë¶„ì‚° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")

# ë¡œì»¬ ëª¨ë“ˆ
try:
    from large_scale_training_pipeline import LargeScaleDatasetDatabase, TrainingExample
    from neural_gpt_autoci import NeuralGPTAutoCI, ModelConfig, GPTDataset
    MODULES_AVAILABLE = True
except ImportError:
    MODULES_AVAILABLE = False
    print("âš ï¸ ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # ì‹œë®¬ë ˆì´ì…˜ìš© ë”ë¯¸ í´ë˜ìŠ¤
    class ModelConfig:
        def __init__(self):
            self.vocab_size = 50000
            self.hidden_size = 1024
            self.num_layers = 12
            self.num_heads = 16
            self.total_parameters = 1000000000  # 10ì–µ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DistributedConfig:
    """ë¶„ì‚° í•™ìŠµ ì„¤ì •"""
    num_gpus: int = 4
    num_nodes: int = 1
    world_size: int = 4  # num_gpus * num_nodes
    batch_size_per_gpu: int = 8
    gradient_accumulation_steps: int = 16
    max_epochs: int = 100
    learning_rate: float = 1e-4
    warmup_steps: int = 10000
    save_steps: int = 5000
    eval_steps: int = 1000
    logging_steps: int = 100
    
    # ëª¨ë¸ ë³‘ë ¬í™” ì„¤ì •
    model_parallel: bool = True
    pipeline_parallel_size: int = 2
    data_parallel_size: int = 2
    
    # ìµœì í™” ì„¤ì •
    mixed_precision: bool = True
    gradient_checkpointing: bool = True
    cpu_offload: bool = True
    
    @property
    def global_batch_size(self) -> int:
        return self.batch_size_per_gpu * self.world_size * self.gradient_accumulation_steps

@dataclass
class TrainingMetrics:
    """í•™ìŠµ ë©”íŠ¸ë¦­"""
    epoch: int
    step: int
    loss: float
    learning_rate: float
    grad_norm: float
    memory_usage: float
    throughput: float  # tokens per second
    timestamp: str

class DistributedTrainer:
    """ë¶„ì‚° í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, config: DistributedConfig, model_config: ModelConfig):
        self.config = config
        self.model_config = model_config
        self.device = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.global_step = 0
        self.best_loss = float('inf')
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.training_metrics = []
        self.validation_metrics = []
        
        # ë°ì´í„°
        self.train_dataset = None
        self.val_dataset = None
        self.train_loader = None
        self.val_loader = None
        
        # ì²´í¬í¬ì¸íŠ¸
        self.checkpoint_dir = "checkpoints"
        os.makedirs(self.checkpoint_dir, exist_ok=True)

    def setup_distributed(self, rank: int, world_size: int):
        """ë¶„ì‚° í™˜ê²½ ì„¤ì •"""
        if not TORCH_AVAILABLE:
            logger.warning("PyTorch ì—†ìŒ - ë¶„ì‚° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜")
            return
        
        try:
            os.environ['MASTER_ADDR'] = 'localhost'
            os.environ['MASTER_PORT'] = '12355'
            
            # ë¶„ì‚° í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”
            dist.init_process_group("nccl", rank=rank, world_size=world_size)
            
            # GPU ì„¤ì •
            torch.cuda.set_device(rank)
            self.device = torch.device(f'cuda:{rank}')
            
            logger.info(f"âœ… ë¶„ì‚° í™˜ê²½ ì„¤ì • ì™„ë£Œ - Rank: {rank}, World Size: {world_size}")
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì‚° í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.device = torch.device('cpu')

    def setup_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if not TORCH_AVAILABLE or not MODULES_AVAILABLE:
            logger.warning("ëª¨ë¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
            return self._setup_simulation_model()
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = NeuralGPTAutoCI(self.model_config)
            
            if self.device:
                self.model = self.model.to(self.device)
            
            # ë¶„ì‚° ë°ì´í„° ë³‘ë ¬í™”
            if torch.cuda.is_available() and dist.is_initialized():
                self.model = DDP(self.model, device_ids=[self.device.index])
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=0.01,
                betas=(0.9, 0.95)
            )
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            if hasattr(torch.optim.lr_scheduler, 'get_linear_schedule_with_warmup'):
                self.scheduler = torch.optim.lr_scheduler.get_linear_schedule_with_warmup(
                    self.optimizer,
                    num_warmup_steps=self.config.warmup_steps,
                    num_training_steps=self.config.max_epochs * 1000
                )
            
            # Mixed Precision
            if self.config.mixed_precision and torch.cuda.is_available():
                self.scaler = torch.cuda.amp.GradScaler()
            
            logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {self.model_config.total_parameters:,}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return self._setup_simulation_model()

    def _setup_simulation_model(self):
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì„¤ì •"""
        class SimulationModel:
            def __init__(self, config):
                self.config = config
                self.parameters_count = config.total_parameters
                
            def train(self):
                pass
                
            def eval(self):
                pass
                
            def parameters(self):
                return []
                
            def state_dict(self):
                return {}
                
            def load_state_dict(self, state_dict):
                pass
        
        self.model = SimulationModel(self.model_config)
        logger.info(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì„¤ì • ì™„ë£Œ")

    def setup_data(self):
        """ë°ì´í„° ë¡œë” ì„¤ì •"""
        try:
            if MODULES_AVAILABLE:
                # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                db = LargeScaleDatasetDatabase()
                
                # í•™ìŠµ ë°ì´í„°
                train_examples = []
                batch_size = 1000
                offset = 0
                
                while True:
                    batch = db.get_training_batch(batch_size, 'train')
                    if not batch:
                        break
                    train_examples.extend(batch)
                    offset += batch_size
                    if len(train_examples) >= 10000:  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
                        break
                
                # ê²€ì¦ ë°ì´í„°
                val_examples = db.get_training_batch(2000, 'validation')
                
                logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ - Train: {len(train_examples)}, Val: {len(val_examples)}")
                
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
                train_examples = self._generate_simulation_data(10000)
                val_examples = self._generate_simulation_data(2000)
                logger.info("ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì™„ë£Œ")
            
            # ë°ì´í„°ì…‹ ìƒì„±
            if TORCH_AVAILABLE and MODULES_AVAILABLE:
                self.train_dataset = GPTDataset(train_examples)
                self.val_dataset = GPTDataset(val_examples)
                
                # ë¶„ì‚° ìƒ˜í”ŒëŸ¬
                if dist.is_initialized():
                    train_sampler = DistributedSampler(self.train_dataset)
                    val_sampler = DistributedSampler(self.val_dataset)
                else:
                    train_sampler = None
                    val_sampler = None
                
                # ë°ì´í„° ë¡œë”
                self.train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=self.config.batch_size_per_gpu,
                    sampler=train_sampler,
                    shuffle=(train_sampler is None),
                    num_workers=4,
                    pin_memory=True
                )
                
                self.val_loader = DataLoader(
                    self.val_dataset,
                    batch_size=self.config.batch_size_per_gpu,
                    sampler=val_sampler,
                    shuffle=False,
                    num_workers=4,
                    pin_memory=True
                )
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ë¡œë”
                self.train_loader = self._create_simulation_loader(train_examples)
                self.val_loader = self._create_simulation_loader(val_examples)
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì„¤ì • ì˜¤ë¥˜: {e}")
            # í´ë°± ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            self.train_loader = self._create_simulation_loader([])
            self.val_loader = self._create_simulation_loader([])

    def _generate_simulation_data(self, count: int) -> List[Dict]:
        """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±"""
        data = []
        for i in range(count):
            data.append({
                'input_text': f"Unity ì§ˆë¬¸ {i}",
                'target_output': f"Unity ë‹µë³€ {i}",
                'tokens': 50
            })
        return data

    def _create_simulation_loader(self, data: List[Dict]):
        """ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë¡œë”"""
        class SimulationLoader:
            def __init__(self, data, batch_size):
                self.data = data
                self.batch_size = batch_size
                
            def __iter__(self):
                for i in range(0, len(self.data), self.batch_size):
                    batch = self.data[i:i + self.batch_size]
                    yield {
                        'input_ids': [list(range(50)) for _ in batch],
                        'attention_mask': [[1] * 50 for _ in batch],
                        'labels': [list(range(50)) for _ in batch]
                    }
            
            def __len__(self):
                return (len(self.data) + self.batch_size - 1) // self.batch_size
        
        return SimulationLoader(data, self.config.batch_size_per_gpu)

    def train_epoch(self, epoch: int):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        logger.info(f"ğŸš€ ì—í¬í¬ {epoch} í•™ìŠµ ì‹œì‘")
        
        try:
            for batch_idx, batch in enumerate(self.train_loader):
                start_time = time.time()
                
                # ì‹¤ì œ ë˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ
                if TORCH_AVAILABLE and hasattr(batch, 'get'):
                    loss = self._train_step_real(batch)
                else:
                    loss = self._train_step_simulation(batch)
                
                epoch_loss += loss
                num_batches += 1
                
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                if self.global_step % self.config.logging_steps == 0:
                    self._log_training_metrics(epoch, loss, time.time() - start_time)
                
                # ê²€ì¦
                if self.global_step % self.config.eval_steps == 0:
                    val_loss = self.evaluate()
                    self._save_checkpoint_if_best(val_loss)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.global_step % self.config.save_steps == 0:
                    self._save_checkpoint()
                
                self.global_step += 1
                
                if batch_idx >= 100:  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
                    break
        
        except KeyboardInterrupt:
            logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
            return
        except Exception as e:
            logger.error(f"í•™ìŠµ ì˜¤ë¥˜: {e}")
            return
        
        avg_loss = epoch_loss / max(num_batches, 1)
        logger.info(f"âœ… ì—í¬í¬ {epoch} ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        return avg_loss

    def _train_step_real(self, batch) -> float:
        """ì‹¤ì œ PyTorch í•™ìŠµ ìŠ¤í…"""
        if self.config.mixed_precision and self.scaler:
            with torch.cuda.amp.autocast():
                outputs = self.model(**batch)
                loss = outputs.loss
            
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            outputs = self.model(**batch)
            loss = outputs.loss
            loss.backward()
            self.optimizer.step()
        
        if self.scheduler:
            self.scheduler.step()
        
        self.optimizer.zero_grad()
        
        return loss.item()

    def _train_step_simulation(self, batch) -> float:
        """ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ìŠ¤í…"""
        # ê°€ìƒì˜ ì†ì‹¤ ê³„ì‚° (ì‹¤ì œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜)
        base_loss = 2.0
        step_reduction = self.global_step * 0.00001
        noise = (time.time() % 1) * 0.1
        
        simulated_loss = max(0.1, base_loss - step_reduction + noise)
        
        # í•™ìŠµ ì§„í–‰ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§€ì—°
        time.sleep(0.01)
        
        return simulated_loss

    def evaluate(self) -> float:
        """ëª¨ë¸ í‰ê°€"""
        self.model.eval()
        eval_loss = 0.0
        num_batches = 0
        
        logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        try:
            with torch.no_grad() if TORCH_AVAILABLE else contextlib.nullcontext():
                for batch_idx, batch in enumerate(self.val_loader):
                    if TORCH_AVAILABLE and hasattr(batch, 'get'):
                        if self.config.mixed_precision:
                            with torch.cuda.amp.autocast():
                                outputs = self.model(**batch)
                                loss = outputs.loss
                        else:
                            outputs = self.model(**batch)
                            loss = outputs.loss
                        eval_loss += loss.item()
                    else:
                        # ì‹œë®¬ë ˆì´ì…˜ í‰ê°€
                        eval_loss += self._train_step_simulation(batch) * 0.9
                    
                    num_batches += 1
                    
                    if batch_idx >= 20:  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
                        break
        
        except Exception as e:
            logger.error(f"í‰ê°€ ì˜¤ë¥˜: {e}")
            return float('inf')
        
        avg_eval_loss = eval_loss / max(num_batches, 1)
        logger.info(f"ğŸ“Š í‰ê°€ ì™„ë£Œ - ì†ì‹¤: {avg_eval_loss:.4f}")
        
        self.model.train()
        return avg_eval_loss

    def _log_training_metrics(self, epoch: int, loss: float, step_time: float):
        """í•™ìŠµ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        metrics = TrainingMetrics(
            epoch=epoch,
            step=self.global_step,
            loss=loss,
            learning_rate=self.config.learning_rate,
            grad_norm=0.0,  # ì‹œë®¬ë ˆì´ì…˜
            memory_usage=0.0,  # ì‹œë®¬ë ˆì´ì…˜
            throughput=self.config.batch_size_per_gpu / step_time,
            timestamp=datetime.now().isoformat()
        )
        
        self.training_metrics.append(metrics)
        
        if self.global_step % (self.config.logging_steps * 10) == 0:
            logger.info(f"Step {self.global_step}: Loss={loss:.4f}, LR={metrics.learning_rate:.6f}")

    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = os.path.join(
            self.checkpoint_dir, 
            f"checkpoint_step_{self.global_step}.pt"
        )
        
        try:
            if TORCH_AVAILABLE and hasattr(self.model, 'state_dict'):
                checkpoint = {
                    'epoch': self.current_epoch,
                    'global_step': self.global_step,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else {},
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else {},
                    'config': asdict(self.config),
                    'model_config': asdict(self.model_config)
                }
                torch.save(checkpoint, checkpoint_path)
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ì²´í¬í¬ì¸íŠ¸
                checkpoint = {
                    'epoch': self.current_epoch,
                    'global_step': self.global_step,
                    'config': asdict(self.config),
                    'model_config': asdict(self.model_config),
                    'metrics': [asdict(m) for m in self.training_metrics[-100:]]
                }
                with open(checkpoint_path.replace('.pt', '.json'), 'w') as f:
                    json.dump(checkpoint, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_checkpoint_if_best(self, val_loss: float):
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            best_path = os.path.join(self.checkpoint_dir, "best_model.pt")
            
            try:
                if TORCH_AVAILABLE and hasattr(self.model, 'state_dict'):
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'val_loss': val_loss,
                        'global_step': self.global_step
                    }, best_path)
                else:
                    with open(best_path.replace('.pt', '.json'), 'w') as f:
                        json.dump({
                            'val_loss': val_loss,
                            'global_step': self.global_step,
                            'timestamp': datetime.now().isoformat()
                        }, f, indent=2)
                
                logger.info(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {val_loss:.4f}")
                
            except Exception as e:
                logger.error(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def train(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        logger.info(f"ğŸš€ ë¶„ì‚° í•™ìŠµ ì‹œì‘")
        logger.info(f"ğŸ“Š ì„¤ì •: {self.config.world_size}ê°œ í”„ë¡œì„¸ìŠ¤, {self.config.max_epochs}ê°œ ì—í¬í¬")
        
        start_time = time.time()
        
        try:
            for epoch in range(self.config.max_epochs):
                self.current_epoch = epoch
                
                # ì—í¬í¬ í•™ìŠµ
                avg_loss = self.train_epoch(epoch)
                
                # ê²€ì¦
                val_loss = self.evaluate()
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                self.validation_metrics.append({
                    'epoch': epoch,
                    'train_loss': avg_loss,
                    'val_loss': val_loss,
                    'timestamp': datetime.now().isoformat()
                })
                
                logger.info(f"ì—í¬í¬ {epoch}: Train Loss={avg_loss:.4f}, Val Loss={val_loss:.4f}")
                
                # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                if self._should_early_stop():
                    logger.info("ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ë§Œì¡±")
                    break
        
        except KeyboardInterrupt:
            logger.info("í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­")
        except Exception as e:
            logger.error(f"í•™ìŠµ ì˜¤ë¥˜: {e}")
        
        finally:
            # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self._save_checkpoint()
            
            # í•™ìŠµ ì™„ë£Œ ë³´ê³ ì„œ
            self._generate_training_report()
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time/3600:.1f}ì‹œê°„)")

    def _should_early_stop(self) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ í™•ì¸"""
        if len(self.validation_metrics) < 5:
            return False
        
        # ìµœê·¼ 5ê°œ ì—í¬í¬ì˜ ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        recent_losses = [m['val_loss'] for m in self.validation_metrics[-5:]]
        return all(loss >= recent_losses[0] for loss in recent_losses[1:])

    def _generate_training_report(self):
        """í•™ìŠµ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'training_summary': {
                'total_epochs': self.current_epoch + 1,
                'total_steps': self.global_step,
                'best_val_loss': self.best_loss,
                'final_train_loss': self.training_metrics[-1].loss if self.training_metrics else 0,
                'training_time_hours': len(self.training_metrics) * 0.01 / 3600  # ì‹œë®¬ë ˆì´ì…˜
            },
            'config': asdict(self.config),
            'model_config': asdict(self.model_config),
            'training_metrics': [asdict(m) for m in self.training_metrics[-100:]],
            'validation_metrics': self.validation_metrics,
            'generated_at': datetime.now().isoformat()
        }
        
        report_path = os.path.join(self.checkpoint_dir, "training_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ í•™ìŠµ ë³´ê³ ì„œ ì €ì¥: {report_path}")

def run_distributed_training(rank: int, world_size: int, config: DistributedConfig, model_config: ModelConfig):
    """ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ë¶„ì‚° íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = DistributedTrainer(config, model_config)
        
        # ë¶„ì‚° í™˜ê²½ ì„¤ì •
        trainer.setup_distributed(rank, world_size)
        
        # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
        trainer.setup_model()
        trainer.setup_data()
        
        # í•™ìŠµ ì‹œì‘
        trainer.train()
        
    except Exception as e:
        logger.error(f"ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ ì˜¤ë¥˜ (Rank {rank}): {e}")
    
    finally:
        # ë¶„ì‚° í™˜ê²½ ì •ë¦¬
        if TORCH_AVAILABLE and dist.is_initialized():
            dist.destroy_process_group()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë¶„ì‚° ì‹ ê²½ë§ í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        # ì„¤ì •
        dist_config = DistributedConfig(
            num_gpus=1,  # í…ŒìŠ¤íŠ¸ìš©
            num_nodes=1,
            world_size=1,
            batch_size_per_gpu=4,
            max_epochs=5,  # í…ŒìŠ¤íŠ¸ìš©
            mixed_precision=False,  # ì‹œë®¬ë ˆì´ì…˜
            gradient_checkpointing=False
        )
        
        if MODULES_AVAILABLE:
            from neural_gpt_autoci import ModelConfig
            model_config = ModelConfig(
                vocab_size=50000,
                hidden_size=1024,  # í…ŒìŠ¤íŠ¸ìš© ì¶•ì†Œ
                num_layers=12,
                num_heads=16,
                max_position_embeddings=512
            )
        else:
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì„¤ì •
            class ModelConfig:
                def __init__(self):
                    self.vocab_size = 50000
                    self.hidden_size = 1024
                    self.num_layers = 12
                    self.num_heads = 16
                    self.total_parameters = 1000000000  # 10ì–µ
            
            model_config = ModelConfig()
        
        logger.info(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model_config.total_parameters:,}")
        logger.info(f"ì „ì—­ ë°°ì¹˜ í¬ê¸°: {dist_config.global_batch_size}")
        
        # ë¶„ì‚° í•™ìŠµ ì‹œì‘
        if TORCH_AVAILABLE and dist_config.world_size > 1:
            # ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë¶„ì‚° í•™ìŠµ
            torch_mp.spawn(
                run_distributed_training,
                args=(dist_config.world_size, dist_config, model_config),
                nprocs=dist_config.world_size,
                join=True
            )
        else:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í•™ìŠµ
            run_distributed_training(0, 1, dist_config, model_config)
        
        print("ğŸ‰ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        return 0
        
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1

if __name__ == "__main__":
    import contextlib
    import sys
    sys.exit(main())