#!/usr/bin/env python3
"""
AutoCI ì‹¤ì œ ì‹ ê²½ë§ ê¸°ë°˜ í•™ìŠµ AI ì‹œìŠ¤í…œ
PyTorchë¥¼ ì‚¬ìš©í•œ ì§„ì§œ í•™ìŠµí•˜ëŠ” AI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import sqlite3
import threading
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import os

@dataclass
class ConversationData:
    """ëŒ€í™” ë°ì´í„° êµ¬ì¡°"""
    user_input: str
    ai_response: str
    feedback_score: float  # -1.0 to 1.0
    timestamp: str
    context_vector: Optional[np.ndarray] = None

class NeuralResponseNetwork(nn.Module):
    """ì‹ ê²½ë§ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_size: int = 512, hidden_size: int = 256, output_size: int = 128):
        super(NeuralResponseNetwork, self).__init__()
        
        # ì¸ì½”ë” ë ˆì´ì–´
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # ì‘ë‹µ ìƒì„± ë ˆì´ì–´
        self.response_generator = nn.Sequential(
            nn.Linear(hidden_size, output_size),
            nn.ReLU(),
            nn.Linear(output_size, output_size)
        )
        
        # í’ˆì§ˆ í‰ê°€ ë ˆì´ì–´
        self.quality_evaluator = nn.Sequential(
            nn.Linear(hidden_size + output_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # 0-1 í’ˆì§ˆ ì ìˆ˜
        )
    
    def forward(self, input_vector: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        encoded = self.encoder(input_vector)
        response_vector = self.response_generator(encoded)
        
        # í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•´ ì¸ì½”ë”©ê³¼ ì‘ë‹µ ë²¡í„° ê²°í•©
        combined = torch.cat([encoded, response_vector], dim=1)
        quality_score = self.quality_evaluator(combined)
        
        return response_vector, quality_score

class MemorySystem:
    """ê¸°ì–µ ì‹œìŠ¤í…œ - ë‹¨ê¸°/ì¥ê¸°/ì‘ì—… ë©”ëª¨ë¦¬"""
    
    def __init__(self, max_short_term: int = 100, max_long_term: int = 10000):
        self.short_term_memory: List[ConversationData] = []
        self.long_term_memory: List[ConversationData] = []
        self.working_memory: Dict[str, any] = {}
        
        self.max_short_term = max_short_term
        self.max_long_term = max_long_term
        
        # ë²¡í„°í™”ê¸°
        self.vectorizer = TfidfVectorizer(max_features=512, ngram_range=(1, 2))
        self.is_vectorizer_fitted = False
    
    def add_conversation(self, conversation: ConversationData):
        """ëŒ€í™” ì¶”ê°€"""
        # ë²¡í„°í™”
        if not self.is_vectorizer_fitted:
            # ì²« ë²ˆì§¸ ëŒ€í™”ë¡œ ë²¡í„°í™”ê¸° í•™ìŠµ
            self.vectorizer.fit([conversation.user_input])
            self.is_vectorizer_fitted = True
        
        try:
            conversation.context_vector = self.vectorizer.transform([conversation.user_input]).toarray()[0]
        except:
            # ë²¡í„°í™” ì‹¤íŒ¨ì‹œ ì¬í•™ìŠµ
            all_texts = [conv.user_input for conv in self.short_term_memory[-10:]]
            all_texts.append(conversation.user_input)
            self.vectorizer.fit(all_texts)
            conversation.context_vector = self.vectorizer.transform([conversation.user_input]).toarray()[0]
        
        # ë‹¨ê¸° ë©”ëª¨ë¦¬ì— ì¶”ê°€
        self.short_term_memory.append(conversation)
        
        # ë‹¨ê¸° ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
        if len(self.short_term_memory) > self.max_short_term:
            # ì˜¤ë˜ëœ ê²ƒì„ ì¥ê¸° ë©”ëª¨ë¦¬ë¡œ ì´ë™
            old_conversation = self.short_term_memory.pop(0)
            self.promote_to_long_term(old_conversation)
    
    def promote_to_long_term(self, conversation: ConversationData):
        """ì¥ê¸° ë©”ëª¨ë¦¬ë¡œ ìŠ¹ê²©"""
        # ì¤‘ìš”í•œ ëŒ€í™”ë§Œ ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥ (í”¼ë“œë°±ì´ ìˆê±°ë‚˜ ì ìˆ˜ê°€ ë†’ì€ ê²ƒ)
        if abs(conversation.feedback_score) > 0.3:
            self.long_term_memory.append(conversation)
            
            # ì¥ê¸° ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
            if len(self.long_term_memory) > self.max_long_term:
                self.long_term_memory.pop(0)
    
    def find_similar_conversations(self, query: str, top_k: int = 5) -> List[ConversationData]:
        """ìœ ì‚¬í•œ ëŒ€í™” ì°¾ê¸°"""
        if not self.is_vectorizer_fitted:
            return []
        
        try:
            query_vector = self.vectorizer.transform([query]).toarray()[0]
        except:
            return []
        
        all_conversations = self.short_term_memory + self.long_term_memory
        similarities = []
        
        for conv in all_conversations:
            if conv.context_vector is not None:
                similarity = cosine_similarity([query_vector], [conv.context_vector])[0][0]
                similarities.append((similarity, conv))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x[0], reverse=True)
        return [conv for _, conv in similarities[:top_k]]

class RealLearningAutoCI:
    """ì‹¤ì œ í•™ìŠµí•˜ëŠ” AutoCI ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_path: str = "autoci_neural_model.pth"):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”
        self.model = NeuralResponseNetwork().to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()
        
        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.memory = MemorySystem()
        
        # í•™ìŠµ ìƒíƒœ
        self.learning_enabled = True
        self.is_training = False
        self.training_thread = None
        
        # í†µê³„
        self.stats = {
            "total_conversations": 0,
            "total_training_epochs": 0,
            "average_quality_score": 0.0,
            "last_learning_time": None,
            "model_accuracy": 0.0
        }
        
        # ì‘ë‹µ í…œí”Œë¦¿
        self.response_templates = {
            "korean_greeting": ["ì•ˆë…•í•˜ì„¸ìš”!", "ë°˜ê°‘ìŠµë‹ˆë‹¤!", "ì•ˆë…•íˆê³„ì„¸ìš”!"],
            "unity_help": ["Unity ê´€ë ¨ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", "Unity ê°œë°œì„ ì§€ì›í•´ë“œë¦½ë‹ˆë‹¤."],
            "code_analysis": ["ì½”ë“œë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", "ì½”ë“œ ë¦¬ë·°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."],
            "error_help": ["ì˜¤ë¥˜ í•´ê²°ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", "ë¬¸ì œë¥¼ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤."]
        }
        
        self.load_model()
        self.start_background_learning()
        
        print(f"ğŸ§  ì‹¤ì œ í•™ìŠµ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(self.model_path):
            try:
                checkpoint = torch.load(self.model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.stats = checkpoint.get('stats', self.stats)
                print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨: {self.model_path}")
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ ëª¨ë¸ ì‹œì‘: {e}")
        else:
            print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘")
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'stats': self.stats
        }, self.model_path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨: {self.model_path}")
    
    def process_conversation(self, user_input: str, ai_response: str, 
                           feedback_score: Optional[float] = None) -> Dict:
        """ëŒ€í™” ì²˜ë¦¬ ë° í•™ìŠµ"""
        
        # í”¼ë“œë°± ì ìˆ˜ ê³„ì‚° (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì¶”ì •)
        if feedback_score is None:
            feedback_score = self.estimate_feedback_score(user_input, ai_response)
        
        # ëŒ€í™” ë°ì´í„° ìƒì„±
        conversation = ConversationData(
            user_input=user_input,
            ai_response=ai_response,
            feedback_score=feedback_score,
            timestamp=datetime.now().isoformat()
        )
        
        # ë©”ëª¨ë¦¬ì— ì¶”ê°€
        self.memory.add_conversation(conversation)
        self.stats["total_conversations"] += 1
        
        # ì‹¤ì‹œê°„ í•™ìŠµ íŠ¸ë¦¬ê±°
        if self.learning_enabled and abs(feedback_score) > 0.5:
            self.trigger_learning(conversation)
        
        return {
            "conversation_id": self.stats["total_conversations"],
            "feedback_score": feedback_score,
            "learning_triggered": abs(feedback_score) > 0.5,
            "similar_conversations": len(self.memory.find_similar_conversations(user_input))
        }
    
    def estimate_feedback_score(self, user_input: str, ai_response: str) -> float:
        """í”¼ë“œë°± ì ìˆ˜ ìë™ ì¶”ì •"""
        # ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        try:
            input_vector = self.memory.vectorizer.transform([user_input]).toarray()[0]
            input_tensor = torch.FloatTensor(input_vector).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                _, quality_score = self.model(input_tensor)
                estimated_score = quality_score.item()
                
                # -1 ~ 1 ë²”ìœ„ë¡œ ë³€í™˜
                return (estimated_score - 0.5) * 2.0
                
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜
            return 0.0
    
    def generate_response(self, user_input: str) -> Tuple[str, float]:
        """ì‹ ê²½ë§ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        
        # ìœ ì‚¬í•œ ëŒ€í™” ì°¾ê¸°
        similar_conversations = self.memory.find_similar_conversations(user_input, top_k=3)
        
        # ì‹ ê²½ë§ìœ¼ë¡œ ì‘ë‹µ ë²¡í„° ìƒì„±
        try:
            input_vector = self.memory.vectorizer.transform([user_input]).toarray()[0]
            input_tensor = torch.FloatTensor(input_vector).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                response_vector, quality_score = self.model(input_tensor)
                
            # ì‘ë‹µ ë²¡í„°ë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í…œí”Œë¦¿ ê¸°ë°˜)
            response_text = self.vector_to_response(response_vector.cpu().numpy()[0], user_input)
            
            return response_text, quality_score.item()
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ì‘ë‹µ
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”.", 0.5
    
    def vector_to_response(self, response_vector: np.ndarray, user_input: str) -> str:
        """ì‘ë‹µ ë²¡í„°ë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        
        # ê°„ë‹¨í•œ í…œí”Œë¦¿ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë””ì½”ë”© í•„ìš”)
        user_input_lower = user_input.lower()
        
        if any(word in user_input_lower for word in ["ì•ˆë…•", "hello", "hi"]):
            return np.random.choice(self.response_templates["korean_greeting"])
        elif any(word in user_input_lower for word in ["unity", "ìœ ë‹ˆí‹°"]):
            return np.random.choice(self.response_templates["unity_help"])
        elif any(word in user_input_lower for word in ["ì½”ë“œ", "code", "ë¶„ì„"]):
            return np.random.choice(self.response_templates["code_analysis"])
        elif any(word in user_input_lower for word in ["ì˜¤ë¥˜", "ì—ëŸ¬", "error", "ë¬¸ì œ"]):
            return np.random.choice(self.response_templates["error_help"])
        else:
            return "ë„¤, ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    def trigger_learning(self, conversation: ConversationData):
        """ì¦‰ì‹œ í•™ìŠµ íŠ¸ë¦¬ê±°"""
        if not self.is_training:
            self.is_training = True
            threading.Thread(target=self._immediate_learning, args=(conversation,)).start()
    
    def _immediate_learning(self, conversation: ConversationData):
        """ì¦‰ì‹œ í•™ìŠµ ì‹¤í–‰"""
        try:
            if conversation.context_vector is not None:
                # ì…ë ¥ í…ì„œ ì¤€ë¹„
                input_tensor = torch.FloatTensor(conversation.context_vector).unsqueeze(0).to(self.device)
                target_quality = torch.FloatTensor([[conversation.feedback_score]]).to(self.device)
                
                # ìˆœì „íŒŒ
                response_vector, predicted_quality = self.model(input_tensor)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = self.criterion(predicted_quality, target_quality)
                
                # ì—­ì „íŒŒ
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["last_learning_time"] = datetime.now().isoformat()
                self.stats["total_training_epochs"] += 1
                
                print(f"ğŸ”„ ì¦‰ì‹œ í•™ìŠµ ì™„ë£Œ - ì†ì‹¤: {loss.item():.4f}")
                
        except Exception as e:
            print(f"âŒ ì¦‰ì‹œ í•™ìŠµ ì˜¤ë¥˜: {e}")
        finally:
            self.is_training = False
    
    def start_background_learning(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘"""
        if self.training_thread is None or not self.training_thread.is_alive():
            self.training_thread = threading.Thread(target=self._background_learning_loop)
            self.training_thread.daemon = True
            self.training_thread.start()
            print("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘ë¨")
    
    def _background_learning_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë£¨í”„"""
        while self.learning_enabled:
            try:
                # 5ë¶„ë§ˆë‹¤ ë°°ì¹˜ í•™ìŠµ
                time.sleep(300)
                
                if len(self.memory.short_term_memory) >= 10:
                    self._batch_learning()
                    
            except Exception as e:
                print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def _batch_learning(self):
        """ë°°ì¹˜ í•™ìŠµ"""
        if self.is_training:
            return
            
        self.is_training = True
        print("ğŸ¯ ë°°ì¹˜ í•™ìŠµ ì‹œì‘...")
        
        try:
            # ìµœê·¼ ëŒ€í™”ë“¤ë¡œ ë°°ì¹˜ ìƒì„±
            recent_conversations = self.memory.short_term_memory[-20:]
            
            inputs = []
            targets = []
            
            for conv in recent_conversations:
                if conv.context_vector is not None:
                    inputs.append(conv.context_vector)
                    targets.append([conv.feedback_score])
            
            if len(inputs) > 0:
                input_tensor = torch.FloatTensor(inputs).to(self.device)
                target_tensor = torch.FloatTensor(targets).to(self.device)
                
                # ì—¬ëŸ¬ ë²ˆ í•™ìŠµ
                for epoch in range(5):
                    response_vectors, predicted_qualities = self.model(input_tensor)
                    loss = self.criterion(predicted_qualities, target_tensor)
                    
                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()
                
                print(f"âœ… ë°°ì¹˜ í•™ìŠµ ì™„ë£Œ - ìµœì¢… ì†ì‹¤: {loss.item():.4f}")
                self.stats["total_training_epochs"] += 5
                
                # ëª¨ë¸ ì €ì¥
                self.save_model()
                
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í•™ìŠµ ì˜¤ë¥˜: {e}")
        finally:
            self.is_training = False
    
    def get_learning_status(self) -> Dict:
        """í•™ìŠµ ìƒíƒœ ë°˜í™˜"""
        return {
            "device": str(self.device),
            "learning_enabled": self.learning_enabled,
            "is_training": self.is_training,
            "stats": self.stats,
            "memory_stats": {
                "short_term_memory": len(self.memory.short_term_memory),
                "long_term_memory": len(self.memory.long_term_memory),
                "working_memory": len(self.memory.working_memory)
            },
            "model_info": {
                "total_parameters": sum(p.numel() for p in self.model.parameters()),
                "trainable_parameters": sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            }
        }
    
    def chat_interface(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("\nğŸ¤– AutoCI ì‹¤ì œ í•™ìŠµ AIì™€ ëŒ€í™”í•˜ê¸°")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥")
        print("=" * 50)
        
        while True:
            try:
                user_input = input("\nì‚¬ìš©ì: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                
                if user_input.lower() == 'status':
                    status = self.get_learning_status()
                    print(f"\nğŸ“Š í•™ìŠµ ìƒíƒœ:")
                    for key, value in status["stats"].items():
                        print(f"  {key}: {value}")
                    continue
                
                # AI ì‘ë‹µ ìƒì„±
                ai_response, confidence = self.generate_response(user_input)
                print(f"\nAutoCI: {ai_response}")
                print(f"(ì‹ ë¢°ë„: {confidence:.2f})")
                
                # ì‚¬ìš©ì í”¼ë“œë°± ë°›ê¸°
                feedback = input("í”¼ë“œë°± (ì¢‹ìŒ: +, ë‚˜ì¨: -, ê·¸ëƒ¥ Enter): ").strip()
                
                feedback_score = 0.0
                if feedback == '+':
                    feedback_score = 1.0
                elif feedback == '-':
                    feedback_score = -1.0
                
                # ëŒ€í™” ì²˜ë¦¬ ë° í•™ìŠµ
                result = self.process_conversation(user_input, ai_response, feedback_score)
                
                if result["learning_triggered"]:
                    print("ğŸ§  í•™ìŠµì´ íŠ¸ë¦¬ê±°ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self.save_model()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AutoCI ì‹¤ì œ í•™ìŠµ AI ì‹œìŠ¤í…œ ì‹œì‘")
    
    # í•™ìŠµ AI ì´ˆê¸°í™”
    learning_ai = RealLearningAutoCI()
    
    # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì‹œì‘
    learning_ai.chat_interface()

if __name__ == "__main__":
    main()