#!/usr/bin/env python3
"""
AutoCI ìƒìš©í™” ìˆ˜ì¤€ AI ëŒ€í™” ì—”ì§„
ChatGPT ìˆ˜ì¤€ì˜ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ ëŒ€í™” ëŠ¥ë ¥
"""

import os
import sys
import json
import sqlite3
import logging
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
import re
import threading
import time
import hashlib
from collections import defaultdict, deque
import pickle

# ê³ ê¸‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers import AutoTokenizer, AutoModel
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("âš ï¸  Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class CommercialDialogueEngine:
    """ìƒìš©í™” ìˆ˜ì¤€ì˜ AI ëŒ€í™” ì—”ì§„"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.base_path = Path(__file__).parent
        self.data_path = self.base_path / "commercial_data"
        self.data_path.mkdir(exist_ok=True)
        
        # ëŒ€í™” í’ˆì§ˆ íŒŒë¼ë¯¸í„°
        self.quality_threshold = 0.85  # ìƒìš©í™” í’ˆì§ˆ ê¸°ì¤€
        self.response_creativity = 0.7  # ì°½ì˜ì„± ìˆ˜ì¤€
        self.context_window = 10  # ë¬¸ë§¥ ì°½ í¬ê¸°
        
        # ëŒ€í™” ìŠ¤íƒ€ì¼ ë§¤íŠ¸ë¦­ìŠ¤
        self.dialogue_styles = {
            'professional': {'formality': 0.9, 'empathy': 0.6, 'clarity': 0.95},
            'friendly': {'formality': 0.4, 'empathy': 0.9, 'clarity': 0.8},
            'technical': {'formality': 0.8, 'empathy': 0.3, 'clarity': 1.0},
            'educational': {'formality': 0.7, 'empathy': 0.7, 'clarity': 0.9}
        }
        
        # ê³ ê¸‰ ëŒ€í™” ì»´í¬ë„ŒíŠ¸
        self.components = {
            'intent_analyzer': IntentAnalyzer(),
            'context_manager': ContextManager(),
            'response_generator': ResponseGenerator(),
            'quality_checker': QualityChecker(),
            'emotion_engine': EmotionEngine()
        }
        
        # ëŒ€í™” ë©”ëª¨ë¦¬ (ë‹¨ê¸°/ì¥ê¸°)
        self.short_term_memory = deque(maxlen=50)
        self.long_term_memory = ConversationMemory()
        
        # í•™ìŠµ ë°ì´í„°
        self.conversation_patterns = defaultdict(list)
        self.successful_responses = []
        
        # ì´ˆê¸°í™”
        self._initialize_models()
        self._load_conversation_data()
        
    def _initialize_models(self):
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        if HAS_TRANSFORMERS:
            try:
                # í•œêµ­ì–´ BERT ëª¨ë¸ ì‚¬ìš©
                self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
                self.model = AutoModel.from_pretrained("klue/bert-base")
                logger.info("í•œêµ­ì–´ BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                HAS_TRANSFORMERS = False
                logger.warning("BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œ ì‚¬ìš©")
        
        # ì»¤ìŠ¤í…€ ì‹ ê²½ë§
        self.dialogue_network = DialogueNeuralNetwork()
        
    def process_dialogue(self, user_input: str, context: Dict = None) -> Dict[str, Any]:
        """ìƒìš©í™” ìˆ˜ì¤€ì˜ ëŒ€í™” ì²˜ë¦¬"""
        start_time = time.time()
        
        # 1. ì…ë ¥ ì „ì²˜ë¦¬ ë° ë¶„ì„
        processed_input = self._preprocess_input(user_input)
        
        # 2. ì˜ë„ ë¶„ì„ (ë‹¤ì¤‘ ì˜ë„ ì§€ì›)
        intents = self.components['intent_analyzer'].analyze(processed_input)
        
        # 3. ë¬¸ë§¥ ê´€ë¦¬
        context_data = self.components['context_manager'].update(
            processed_input, 
            self.short_term_memory,
            context
        )
        
        # 4. ê°ì • ë¶„ì„
        emotion_data = self.components['emotion_engine'].analyze(
            processed_input,
            context_data
        )
        
        # 5. ì‘ë‹µ ìƒì„± (ë‹¤ì¤‘ í›„ë³´)
        response_candidates = self._generate_responses(
            processed_input,
            intents,
            context_data,
            emotion_data
        )
        
        # 6. í’ˆì§ˆ ê²€ì¦ ë° ìµœì  ì‘ë‹µ ì„ íƒ
        best_response = self._select_best_response(
            response_candidates,
            user_input,
            context_data
        )
        
        # 7. ëŒ€í™” ê¸°ë¡ ë° í•™ìŠµ
        self._record_conversation(
            user_input,
            best_response,
            intents,
            emotion_data,
            context_data
        )
        
        # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        response_time = time.time() - start_time
        
        return {
            'response': best_response['text'],
            'confidence': best_response['confidence'],
            'intents': intents,
            'emotion': emotion_data,
            'context': context_data,
            'response_time': response_time,
            'quality_score': best_response['quality_score']
        }
    
    def _preprocess_input(self, text: str) -> Dict[str, Any]:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ì œ
        cleaned_text = text.strip()
        
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', cleaned_text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # í† í°í™”
        tokens = []
        if HAS_TRANSFORMERS and hasattr(self, 'tokenizer'):
            encoded = self.tokenizer(cleaned_text, return_tensors='pt')
            tokens = self.tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])
        else:
            # ê°„ë‹¨í•œ í† í°í™”
            tokens = cleaned_text.split()
        
        # ì–¸ì–´ íŠ¹ì„± ë¶„ì„
        features = {
            'length': len(cleaned_text),
            'sentence_count': len(sentences),
            'avg_sentence_length': np.mean([len(s.split()) for s in sentences]) if sentences else 0,
            'question_marks': cleaned_text.count('?'),
            'exclamation_marks': cleaned_text.count('!'),
            'formal_markers': sum(1 for marker in ['ìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì„¸ìš”'] if marker in cleaned_text),
            'informal_markers': sum(1 for marker in ['ì•¼', 'ì–´', 'ì•„'] if marker in cleaned_text)
        }
        
        return {
            'original': text,
            'cleaned': cleaned_text,
            'sentences': sentences,
            'tokens': tokens,
            'features': features
        }
    
    def _generate_responses(self, processed_input: Dict, intents: List[Dict],
                          context: Dict, emotion: Dict) -> List[Dict]:
        """ë‹¤ì¤‘ ì‘ë‹µ í›„ë³´ ìƒì„±"""
        candidates = []
        
        # 1. í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ
        template_responses = self._generate_template_responses(intents)
        candidates.extend(template_responses)
        
        # 2. í•™ìŠµëœ íŒ¨í„´ ê¸°ë°˜ ì‘ë‹µ
        pattern_responses = self._generate_pattern_responses(
            processed_input['cleaned'],
            context
        )
        candidates.extend(pattern_responses)
        
        # 3. ìƒì„± ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ (if available)
        if self.dialogue_network:
            generated_responses = self._generate_neural_responses(
                processed_input,
                intents,
                context,
                emotion
            )
            candidates.extend(generated_responses)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì‘ë‹µ
        context_responses = self._generate_context_aware_responses(
            processed_input,
            context,
            emotion
        )
        candidates.extend(context_responses)
        
        # ê° í›„ë³´ì— ì ìˆ˜ ë¶€ì—¬
        for candidate in candidates:
            candidate['quality_score'] = self._evaluate_response_quality(
                candidate['text'],
                processed_input['cleaned'],
                context
            )
        
        return candidates
    
    def _generate_template_responses(self, intents: List[Dict]) -> List[Dict]:
        """í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        templates = {
            'greeting': [
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?",
                "ë°˜ê°‘ìŠµë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                "ì•ˆë…•í•˜ì„¸ìš”! C#ê³¼ Unity ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?"
            ],
            'question': [
                "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”! {topic}ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "{topic}ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ê²ƒ ê°™ë„¤ìš”. ìì„¸íˆ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ë„¤, {topic}ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ìë©´..."
            ],
            'error': [
                "{error_type} ì—ëŸ¬ëŠ” ì£¼ë¡œ {cause} ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. í•´ê²° ë°©ë²•ì€...",
                "ì´ëŸ° ì—ëŸ¬ë¥¼ í•´ê²°í•˜ë ¤ë©´ ë¨¼ì € {check_point}ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.",
                "{error_type}ê°€ ë°œìƒí–ˆêµ°ìš”. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼í•´ë³´ì„¸ìš”:"
            ],
            'request': [
                "ë„¤, {task}ë¥¼ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¨¼ì €...",
                "{task}ë¥¼ ìœ„í•œ ì½”ë“œë¥¼ ì‘ì„±í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ì•Œê² ìŠµë‹ˆë‹¤. {task}ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
            ]
        }
        
        responses = []
        for intent in intents[:2]:  # ìƒìœ„ 2ê°œ ì˜ë„
            intent_type = intent['type']
            if intent_type in templates:
                for template in templates[intent_type][:2]:
                    response_text = template.format(**intent.get('params', {}))
                    responses.append({
                        'text': response_text,
                        'type': 'template',
                        'intent': intent_type,
                        'confidence': intent['confidence'] * 0.8
                    })
        
        return responses
    
    def _generate_pattern_responses(self, user_input: str, context: Dict) -> List[Dict]:
        """í•™ìŠµëœ íŒ¨í„´ ê¸°ë°˜ ì‘ë‹µ"""
        responses = []
        
        # ìœ ì‚¬í•œ ëŒ€í™” íŒ¨í„´ ê²€ìƒ‰
        similar_patterns = self._find_similar_patterns(user_input)
        
        for pattern in similar_patterns[:3]:
            if pattern['success_rate'] > 0.7:
                responses.append({
                    'text': pattern['response'],
                    'type': 'pattern',
                    'confidence': pattern['similarity'] * pattern['success_rate'],
                    'pattern_id': pattern['id']
                })
        
        return responses
    
    def _generate_neural_responses(self, processed_input: Dict, intents: List[Dict],
                                  context: Dict, emotion: Dict) -> List[Dict]:
        """ì‹ ê²½ë§ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        responses = []
        
        try:
            # ì…ë ¥ ì¸ì½”ë”©
            input_vector = self._encode_input(processed_input, intents, context, emotion)
            
            # ì‹ ê²½ë§ í†µê³¼
            with torch.no_grad():
                output = self.dialogue_network(input_vector)
                
            # ë””ì½”ë”©
            response_text = self._decode_output(output, context)
            
            responses.append({
                'text': response_text,
                'type': 'neural',
                'confidence': float(torch.sigmoid(output.max()).item())
            })
            
        except Exception as e:
            logger.error(f"ì‹ ê²½ë§ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return responses
    
    def _generate_context_aware_responses(self, processed_input: Dict,
                                        context: Dict, emotion: Dict) -> List[Dict]:
        """ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì‘ë‹µ ìƒì„±"""
        responses = []
        
        # ì´ì „ ëŒ€í™” ë¶„ì„
        recent_topics = context.get('recent_topics', [])
        conversation_flow = context.get('flow', 'normal')
        
        # ëŒ€í™” íë¦„ì— ë§ëŠ” ì‘ë‹µ
        if conversation_flow == 'problem_solving':
            responses.append({
                'text': "ì´ì „ì— ë§ì”€í•˜ì‹  ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•´ë³¼ê¹Œìš”?",
                'type': 'contextual',
                'confidence': 0.7
            })
        elif conversation_flow == 'learning':
            responses.append({
                'text': "ì¢‹ì•„ìš”! ì´ì œ ë” ê¹Šì´ ìˆëŠ” ë‚´ìš©ì„ ì•Œì•„ë³¼ê¹Œìš”?",
                'type': 'contextual',
                'confidence': 0.7
            })
        
        # ê°ì • ê¸°ë°˜ ì‘ë‹µ
        if emotion.get('type') == 'frustrated':
            responses.append({
                'text': "ì–´ë ¤ìš°ì…¨êµ°ìš”. ì²œì²œíˆ í•˜ë‚˜ì”© í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì €...",
                'type': 'empathetic',
                'confidence': 0.8
            })
        
        return responses
    
    def _select_best_response(self, candidates: List[Dict], 
                            user_input: str, context: Dict) -> Dict:
        """ìµœì  ì‘ë‹µ ì„ íƒ"""
        if not candidates:
            return {
                'text': "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”?",
                'confidence': 0.3,
                'quality_score': 0.5
            }
        
        # í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬
        candidates.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        # ìƒìœ„ í›„ë³´ ì¤‘ ë‹¤ì–‘ì„± ê³ ë ¤í•˜ì—¬ ì„ íƒ
        best_candidate = candidates[0]
        
        # í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸
        if best_candidate['quality_score'] < self.quality_threshold:
            # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì•ˆì „í•œ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
            best_candidate = self._generate_safe_response(user_input, context)
        
        return best_candidate
    
    def _evaluate_response_quality(self, response: str, user_input: str, 
                                 context: Dict) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        scores = []
        
        # 1. ê´€ë ¨ì„± ì ìˆ˜
        relevance = self._calculate_relevance(response, user_input)
        scores.append(relevance)
        
        # 2. ì¼ê´€ì„± ì ìˆ˜
        consistency = self._calculate_consistency(response, context)
        scores.append(consistency)
        
        # 3. ì™„ì„±ë„ ì ìˆ˜
        completeness = self._calculate_completeness(response)
        scores.append(completeness)
        
        # 4. ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜
        naturalness = self._calculate_naturalness(response)
        scores.append(naturalness)
        
        # 5. ìœ ìš©ì„± ì ìˆ˜
        usefulness = self._calculate_usefulness(response, user_input)
        scores.append(usefulness)
        
        # ê°€ì¤‘ í‰ê· 
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]
        quality_score = sum(s * w for s, w in zip(scores, weights))
        
        return quality_score
    
    def _calculate_relevance(self, response: str, user_input: str) -> float:
        """ê´€ë ¨ì„± ê³„ì‚°"""
        # í‚¤ì›Œë“œ ì¤‘ë³µë„
        input_keywords = set(self._extract_keywords(user_input))
        response_keywords = set(self._extract_keywords(response))
        
        if not input_keywords:
            return 0.5
        
        overlap = len(input_keywords & response_keywords)
        relevance = overlap / len(input_keywords)
        
        return min(1.0, relevance * 1.5)  # ë¶€ìŠ¤íŒ…
    
    def _calculate_consistency(self, response: str, context: Dict) -> float:
        """ì¼ê´€ì„± ê³„ì‚°"""
        # ì´ì „ ì‘ë‹µë“¤ê³¼ì˜ ì¼ê´€ì„± ì²´í¬
        previous_responses = context.get('previous_responses', [])
        
        if not previous_responses:
            return 0.8
        
        # ëª¨ìˆœ ì²´í¬
        contradictions = 0
        for prev in previous_responses[-3:]:
            if self._has_contradiction(response, prev):
                contradictions += 1
        
        consistency = 1.0 - (contradictions * 0.3)
        return max(0.0, consistency)
    
    def _calculate_completeness(self, response: str) -> float:
        """ì™„ì„±ë„ ê³„ì‚°"""
        # ë¬¸ì¥ êµ¬ì¡° ì™„ì„±ë„
        sentences = re.split(r'[.!?]+', response)
        valid_sentences = 0
        
        for sentence in sentences:
            if sentence.strip() and len(sentence.split()) >= 3:
                valid_sentences += 1
        
        if not sentences:
            return 0.0
        
        return valid_sentences / len(sentences)
    
    def _calculate_naturalness(self, response: str) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í‰ê°€
        naturalness = 1.0
        
        # ë°˜ë³µ ì²´í¬
        words = response.split()
        if len(words) > 3:
            unique_ratio = len(set(words)) / len(words)
            naturalness *= unique_ratio
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„í¬
        sentences = re.split(r'[.!?]+', response)
        lengths = [len(s.split()) for s in sentences if s.strip()]
        if lengths:
            avg_length = np.mean(lengths)
            if avg_length < 5 or avg_length > 20:
                naturalness *= 0.8
        
        return naturalness
    
    def _calculate_usefulness(self, response: str, user_input: str) -> float:
        """ìœ ìš©ì„± ê³„ì‚°"""
        usefulness = 0.5
        
        # êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
        if any(pattern in response for pattern in ['ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤', 'ë°©ë²•ì€', 'í•´ê²°í•˜ë ¤ë©´']):
            usefulness += 0.2
        
        # ì½”ë“œ í¬í•¨ ì—¬ë¶€ (ê¸°ìˆ ì  ì§ˆë¬¸ì¸ ê²½ìš°)
        if 'ì½”ë“œ' in user_input or 'code' in user_input.lower():
            if '```' in response or 'class' in response or 'public' in response:
                usefulness += 0.3
        
        # ë‹¨ê³„ë³„ ì„¤ëª… ì—¬ë¶€
        if any(marker in response for marker in ['ì²«ì§¸', 'ë‘˜ì§¸', '1.', '2.', 'ë¨¼ì €']):
            usefulness += 0.2
        
        return min(1.0, usefulness)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼',
                    'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or'}
        
        words = re.findall(r'\w+', text.lower())
        keywords = [w for w in words if w not in stopwords and len(w) > 1]
        
        return keywords
    
    def _has_contradiction(self, text1: str, text2: str) -> bool:
        """ëª¨ìˆœ ê²€ì‚¬"""
        # ê°„ë‹¨í•œ ëª¨ìˆœ íŒ¨í„´
        contradictions = [
            ('í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤', 'í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'),
            ('ê°€ëŠ¥í•©ë‹ˆë‹¤', 'ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤'),
            ('ë§ìŠµë‹ˆë‹¤', 'í‹€ë ¸ìŠµë‹ˆë‹¤'),
            ('ì˜ˆ', 'ì•„ë‹ˆì˜¤')
        ]
        
        for pos, neg in contradictions:
            if (pos in text1 and neg in text2) or (neg in text1 and pos in text2):
                return True
        
        return False
    
    def _find_similar_patterns(self, user_input: str) -> List[Dict]:
        """ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰"""
        similar_patterns = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
        conn = sqlite3.connect(str(self.data_path / "dialogue_patterns.db"))
        cursor = conn.cursor()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        keywords = self._extract_keywords(user_input)
        
        for keyword in keywords[:5]:
            cursor.execute('''
                SELECT id, input_pattern, response, success_rate
                FROM dialogue_patterns
                WHERE input_pattern LIKE ?
                ORDER BY success_rate DESC
                LIMIT 5
            ''', (f'%{keyword}%',))
            
            for row in cursor.fetchall():
                similarity = self._calculate_similarity(user_input, row[1])
                similar_patterns.append({
                    'id': row[0],
                    'input': row[1],
                    'response': row[2],
                    'success_rate': row[3],
                    'similarity': similarity
                })
        
        conn.close()
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_patterns.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similar_patterns[:5]
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ìì¹´ë“œ ìœ ì‚¬ë„
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def _encode_input(self, processed_input: Dict, intents: List[Dict],
                     context: Dict, emotion: Dict) -> torch.Tensor:
        """ì…ë ¥ ì¸ì½”ë”©"""
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        features = []
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§•
        features.extend([
            processed_input['features']['length'] / 200,
            processed_input['features']['sentence_count'] / 5,
            processed_input['features']['question_marks'] / 3,
        ])
        
        # ì˜ë„ íŠ¹ì§•
        intent_vector = [0] * 10  # 10ê°œ ì˜ë„ ì¹´í…Œê³ ë¦¬
        for intent in intents[:3]:
            intent_idx = hash(intent['type']) % 10
            intent_vector[intent_idx] = intent['confidence']
        features.extend(intent_vector)
        
        # ê°ì • íŠ¹ì§•
        emotion_vector = [0] * 5  # 5ê°œ ê°ì • ì¹´í…Œê³ ë¦¬
        emotion_idx = hash(emotion.get('type', 'neutral')) % 5
        emotion_vector[emotion_idx] = emotion.get('intensity', 0.5)
        features.extend(emotion_vector)
        
        # í…ì„œ ë³€í™˜
        return torch.tensor(features, dtype=torch.float32)
    
    def _decode_output(self, output: torch.Tensor, context: Dict) -> str:
        """ì¶œë ¥ ë””ì½”ë”©"""
        # ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë””ì½”ë”©
        output_values = output.squeeze().tolist()
        
        # ì‘ë‹µ íƒ€ì… ê²°ì •
        response_type_idx = np.argmax(output_values[:5])
        response_types = ['informative', 'helpful', 'clarifying', 'encouraging', 'technical']
        response_type = response_types[response_type_idx]
        
        # ì‘ë‹µ í…œí”Œë¦¿
        templates = {
            'informative': "ì œê°€ ì•Œê¸°ë¡œëŠ” {topic}ì— ëŒ€í•´ {detail} ì…ë‹ˆë‹¤.",
            'helpful': "ë„ì›€ì´ ë˜ë„ë¡ {action}ë¥¼ í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            'clarifying': "í˜¹ì‹œ {clarification}ë¥¼ ì›í•˜ì‹œëŠ” ê±´ê°€ìš”?",
            'encouraging': "ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤! {suggestion}ë¥¼ í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.",
            'technical': "ê¸°ìˆ ì ìœ¼ë¡œ ì„¤ëª…ë“œë¦¬ë©´ {technical_detail} ì…ë‹ˆë‹¤."
        }
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ì±„ìš°ê¸°
        template = templates.get(response_type, "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤.")
        params = self._extract_template_params(template, context)
        
        return template.format(**params)
    
    def _extract_template_params(self, template: str, context: Dict) -> Dict[str, str]:
        """í…œí”Œë¦¿ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        params = {}
        
        # í…œí”Œë¦¿ì—ì„œ í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì°¾ê¸°
        placeholders = re.findall(r'{(\w+)}', template)
        
        for placeholder in placeholders:
            if placeholder == 'topic':
                params[placeholder] = context.get('current_topic', 'C# í”„ë¡œê·¸ë˜ë°')
            elif placeholder == 'detail':
                params[placeholder] = "ì¤‘ìš”í•œ ê°œë…"
            elif placeholder == 'action':
                params[placeholder] = "ì½”ë“œ ì˜ˆì œë¥¼ ì‘ì„±"
            elif placeholder == 'clarification':
                params[placeholder] = "ì´ ë¶€ë¶„"
            elif placeholder == 'suggestion':
                params[placeholder] = "ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼"
            elif placeholder == 'technical_detail':
                params[placeholder] = "ë‹¤ìŒê³¼ ê°™ì€ ì›ë¦¬ë¡œ ì‘ë™í•©ë‹ˆë‹¤"
        
        return params
    
    def _generate_safe_response(self, user_input: str, context: Dict) -> Dict:
        """ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        safe_responses = [
            "í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
            "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ê³  ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ì¢‹ì€ í¬ì¸íŠ¸ë¥¼ ì§šì–´ì£¼ì…¨ë„¤ìš”. ì œê°€ ì´í•´í•œ ë°”ë¡œëŠ”..."
        ]
        
        import random
        return {
            'text': random.choice(safe_responses),
            'confidence': 0.6,
            'quality_score': 0.7
        }
    
    def _record_conversation(self, user_input: str, response: Dict,
                           intents: List[Dict], emotion: Dict, context: Dict):
        """ëŒ€í™” ê¸°ë¡ ë° í•™ìŠµ"""
        # ë‹¨ê¸° ë©”ëª¨ë¦¬ì— ì¶”ê°€
        self.short_term_memory.append({
            'timestamp': datetime.now(),
            'user_input': user_input,
            'response': response['text'],
            'intents': intents,
            'emotion': emotion,
            'quality_score': response['quality_score']
        })
        
        # ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥
        self.long_term_memory.store({
            'user_input': user_input,
            'response': response,
            'context': context,
            'metadata': {
                'intents': intents,
                'emotion': emotion,
                'timestamp': datetime.now()
            }
        })
        
        # íŒ¨í„´ í•™ìŠµ
        if response['quality_score'] > 0.8:
            self._learn_pattern(user_input, response['text'], intents)
    
    def _learn_pattern(self, user_input: str, response: str, intents: List[Dict]):
        """ì„±ê³µì ì¸ íŒ¨í„´ í•™ìŠµ"""
        pattern_key = intents[0]['type'] if intents else 'general'
        
        self.conversation_patterns[pattern_key].append({
            'input': user_input,
            'response': response,
            'timestamp': datetime.now()
        })
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        if len(self.conversation_patterns[pattern_key]) % 10 == 0:
            self._save_patterns_to_db()
    
    def _save_patterns_to_db(self):
        """íŒ¨í„´ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(str(self.data_path / "dialogue_patterns.db"))
        cursor = conn.cursor()
        
        # í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS dialogue_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                input_pattern TEXT,
                response TEXT,
                pattern_type TEXT,
                success_rate REAL DEFAULT 0.5,
                usage_count INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # íŒ¨í„´ ì €ì¥
        for pattern_type, patterns in self.conversation_patterns.items():
            for pattern in patterns[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                cursor.execute('''
                    INSERT INTO dialogue_patterns 
                    (input_pattern, response, pattern_type)
                    VALUES (?, ?, ?)
                ''', (pattern['input'], pattern['response'], pattern_type))
        
        conn.commit()
        conn.close()
    
    def _load_conversation_data(self):
        """ëŒ€í™” ë°ì´í„° ë¡œë“œ"""
        # ê¸°ì¡´ íŒ¨í„´ ë¡œë“œ
        db_path = self.data_path / "dialogue_patterns.db"
        if db_path.exists():
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT pattern_type, input_pattern, response, success_rate
                FROM dialogue_patterns
                WHERE success_rate > 0.7
                ORDER BY success_rate DESC
                LIMIT 100
            ''')
            
            for row in cursor.fetchall():
                pattern_type, input_pattern, response, success_rate = row
                self.successful_responses.append({
                    'type': pattern_type,
                    'input': input_pattern,
                    'response': response,
                    'success_rate': success_rate
                })
            
            conn.close()


class IntentAnalyzer:
    """ì˜ë„ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.intent_patterns = {
            'greeting': ['ì•ˆë…•', 'í•˜ì´', 'hello', 'ë°˜ê°€ì›Œ'],
            'question': ['?', 'ë­', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””', 'ë¬´ì—‡'],
            'request': ['í•´ì¤˜', 'ë§Œë“¤ì–´', 'ë³´ì—¬ì¤˜', 'ì•Œë ¤ì¤˜', 'ì„¤ëª…í•´'],
            'error': ['ì—ëŸ¬', 'ì˜¤ë¥˜', 'error', 'exception', 'ì•ˆë¼', 'ì•ˆë¨'],
            'feedback': ['ê³ ë§ˆì›Œ', 'ê°ì‚¬', 'ì¢‹ì•„', 'ë‚˜ë¹ ', 'ë³„ë¡œ', 'ìµœê³ '],
            'learning': ['ë°°ìš°ê³ ', 'ê³µë¶€', 'ì•Œê³ ì‹¶', 'ê¶ê¸ˆ'],
            'technical': ['ì½”ë“œ', 'code', 'í•¨ìˆ˜', 'class', 'method', 'async'],
            'clarification': ['ë¬´ìŠ¨ ë§', 'ì´í•´ê°€', 'ë‹¤ì‹œ', 'ì„¤ëª…'],
            'confirmation': ['ë§ì•„', 'ê·¸ë˜', 'ë„¤', 'ì˜ˆ', 'í™•ì¸'],
            'completion': ['ì™„ë£Œ', 'ë', 'ë‹¤í–ˆ', 'ë§ˆì¹¨']
        }
    
    def analyze(self, processed_input: Dict) -> List[Dict]:
        """ë‹¤ì¤‘ ì˜ë„ ë¶„ì„"""
        text = processed_input['cleaned'].lower()
        intents = []
        
        # íŒ¨í„´ ë§¤ì¹­
        for intent_type, patterns in self.intent_patterns.items():
            confidence = 0.0
            matches = 0
            
            for pattern in patterns:
                if pattern in text:
                    matches += 1
            
            if matches > 0:
                confidence = min(1.0, matches / len(patterns) * 2)
                intents.append({
                    'type': intent_type,
                    'confidence': confidence,
                    'params': self._extract_params(text, intent_type)
                })
        
        # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        intents.sort(key=lambda x: x['confidence'], reverse=True)
        
        # ì˜ë„ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if not intents:
            intents.append({
                'type': 'general',
                'confidence': 0.5,
                'params': {}
            })
        
        return intents
    
    def _extract_params(self, text: str, intent_type: str) -> Dict:
        """ì˜ë„ë³„ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        params = {}
        
        if intent_type == 'error':
            # ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ
            error_types = ['NullReferenceException', 'IndexOutOfRange', 
                          'ArgumentException', 'InvalidOperation']
            for error in error_types:
                if error.lower() in text.lower():
                    params['error_type'] = error
                    break
        
        elif intent_type == 'question':
            # ì£¼ì œ ì¶”ì¶œ
            if 'unity' in text:
                params['topic'] = 'Unity'
            elif 'c#' in text or 'csharp' in text:
                params['topic'] = 'C#'
        
        elif intent_type == 'request':
            # ì‘ì—… íƒ€ì… ì¶”ì¶œ
            if 'ì½”ë“œ' in text:
                params['task'] = 'ì½”ë“œ ì‘ì„±'
            elif 'ì„¤ëª…' in text:
                params['task'] = 'ì„¤ëª…'
        
        return params


class ContextManager:
    """ë¬¸ë§¥ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.context_window = 10
        self.topic_tracker = TopicTracker()
        
    def update(self, processed_input: Dict, memory: deque, 
               external_context: Dict = None) -> Dict:
        """ë¬¸ë§¥ ì—…ë°ì´íŠ¸"""
        context = {
            'conversation_length': len(memory),
            'current_topic': self.topic_tracker.get_current_topic(processed_input),
            'recent_topics': self.topic_tracker.get_recent_topics(),
            'flow': self._analyze_conversation_flow(memory),
            'user_state': self._analyze_user_state(memory),
            'previous_responses': [m['response'] for m in list(memory)[-3:]]
        }
        
        if external_context:
            context.update(external_context)
        
        return context
    
    def _analyze_conversation_flow(self, memory: deque) -> str:
        """ëŒ€í™” íë¦„ ë¶„ì„"""
        if len(memory) < 2:
            return 'starting'
        
        recent_intents = []
        for m in list(memory)[-5:]:
            if 'intents' in m and m['intents']:
                recent_intents.append(m['intents'][0]['type'])
        
        # íë¦„ íŒ¨í„´ ì¸ì‹
        if recent_intents.count('error') >= 2:
            return 'problem_solving'
        elif recent_intents.count('question') >= 3:
            return 'learning'
        elif recent_intents.count('request') >= 2:
            return 'task_oriented'
        
        return 'normal'
    
    def _analyze_user_state(self, memory: deque) -> Dict:
        """ì‚¬ìš©ì ìƒíƒœ ë¶„ì„"""
        if not memory:
            return {'engagement': 'neutral', 'expertise': 'unknown'}
        
        recent_emotions = []
        for m in list(memory)[-5:]:
            if 'emotion' in m:
                recent_emotions.append(m['emotion'].get('type', 'neutral'))
        
        # ì°¸ì—¬ë„ ê³„ì‚°
        if recent_emotions.count('excited') > 1:
            engagement = 'high'
        elif recent_emotions.count('frustrated') > 1:
            engagement = 'low'
        else:
            engagement = 'medium'
        
        return {
            'engagement': engagement,
            'expertise': self._estimate_expertise(memory)
        }
    
    def _estimate_expertise(self, memory: deque) -> str:
        """ì „ë¬¸ì„± ìˆ˜ì¤€ ì¶”ì •"""
        technical_terms = 0
        total_inputs = len(memory)
        
        if total_inputs == 0:
            return 'unknown'
        
        for m in memory:
            input_text = m.get('user_input', '').lower()
            if any(term in input_text for term in 
                   ['async', 'delegate', 'interface', 'abstract', 'generic']):
                technical_terms += 1
        
        ratio = technical_terms / total_inputs
        
        if ratio > 0.3:
            return 'expert'
        elif ratio > 0.1:
            return 'intermediate'
        else:
            return 'beginner'


class ResponseGenerator:
    """ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.style_adapter = StyleAdapter()
        
    def generate(self, intent: str, context: Dict, style: str = 'professional') -> str:
        """ìŠ¤íƒ€ì¼ì— ë§ëŠ” ì‘ë‹µ ìƒì„±"""
        base_response = self._generate_base_response(intent, context)
        styled_response = self.style_adapter.apply_style(base_response, style)
        
        return styled_response
    
    def _generate_base_response(self, intent: str, context: Dict) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        # ì˜ë„ë³„ ê¸°ë³¸ ì‘ë‹µ
        responses = {
            'greeting': "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            'question': "ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            'error': "ì—ëŸ¬ í•´ê²°ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            'request': "ë„¤, ë°”ë¡œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        }
        
        return responses.get(intent, "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤.")


class QualityChecker:
    """í’ˆì§ˆ ê²€ì¦ê¸°"""
    
    def check(self, response: str, context: Dict) -> Dict[str, float]:
        """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦"""
        return {
            'grammar': self._check_grammar(response),
            'coherence': self._check_coherence(response, context),
            'completeness': self._check_completeness(response),
            'appropriateness': self._check_appropriateness(response, context)
        }
    
    def _check_grammar(self, response: str) -> float:
        """ë¬¸ë²• ê²€ì‚¬"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê²€ì‚¬
        score = 1.0
        
        # ë¬¸ì¥ ì¢…ê²° í™•ì¸
        if not response.strip().endswith(('.', '!', '?', 'ìš”')):
            score -= 0.2
        
        # ë§ì¶¤ë²• íŒ¨í„´
        common_errors = [
            ('ë¬', 'ë'),
            ('í–‡', 'í–ˆ'),
            ('ì—¤', 'ì—ˆ')
        ]
        
        for error, _ in common_errors:
            if error in response:
                score -= 0.1
        
        return max(0.0, score)
    
    def _check_coherence(self, response: str, context: Dict) -> float:
        """ì¼ê´€ì„± ê²€ì‚¬"""
        # ì´ì „ ì‘ë‹µê³¼ì˜ ì¼ê´€ì„±
        previous = context.get('previous_responses', [])
        
        if not previous:
            return 0.9
        
        # ì£¼ì œ ì¼ê´€ì„±
        current_topic = context.get('current_topic', '')
        if current_topic and current_topic.lower() not in response.lower():
            return 0.7
        
        return 0.9
    
    def _check_completeness(self, response: str) -> float:
        """ì™„ì„±ë„ ê²€ì‚¬"""
        # ì‘ë‹µ ê¸¸ì´
        if len(response) < 10:
            return 0.5
        elif len(response) > 500:
            return 0.8
        
        return 1.0
    
    def _check_appropriateness(self, response: str, context: Dict) -> float:
        """ì ì ˆì„± ê²€ì‚¬"""
        user_state = context.get('user_state', {})
        
        # ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ”ì§€
        if user_state.get('expertise') == 'beginner':
            # ë„ˆë¬´ ê¸°ìˆ ì ì¸ ìš©ì–´ê°€ ë§ìœ¼ë©´ ê°ì 
            technical_terms = ['polymorphism', 'delegate', 'lambda', 'LINQ']
            term_count = sum(1 for term in technical_terms if term in response)
            if term_count > 2:
                return 0.7
        
        return 0.9


class EmotionEngine:
    """ê°ì • ë¶„ì„ ì—”ì§„"""
    
    def analyze(self, processed_input: Dict, context: Dict) -> Dict:
        """ê°ì • ë¶„ì„"""
        text = processed_input['cleaned']
        
        emotion_indicators = {
            'happy': ['ì¢‹ì•„', 'ê¸°ë»', 'ê°ì‚¬', 'ìµœê³ ', 'ğŸ˜Š', 'ğŸ˜„'],
            'sad': ['ìŠ¬í¼', 'ì•„ì‰¬ì›Œ', 'í˜ë“¤ì–´', 'ğŸ˜¢', 'ğŸ˜­'],
            'angry': ['í™”ë‚˜', 'ì§œì¦', 'ì‹«ì–´', 'ğŸ˜¡', 'ğŸ˜ '],
            'frustrated': ['ë‹µë‹µ', 'ì–´ë ¤ì›Œ', 'ëª¨ë¥´ê² ', 'ì•ˆë¼', 'ğŸ˜¤'],
            'excited': ['ì‹ ë‚˜', 'ëŒ€ë°•', 'ì™€', 'ê¸°ëŒ€', 'ğŸ‰'],
            'confused': ['í—·ê°ˆë ¤', 'ì´í•´ê°€', 'ë­”ì§€', 'ğŸ¤”'],
            'neutral': []
        }
        
        detected_emotion = 'neutral'
        max_score = 0
        
        for emotion, indicators in emotion_indicators.items():
            score = sum(1 for ind in indicators if ind in text)
            if score > max_score:
                max_score = score
                detected_emotion = emotion
        
        # ê°ì • ê°•ë„ ê³„ì‚°
        intensity = min(1.0, max_score / 3)
        
        return {
            'type': detected_emotion,
            'intensity': intensity,
            'indicators': max_score
        }


class TopicTracker:
    """ì£¼ì œ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.recent_topics = deque(maxlen=5)
        self.topic_keywords = {
            'unity': ['unity', 'ìœ ë‹ˆí‹°', 'gameobject', 'transform', 'component'],
            'csharp': ['c#', 'class', 'method', 'variable', 'namespace'],
            'debugging': ['ì—ëŸ¬', 'error', 'ë””ë²„ê¹…', 'exception', 'ì˜¤ë¥˜'],
            'architecture': ['êµ¬ì¡°', 'pattern', 'ì„¤ê³„', 'architecture', 'íŒ¨í„´'],
            'performance': ['ì„±ëŠ¥', 'performance', 'ìµœì í™”', 'optimize', 'ì†ë„']
        }
    
    def get_current_topic(self, processed_input: Dict) -> str:
        """í˜„ì¬ ì£¼ì œ ì¶”ì¶œ"""
        text = processed_input['cleaned'].lower()
        
        topic_scores = {}
        for topic, keywords in self.topic_keywords.items():
            score = sum(1 for kw in keywords if kw in text)
            if score > 0:
                topic_scores[topic] = score
        
        if topic_scores:
            current_topic = max(topic_scores.items(), key=lambda x: x[1])[0]
            self.recent_topics.append(current_topic)
            return current_topic
        
        return 'general'
    
    def get_recent_topics(self) -> List[str]:
        """ìµœê·¼ ì£¼ì œ ëª©ë¡"""
        return list(self.recent_topics)


class StyleAdapter:
    """ìŠ¤íƒ€ì¼ ì ì‘ê¸°"""
    
    def apply_style(self, text: str, style: str) -> str:
        """í…ìŠ¤íŠ¸ì— ìŠ¤íƒ€ì¼ ì ìš©"""
        if style == 'professional':
            return self._make_professional(text)
        elif style == 'friendly':
            return self._make_friendly(text)
        elif style == 'technical':
            return self._make_technical(text)
        
        return text
    
    def _make_professional(self, text: str) -> str:
        """ì „ë¬¸ì ì¸ ìŠ¤íƒ€ì¼"""
        # ì¡´ëŒ“ë§ ë³€í™˜
        replacements = [
            ('í•´', 'í•˜ì„¸ìš”'),
            ('ë¼', 'ë©ë‹ˆë‹¤'),
            ('ì•¼', 'ì…”ì•¼ í•©ë‹ˆë‹¤')
        ]
        
        for old, new in replacements:
            text = text.replace(old, new)
        
        return text
    
    def _make_friendly(self, text: str) -> str:
        """ì¹œê·¼í•œ ìŠ¤íƒ€ì¼"""
        # ì´ëª¨í‹°ì½˜ ì¶”ê°€
        if text.endswith('!'):
            text += ' ğŸ˜Š'
        
        return text
    
    def _make_technical(self, text: str) -> str:
        """ê¸°ìˆ ì ì¸ ìŠ¤íƒ€ì¼"""
        # ì „ë¬¸ ìš©ì–´ ê°•ì¡°
        return text


class ConversationMemory:
    """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬"""
    
    def __init__(self, db_path: str = "conversation_memory.db"):
        self.db_path = Path(db_path)
        self._init_db()
    
    def _init_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                user_input TEXT,
                response TEXT,
                context TEXT,
                metadata TEXT,
                quality_score REAL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def store(self, conversation: Dict):
        """ëŒ€í™” ì €ì¥"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO conversations 
            (user_input, response, context, metadata, quality_score)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            conversation['user_input'],
            json.dumps(conversation['response']),
            json.dumps(conversation.get('context', {})),
            json.dumps(conversation.get('metadata', {})),
            conversation['response'].get('quality_score', 0.5)
        ))
        
        conn.commit()
        conn.close()


class DialogueNeuralNetwork(nn.Module):
    """ëŒ€í™” ì‹ ê²½ë§"""
    
    def __init__(self, input_size: int = 18, hidden_size: int = 128, output_size: int = 50):
        super().__init__()
        
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        
        self.dropout = nn.Dropout(0.2)
        self.activation = nn.ReLU()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        x = self.activation(self.fc1(x))
        x = self.dropout(x)
        x = self.activation(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    print("ğŸš€ ìƒìš©í™” ìˆ˜ì¤€ AI ëŒ€í™” ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    engine = CommercialDialogueEngine()
    
    test_inputs = [
        "ì•ˆë…•í•˜ì„¸ìš”! Unity ê°œë°œì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ”ë° ë„ì›€ì´ í•„ìš”í•´ìš”.",
        "GameObjectê°€ nullì¸ë° ì™œ NullReferenceExceptionì´ ë°œìƒí•˜ë‚˜ìš”?",
        "ì½”ë£¨í‹´ê³¼ async/await ì¤‘ ë­˜ ì¨ì•¼ í• ê¹Œìš”?",
        "ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤! ë•ë¶„ì— ë¬¸ì œë¥¼ í•´ê²°í–ˆì–´ìš”!"
    ]
    
    for user_input in test_inputs:
        print(f"\nğŸ‘¤ User: {user_input}")
        
        result = engine.process_dialogue(user_input)
        
        print(f"ğŸ¤– AI: {result['response']}")
        print(f"   í’ˆì§ˆ: {result['quality_score']:.2f} | "
              f"ì‹ ë¢°ë„: {result['confidence']:.2f} | "
              f"ì‘ë‹µì‹œê°„: {result['response_time']:.3f}ì´ˆ")