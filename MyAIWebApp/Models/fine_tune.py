#!/usr/bin/env python3
"""
Code Llama Fine-tuning Script
C# ì „ë¬¸ê°€ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import torch
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np
from dataclasses import dataclass, field

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback,
    TrainerCallback
)
from datasets import Dataset, load_dataset
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import wandb
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fine_tuning.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    model_path: str = "../../CodeLlama-7b-Instruct-hf"
    output_dir: str = "./fine_tuned_model"
    data_path: str = "../../expert_training_data/training_dataset.json"
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-5
    warmup_steps: int = 100
    logging_steps: int = 10
    save_steps: int = 500
    eval_steps: int = 100
    save_total_limit: int = 3
    
    # LoRA ì„¤ì •
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    lora_target_modules: List[str] = field(default_factory=lambda: ["q_proj", "v_proj"])
    
    # ê¸°íƒ€ ì„¤ì •
    max_length: int = 2048
    use_8bit: bool = True
    use_wandb: bool = False
    seed: int = 42

class CSharpDataset:
    """C# í•™ìŠµ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 2048):
        self.data_path = Path(data_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ"""
        if not self.data_path.exists():
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"âœ… {len(data)}ê°œì˜ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return data
    
    def prepare_dataset(self) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        formatted_data = []
        
        for item in tqdm(self.data, desc="ë°ì´í„° ì „ì²˜ë¦¬"):
            # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
            if "instruction" in item and "output" in item:
                text = self._format_prompt(
                    instruction=item["instruction"],
                    input_text=item.get("input", ""),
                    output=item["output"]
                )
            elif "code" in item:
                # ì½”ë“œ ì„¤ëª… ìƒì„±
                text = self._format_code_explanation(item["code"], item.get("patterns", []))
            else:
                continue
            
            # í† í°í™”
            tokenized = self.tokenizer(
                text,
                truncation=True,
                max_length=self.max_length,
                padding=False
            )
            
            formatted_data.append({
                "input_ids": tokenized["input_ids"],
                "attention_mask": tokenized["attention_mask"],
                "labels": tokenized["input_ids"].copy()
            })
        
        return Dataset.from_list(formatted_data)
    
    def _format_prompt(self, instruction: str, input_text: str, output: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        if input_text:
            return f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
        else:
            return f"""### Instruction:
{instruction}

### Response:
{output}"""
    
    def _format_code_explanation(self, code: str, patterns: List[str]) -> str:
        """ì½”ë“œ ì„¤ëª… í¬ë§·íŒ…"""
        patterns_str = ", ".join(patterns) if patterns else "ì¼ë°˜ì ì¸ C# ì½”ë“œ"
        
        return f"""### Instruction:
ë‹¤ìŒ C# ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.

### Input:
```csharp
{code[:1000]}  # ì²˜ìŒ 1000ìë§Œ
```

### Response:
ì´ ì½”ë“œëŠ” {patterns_str} íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” C# ì½”ë“œì…ë‹ˆë‹¤. 
ì£¼ìš” íŠ¹ì§•:
- ìµœì‹  C# ê¸°ëŠ¥ í™œìš©
- SOLID ì›ì¹™ ì¤€ìˆ˜
- ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬
- ëª…í™•í•œ ì½”ë“œ êµ¬ì¡°"""

class ProgressCallback(TrainerCallback):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ì½œë°±"""
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if "loss" in logs:
                logger.info(f"Step {state.global_step}: Loss = {logs['loss']:.4f}")
            
            # í•™ìŠµë¥  ë¡œê¹…
            if "learning_rate" in logs:
                logger.info(f"Learning Rate: {logs['learning_rate']:.2e}")
    
    def on_epoch_end(self, args, state, control, **kwargs):
        logger.info(f"âœ… Epoch {state.epoch} ì™„ë£Œ")
    
    def on_train_end(self, args, state, control, **kwargs):
        logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ì´ í•™ìŠµ ìŠ¤í…: {state.global_step}")

class CodeLlamaFineTuner:
    """Code Llama íŒŒì¸íŠœë‹ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
    def setup_wandb(self):
        """Weights & Biases ì„¤ì •"""
        if self.config.use_wandb:
            wandb.init(
                project="autoci-csharp-expert",
                name=f"fine-tuning-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
                config=self.config.__dict__
            )
            logger.info("âœ… Weights & Biases ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        logger.info("ğŸš€ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # ëª¨ë¸ ë¡œë“œ ì„¤ì •
        if self.config.use_8bit:
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.float16
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # 8-bit í•™ìŠµ ì¤€ë¹„
            self.model = prepare_model_for_kbit_training(self.model)
            logger.info("âœ… 8-bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.lora_target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
    def prepare_data(self) -> tuple:
        """ë°ì´í„° ì¤€ë¹„"""
        logger.info("ğŸ“š ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = CSharpDataset(
            self.config.data_path,
            self.tokenizer,
            self.config.max_length
        )
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„
        full_dataset = dataset.prepare_dataset()
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í•  (90/10)
        train_size = int(0.9 * len(full_dataset))
        eval_size = len(full_dataset) - train_size
        
        train_dataset = full_dataset.select(range(train_size))
        eval_dataset = full_dataset.select(range(train_size, train_size + eval_size))
        
        logger.info(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
        logger.info(f"âœ… ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}ê°œ")
        
        return train_dataset, eval_dataset
    
    def setup_training(self, train_dataset, eval_dataset):
        """í•™ìŠµ ì„¤ì •"""
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            warmup_steps=self.config.warmup_steps,
            learning_rate=self.config.learning_rate,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            save_total_limit=self.config.save_total_limit,
            fp16=torch.cuda.is_available(),
            push_to_hub=False,
            report_to="wandb" if self.config.use_wandb else "none",
            seed=self.config.seed,
            remove_unused_columns=False,
            label_names=["labels"]
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # ì½œë°± ì„¤ì •
        callbacks = [ProgressCallback()]
        if eval_dataset is not None:
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=3,
                    early_stopping_threshold=0.001
                )
            )
        
        # Trainer ì„¤ì •
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            callbacks=callbacks
        )
        
        logger.info("âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ")
    
    def train(self):
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("ğŸƒ í•™ìŠµ ì‹œì‘...")
        
        # í•™ìŠµ ì‹œì‘ ì‹œê°„
        start_time = datetime.now()
        
        try:
            # í•™ìŠµ ì‹¤í–‰
            train_result = self.trainer.train()
            
            # í•™ìŠµ ì‹œê°„ ê³„ì‚°
            training_time = datetime.now() - start_time
            hours = training_time.total_seconds() / 3600
            
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {hours:.1f}ì‹œê°„)")
            logger.info(f"ìµœì¢… Loss: {train_result.training_loss:.4f}")
            
            # ëª¨ë¸ ì €ì¥
            self.save_model()
            
            # í•™ìŠµ í†µê³„ ì €ì¥
            self.save_training_stats(train_result, hours)
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        logger.info("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self.trainer.save_model(self.config.output_dir)
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        self.tokenizer.save_pretrained(self.config.output_dir)
        
        # LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
        lora_path = Path(self.config.output_dir) / "adapter_model"
        self.model.save_pretrained(str(lora_path))
        
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.config.output_dir}")
    
    def save_training_stats(self, train_result, hours: float):
        """í•™ìŠµ í†µê³„ ì €ì¥"""
        stats = {
            "training_completed": datetime.now().isoformat(),
            "total_hours": hours,
            "final_loss": float(train_result.training_loss),
            "total_steps": train_result.global_step,
            "model_path": self.config.model_path,
            "output_dir": self.config.output_dir,
            "config": self.config.__dict__
        }
        
        stats_path = Path(self.config.output_dir) / "training_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š í•™ìŠµ í†µê³„ ì €ì¥: {stats_path}")
    
    def run(self):
        """ì „ì²´ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # W&B ì„¤ì •
            self.setup_wandb()
            
            # ëª¨ë¸ ë¡œë“œ
            self.load_model_and_tokenizer()
            
            # ë°ì´í„° ì¤€ë¹„
            train_dataset, eval_dataset = self.prepare_data()
            
            # í•™ìŠµ ì„¤ì •
            self.setup_training(train_dataset, eval_dataset)
            
            # í•™ìŠµ ì‹¤í–‰
            self.train()
            
            logger.info("ğŸ‰ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            if self.config.use_wandb:
                wandb.finish()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Code Llama C# Expert íŒŒì¸íŠœë‹")
    parser.add_argument("--data", type=str, help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--model", type=str, help="ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--output", type=str, help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--epochs", type=int, default=3, help="í•™ìŠµ ì—í¬í¬")
    parser.add_argument("--batch-size", type=int, default=4, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning-rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--use-wandb", action="store_true", help="W&B ì‚¬ìš©")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = ModelConfig()
    
    if args.data:
        config.data_path = args.data
    if args.model:
        config.model_path = args.model
    if args.output:
        config.output_dir = args.output
    if args.epochs:
        config.num_train_epochs = args.epochs
    if args.batch_size:
        config.per_device_train_batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.use_wandb:
        config.use_wandb = True
    
    # íŒŒì¸íŠœë„ˆ ì‹¤í–‰
    fine_tuner = CodeLlamaFineTuner(config)
    fine_tuner.run()

if __name__ == "__main__":
    main()