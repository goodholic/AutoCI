#!/usr/bin/env python3
"""
í–¥ìƒëœ AI ëª¨ë¸ ì„œë²„
Code Llama 7B-Instruct ê¸°ë°˜ C# ì½”ë“œ ìƒì„± ë° ê°œì„  API
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    StoppingCriteria,
    StoppingCriteriaList
)
import logging
import json
import asyncio
from datetime import datetime
from pathlib import Path
import re
from collections import deque
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AutoCI AI Model Server",
    description="Code Llama 7B-Instruct ê¸°ë°˜ C# ì „ë¬¸ê°€ AI ì„œë¹„ìŠ¤",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None
model_config = {
    "model_path": "../../CodeLlama-7b-Instruct-hf",
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 50,
    "repetition_penalty": 1.1
}

# ìš”ì²­ íˆìŠ¤í† ë¦¬ (ê°„ë‹¨í•œ ìºì‹±)
request_history = deque(maxlen=100)

# Pydantic ëª¨ë¸
class GenerateRequest(BaseModel):
    prompt: str = Field(..., description="ì½”ë“œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸")
    max_tokens: Optional[int] = Field(512, description="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜")
    temperature: Optional[float] = Field(0.7, description="ìƒì„± ì˜¨ë„ (0.0-1.0)")
    language: Optional[str] = Field("csharp", description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´")
    context: Optional[str] = Field(None, description="ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸")

class GenerateResponse(BaseModel):
    generated_code: str
    tokens_used: int
    generation_time: float
    language: str
    suggestions: Optional[List[str]] = None

class ImproveRequest(BaseModel):
    code: str = Field(..., description="ê°œì„ í•  ì½”ë“œ")
    language: str = Field("csharp", description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´")
    context: Optional[str] = Field(None, description="ì½”ë“œ ì»¨í…ìŠ¤íŠ¸")
    improvement_type: Optional[str] = Field("general", description="ê°œì„  ìœ í˜•")

class ImproveResponse(BaseModel):
    improved_code: str
    suggestions: List[str]
    quality_score: float
    improvements_made: List[str]

class AnalyzeRequest(BaseModel):
    code: str = Field(..., description="ë¶„ì„í•  ì½”ë“œ")
    language: str = Field("csharp", description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´")

class AnalyzeResponse(BaseModel):
    complexity: str
    patterns_found: List[str]
    potential_issues: List[str]
    best_practices: List[str]
    quality_score: float

# ì»¤ìŠ¤í…€ Stopping Criteria
class CSharpStoppingCriteria(StoppingCriteria):
    """C# ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ì¤‘ë‹¨ ê¸°ì¤€"""
    
    def __init__(self, tokenizer, stop_sequences=["\n\n\n", "```\n", "### Instruction:"]):
        self.tokenizer = tokenizer
        self.stop_sequences = stop_sequences
        
    def __call__(self, input_ids, scores, **kwargs):
        generated_text = self.tokenizer.decode(input_ids[0][-50:], skip_special_tokens=True)
        return any(stop_seq in generated_text for stop_seq in self.stop_sequences)

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
async def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer, device
    
    logger.info("ğŸš€ Code Llama 7B-Instruct ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    try:
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device("cpu")
            logger.warning("âš ï¸  CPU ì‚¬ìš© (ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        # ëª¨ë¸ ê²½ë¡œ í™•ì¸
        model_path = Path(model_config["model_path"])
        if not model_path.exists():
            # ìƒëŒ€ ê²½ë¡œ ì‹œë„
            model_path = Path(__file__).parent / model_config["model_path"]
        
        if not model_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(str(model_path))
        tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (8-bit quantization ì˜µì…˜)
        model_kwargs = {
            "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            "device_map": "auto" if torch.cuda.is_available() else None,
            "low_cpu_mem_usage": True
        }
        
        # 8-bit ë¡œë“œ ì‹œë„
        try:
            model_kwargs["load_in_8bit"] = True
            model = AutoModelForCausalLM.from_pretrained(str(model_path), **model_kwargs)
            logger.info("âœ… 8-bit ì–‘ìí™”ë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            # ì¼ë°˜ ë¡œë“œ
            model_kwargs.pop("load_in_8bit", None)
            model = AutoModelForCausalLM.from_pretrained(str(model_path), **model_kwargs)
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        if device.type == "cpu":
            model = model.to(device)
        
        # ëª¨ë¸ ì •ë³´
        param_count = sum(p.numel() for p in model.parameters()) / 1e9
        logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {param_count:.1f}B")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

# ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    await load_model()
    logger.info("âœ… AI ëª¨ë¸ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """API ë£¨íŠ¸"""
    return {
        "service": "AutoCI AI Model Server",
        "version": "2.0.0",
        "model": "Code Llama 7B-Instruct",
        "status": "ready" if model is not None else "loading",
        "endpoints": {
            "generate": "/generate",
            "improve": "/improve",
            "analyze": "/analyze",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "device": str(device) if device else "not set",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate_code(request: GenerateRequest):
    """ì½”ë“œ ìƒì„± API"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    start_time = time.time()
    
    try:
        # C# íŠ¹í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if request.language.lower() == "csharp":
            system_prompt = """You are an expert C# developer with deep knowledge of:
- .NET Core/5/6/7/8
- ASP.NET Core
- Entity Framework Core
- LINQ
- Async/Await patterns
- SOLID principles
- Design patterns
- Best practices and modern C# features

Generate clean, efficient, and well-documented C# code."""
        else:
            system_prompt = f"You are an expert {request.language} developer."
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"""### System:
{system_prompt}

### Instruction:
{request.prompt}

{f'### Context: {request.context}' if request.context else ''}

### Response:
```{request.language}
"""
        
        # í† í°í™”
        inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=1024)
        if device:
            inputs = inputs.to(device)
        
        # Stopping criteria ì„¤ì •
        stopping_criteria = StoppingCriteriaList([
            CSharpStoppingCriteria(tokenizer)
        ])
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=model_config["top_p"],
                top_k=model_config["top_k"],
                repetition_penalty=model_config["repetition_penalty"],
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                stopping_criteria=stopping_criteria,
                do_sample=True
            )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ìƒì„±ëœ ì½”ë“œë§Œ ì¶”ì¶œ
        code_start = generated.find(f"```{request.language}")
        if code_start != -1:
            code_start += len(f"```{request.language}") + 1
            code_end = generated.find("```", code_start)
            if code_end != -1:
                generated_code = generated[code_start:code_end].strip()
            else:
                generated_code = generated[code_start:].strip()
        else:
            # í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
            response_start = generated.find("### Response:")
            if response_start != -1:
                generated_code = generated[response_start + 13:].strip()
            else:
                generated_code = generated[len(full_prompt):].strip()
        
        # ì½”ë“œ ì •ë¦¬
        generated_code = clean_generated_code(generated_code, request.language)
        
        # ì œì•ˆì‚¬í•­ ìƒì„±
        suggestions = generate_suggestions(generated_code, request.language)
        
        # ì‘ë‹µ
        generation_time = time.time() - start_time
        tokens_used = len(outputs[0]) - len(inputs.input_ids[0])
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        request_history.append({
            "timestamp": datetime.now().isoformat(),
            "prompt": request.prompt,
            "language": request.language,
            "tokens": tokens_used,
            "time": generation_time
        })
        
        return GenerateResponse(
            generated_code=generated_code,
            tokens_used=tokens_used,
            generation_time=round(generation_time, 2),
            language=request.language,
            suggestions=suggestions
        )
        
    except Exception as e:
        logger.error(f"ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/improve", response_model=ImproveResponse)
async def improve_code(request: ImproveRequest):
    """ì½”ë“œ ê°œì„  API"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ì½”ë“œ ë¶„ì„
        quality_score, issues = analyze_code_quality(request.code, request.language)
        
        # ê°œì„  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        improvement_prompt = f"""### Instruction:
Improve the following {request.language} code by:
1. Following best practices and design patterns
2. Improving error handling
3. Optimizing performance
4. Adding proper documentation
5. Ensuring SOLID principles
6. Using modern language features

Original code:
```{request.language}
{request.code}
```

{f'Context: {request.context}' if request.context else ''}

Provide the improved version with explanations.

### Response:
Improved code:
```{request.language}
"""
        
        # ìƒì„±
        response = await generate_code(GenerateRequest(
            prompt=improvement_prompt,
            max_tokens=1024,
            temperature=0.5,  # ë” ë³´ìˆ˜ì ì¸ ìƒì„±
            language=request.language
        ))
        
        # ê°œì„ ì‚¬í•­ ì¶”ì¶œ
        improvements_made = extract_improvements(request.code, response.generated_code)
        
        # ì œì•ˆì‚¬í•­ ìƒì„±
        suggestions = [
            "Consider using dependency injection for better testability",
            "Add XML documentation comments for public APIs",
            "Implement proper logging for production use",
            "Consider async/await for I/O operations",
            "Add unit tests for critical business logic"
        ]
        
        # í’ˆì§ˆ ì ìˆ˜ ì¬ê³„ì‚°
        new_quality_score, _ = analyze_code_quality(response.generated_code, request.language)
        
        return ImproveResponse(
            improved_code=response.generated_code,
            suggestions=suggestions[:5],  # ìƒìœ„ 5ê°œë§Œ
            quality_score=new_quality_score,
            improvements_made=improvements_made
        )
        
    except Exception as e:
        logger.error(f"ì½”ë“œ ê°œì„  ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_code(request: AnalyzeRequest):
    """ì½”ë“œ ë¶„ì„ API"""
    try:
        # ì½”ë“œ ë³µì¡ë„ ë¶„ì„
        complexity = analyze_complexity(request.code)
        
        # íŒ¨í„´ ê°ì§€
        patterns = detect_patterns(request.code, request.language)
        
        # ì ì¬ì  ì´ìŠˆ
        potential_issues = detect_issues(request.code, request.language)
        
        # ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
        best_practices = check_best_practices(request.code, request.language)
        
        # í’ˆì§ˆ ì ìˆ˜
        quality_score, _ = analyze_code_quality(request.code, request.language)
        
        return AnalyzeResponse(
            complexity=complexity,
            patterns_found=patterns,
            potential_issues=potential_issues,
            best_practices=best_practices,
            quality_score=quality_score
        )
        
    except Exception as e:
        logger.error(f"ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# í—¬í¼ í•¨ìˆ˜ë“¤
def clean_generated_code(code: str, language: str) -> str:
    """ìƒì„±ëœ ì½”ë“œ ì •ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ì œê±°
    code = re.sub(r'^```\w*\n?', '', code)
    code = re.sub(r'\n?```$', '', code)
    
    # ì¤‘ë³µëœ ë¹ˆ ì¤„ ì œê±°
    code = re.sub(r'\n\s*\n\s*\n', '\n\n', code)
    
    # ì–¸ì–´ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬
    if language.lower() == "csharp":
        # using ë¬¸ ì •ë¦¬
        lines = code.split('\n')
        using_lines = []
        other_lines = []
        
        for line in lines:
            if line.strip().startswith('using ') and line.strip().endswith(';'):
                using_lines.append(line.strip())
            else:
                other_lines.append(line)
        
        # using ë¬¸ ì •ë ¬
        using_lines = sorted(list(set(using_lines)))
        
        if using_lines:
            code = '\n'.join(using_lines) + '\n\n' + '\n'.join(other_lines)
    
    return code.strip()

def generate_suggestions(code: str, language: str) -> List[str]:
    """ì½”ë“œì— ëŒ€í•œ ì œì•ˆì‚¬í•­ ìƒì„±"""
    suggestions = []
    
    if language.lower() == "csharp":
        # null ì²´í¬ ì œì•ˆ
        if 'null' not in code and ('string' in code or 'object' in code):
            suggestions.append("Consider adding null checks for reference types")
        
        # async/await ì œì•ˆ
        if 'Task' in code and 'async' not in code:
            suggestions.append("Consider using async/await for asynchronous operations")
        
        # LINQ ì œì•ˆ
        if 'for' in code and 'List' in code:
            suggestions.append("Consider using LINQ for collection operations")
        
        # ì˜ˆì™¸ ì²˜ë¦¬ ì œì•ˆ
        if 'try' not in code and ('File' in code or 'Database' in code or 'Http' in code):
            suggestions.append("Add try-catch blocks for potential exceptions")
        
        # ì¸í„°í˜ì´ìŠ¤ ì œì•ˆ
        if 'class' in code and 'interface' not in code:
            suggestions.append("Consider defining interfaces for better abstraction")
    
    return suggestions

def analyze_code_quality(code: str, language: str) -> tuple[float, List[str]]:
    """ì½”ë“œ í’ˆì§ˆ ë¶„ì„"""
    score = 1.0
    issues = []
    
    if language.lower() == "csharp":
        # ì£¼ì„ í™•ì¸
        if '///' not in code and 'public' in code:
            score -= 0.1
            issues.append("Missing XML documentation comments")
        
        # ì—ëŸ¬ ì²˜ë¦¬
        if 'try' not in code and ('Exception' in code or 'throw' in code):
            score -= 0.15
            issues.append("Missing proper error handling")
        
        # ë„¤ì´ë° ì»¨ë²¤ì…˜
        if re.search(r'[a-z][A-Z]', code):  # camelCase in wrong places
            score -= 0.05
            issues.append("Potential naming convention issues")
        
        # ë§¤ì§ ë„˜ë²„
        if re.search(r'[^0-9][0-9]{2,}[^0-9]', code):
            score -= 0.05
            issues.append("Magic numbers detected")
        
        # ê¸´ ë©”ì„œë“œ
        lines = code.split('\n')
        method_lines = 0
        in_method = False
        for line in lines:
            if '{' in line and ('public' in line or 'private' in line or 'protected' in line):
                in_method = True
                method_lines = 0
            elif in_method:
                method_lines += 1
                if method_lines > 50:
                    score -= 0.1
                    issues.append("Methods too long")
                    break
            elif '}' in line and in_method:
                in_method = False
    
    return max(0.0, score), issues

def analyze_complexity(code: str) -> str:
    """ì½”ë“œ ë³µì¡ë„ ë¶„ì„"""
    lines = code.split('\n')
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    loc = len([l for l in lines if l.strip()])
    
    # ì‚¬ì´í´ë¡œë§¤í‹± ë³µì¡ë„ ê°„ë‹¨ ì¶”ì •
    complexity_keywords = ['if', 'else', 'for', 'while', 'switch', 'case', 'catch']
    complexity_count = sum(1 for line in lines for keyword in complexity_keywords if keyword in line)
    
    if complexity_count < 5:
        return "Low"
    elif complexity_count < 10:
        return "Medium"
    else:
        return "High"

def detect_patterns(code: str, language: str) -> List[str]:
    """ë””ìì¸ íŒ¨í„´ ê°ì§€"""
    patterns = []
    
    if language.lower() == "csharp":
        # Singleton
        if 'private static' in code and 'Instance' in code:
            patterns.append("Singleton Pattern")
        
        # Factory
        if 'Create' in code and 'return new' in code:
            patterns.append("Factory Pattern")
        
        # Repository
        if 'Repository' in code or ('Add' in code and 'Get' in code and 'Delete' in code):
            patterns.append("Repository Pattern")
        
        # Observer
        if 'event' in code or 'EventHandler' in code:
            patterns.append("Observer Pattern")
        
        # Dependency Injection
        if 'interface' in code and 'constructor' in code:
            patterns.append("Dependency Injection")
    
    return patterns

def detect_issues(code: str, language: str) -> List[str]:
    """ì ì¬ì  ì´ìŠˆ ê°ì§€"""
    issues = []
    
    if language.lower() == "csharp":
        # SQL Injection ìœ„í—˜
        if 'SELECT' in code and '+' in code:
            issues.append("Potential SQL injection vulnerability")
        
        # í•˜ë“œì½”ë”©ëœ ë¹„ë°€
        if re.search(r'(password|key|secret)\s*=\s*"[^"]+"', code, re.IGNORECASE):
            issues.append("Hardcoded secrets detected")
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°€ëŠ¥ì„±
        if 'IDisposable' in code and 'using' not in code:
            issues.append("Potential memory leak - IDisposable not properly handled")
        
        # ë¬´í•œ ë£¨í”„ ê°€ëŠ¥ì„±
        if 'while (true)' in code and 'break' not in code:
            issues.append("Potential infinite loop")
    
    return issues

def check_best_practices(code: str, language: str) -> List[str]:
    """ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì²´í¬"""
    practices = []
    
    if language.lower() == "csharp":
        # ì¢‹ì€ practices í™•ì¸
        if '///' in code:
            practices.append("âœ“ XML documentation comments used")
        
        if 'async' in code and 'await' in code:
            practices.append("âœ“ Async/await pattern properly used")
        
        if 'try' in code and 'catch' in code:
            practices.append("âœ“ Proper error handling implemented")
        
        if 'interface' in code:
            practices.append("âœ“ Interface-based design")
        
        if 'readonly' in code:
            practices.append("âœ“ Immutability considered")
        
        # ê°œì„  í•„ìš”
        if 'var' not in code and language == "csharp":
            practices.append("âš  Consider using 'var' for type inference")
        
        if 'string.Format' in code:
            practices.append("âš  Consider using string interpolation ($\"\")")
    
    return practices

def extract_improvements(original: str, improved: str) -> List[str]:
    """ê°œì„ ì‚¬í•­ ì¶”ì¶œ"""
    improvements = []
    
    # ê°„ë‹¨í•œ ë¹„êµ
    if 'async' in improved and 'async' not in original:
        improvements.append("Added async/await for asynchronous operations")
    
    if 'try' in improved and 'try' not in original:
        improvements.append("Added error handling")
    
    if '///' in improved and '///' not in original:
        improvements.append("Added XML documentation comments")
    
    if 'interface' in improved and 'interface' not in original:
        improvements.append("Introduced interface-based design")
    
    if improved.count('\n') > original.count('\n') * 1.2:
        improvements.append("Improved code structure and readability")
    
    return improvements

@app.get("/stats")
async def get_stats():
    """ì„œë²„ í†µê³„"""
    return {
        "total_requests": len(request_history),
        "recent_requests": list(request_history)[-10:],
        "model_info": {
            "name": "Code Llama 7B-Instruct",
            "loaded": model is not None,
            "device": str(device) if device else "not set"
        },
        "server_uptime": "N/A"  # êµ¬í˜„ í•„ìš”
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)