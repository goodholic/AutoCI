#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” Advanced Indexing System - ê³ ê¸‰ ë°ì´í„° ì¸ë±ì‹± ë° í•™ìŠµ ê²°ê³¼ ë¬¸ì„œí™”
"""

import json
import os
import sqlite3
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import hashlib
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from collections import defaultdict

class AdvancedIndexer:
    def __init__(self, data_dir: str = "expert_learning_data", db_path: str = "expert_index.db"):
        self.data_dir = Path(data_dir)
        self.db_path = db_path
        self.learning_results_dir = Path("learning_results")
        self.learning_results_dir.mkdir(exist_ok=True)
        
        # ì¸ë±ìŠ¤ í†µê³„
        self.stats = {
            'total_files': 0,
            'total_patterns': 0,
            'total_categories': 0,
            'processing_time': 0,
            'index_size': 0
        }
        
        # íŒ¨í„´ ë¶„ì„ê¸°
        self.pattern_analyzers = {
            'architecture': self.analyze_architecture_patterns,
            'performance': self.analyze_performance_patterns,
            'unity': self.analyze_unity_patterns,
            'async': self.analyze_async_patterns,
            'testing': self.analyze_testing_patterns
        }
        
        self.init_database()
    
    def init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë©”ì¸ ì½”ë“œ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS code_entries (
                id TEXT PRIMARY KEY,
                file_name TEXT,
                category TEXT,
                template_name TEXT,
                description TEXT,
                code_hash TEXT,
                quality_score INTEGER,
                complexity INTEGER,
                created_at TIMESTAMP,
                indexed_at TIMESTAMP
            )
        ''')
        
        # íŒ¨í„´ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_name TEXT,
                pattern_type TEXT,
                code_id TEXT,
                confidence REAL,
                FOREIGN KEY (code_id) REFERENCES code_entries(id)
            )
        ''')
        
        # í‚¤ì›Œë“œ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT,
                code_id TEXT,
                frequency INTEGER,
                FOREIGN KEY (code_id) REFERENCES code_entries(id)
            )
        ''')
        
        # í•™ìŠµ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                timestamp TIMESTAMP,
                patterns_learned INTEGER,
                improvements TEXT,
                metrics TEXT
            )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON code_entries(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_template ON code_entries(template_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pattern ON patterns(pattern_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_keyword ON keywords(keyword)')
        
        conn.commit()
        conn.close()
    
    def index_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì¸ë±ì‹±"""
        start_time = time.time()
        print("ğŸš€ ê³ ê¸‰ ë°ì´í„° ì¸ë±ì‹± ì‹œì‘...")
        
        if not self.data_dir.exists():
            print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return
        
        json_files = list(self.data_dir.glob("*.json"))
        self.stats['total_files'] = len(json_files)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(self.index_single_file, f): f for f in json_files}
            
            processed = 0
            for future in as_completed(futures):
                result = future.result()
                if result:
                    processed += 1
                    if processed % 50 == 0:
                        print(f"  ì§„í–‰ì¤‘... {processed}/{len(json_files)} íŒŒì¼ ì²˜ë¦¬ë¨")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['processing_time'] = time.time() - start_time
        self.update_statistics()
        
        # í•™ìŠµ ê²°ê³¼ ìƒì„±
        self.generate_learning_report()
        
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ! ì²˜ë¦¬ ì‹œê°„: {self.stats['processing_time']:.2f}ì´ˆ")
        print(f"ğŸ“Š í†µê³„: {self.stats['total_files']}ê°œ íŒŒì¼, {self.stats['total_patterns']}ê°œ íŒ¨í„´")
    
    def index_single_file(self, file_path: Path) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ì¸ë±ì‹±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            code = data.get('code', '').strip()
            if len(code) < 100:
                return False
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            entry_id = file_path.stem
            category = data.get('category', 'general')
            template_name = data.get('template_name', '')
            quality_score = data.get('quality_score', 80)
            
            # ê³ ê¸‰ ë¶„ì„
            description = self.extract_description(data, code)
            code_hash = hashlib.sha256(code.encode()).hexdigest()
            complexity = self.calculate_complexity(code)
            patterns = self.extract_all_patterns(code, category)
            keywords = self.extract_keywords(code)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì½”ë“œ ì—”íŠ¸ë¦¬ ì €ì¥
            cursor.execute('''
                INSERT OR REPLACE INTO code_entries 
                (id, file_name, category, template_name, description, code_hash, 
                 quality_score, complexity, created_at, indexed_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (entry_id, file_path.name, category, template_name, description, 
                  code_hash, quality_score, complexity, 
                  datetime.now(), datetime.now()))
            
            # íŒ¨í„´ ì €ì¥
            for pattern_type, pattern_list in patterns.items():
                for pattern_name, confidence in pattern_list:
                    cursor.execute('''
                        INSERT INTO patterns (pattern_name, pattern_type, code_id, confidence)
                        VALUES (?, ?, ?, ?)
                    ''', (pattern_name, pattern_type, entry_id, confidence))
                    self.stats['total_patterns'] += 1
            
            # í‚¤ì›Œë“œ ì €ì¥
            for keyword, frequency in keywords.items():
                cursor.execute('''
                    INSERT INTO keywords (keyword, code_id, frequency)
                    VALUES (?, ?, ?)
                ''', (keyword, entry_id, frequency))
            
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ {file_path}: {e}")
            return False
    
    def extract_description(self, data: Dict, code: str) -> str:
        """ì„¤ëª… ì¶”ì¶œ"""
        if 'description' in data:
            return data['description']
        
        # ì½”ë“œì—ì„œ ì¶”ì¶œ
        lines = code.split('\n')
        for line in lines[:10]:
            if '///' in line or '/*' in line:
                return line.strip('/*/ ').strip()
        
        return "C# ì½”ë“œ íŒ¨í„´"
    
    def calculate_complexity(self, code: str) -> int:
        """ë³µì¡ë„ ê³„ì‚°"""
        complexity = 0
        
        # ìˆœí™˜ ë³µì¡ë„ ê°„ë‹¨ ê³„ì‚°
        complexity += len(re.findall(r'\b(if|else|for|while|switch|case|catch)\b', code))
        
        # ì¤‘ì²© ê¹Šì´
        lines = code.split('\n')
        max_indent = 0
        for line in lines:
            if line.strip():
                indent = (len(line) - len(line.lstrip())) // 4
                max_indent = max(max_indent, indent)
        complexity += max_indent
        
        # ë©”ì„œë“œ ìˆ˜
        complexity += len(re.findall(r'(?:public|private|protected).*?\(', code)) // 2
        
        return min(complexity, 100)
    
    def extract_all_patterns(self, code: str, category: str) -> Dict[str, List[Tuple[str, float]]]:
        """ëª¨ë“  íŒ¨í„´ ì¶”ì¶œ"""
        all_patterns = {}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        if category in self.pattern_analyzers:
            all_patterns[category] = self.pattern_analyzers[category](code)
        
        # ì¼ë°˜ íŒ¨í„´ ë¶„ì„
        all_patterns['general'] = self.analyze_general_patterns(code)
        
        return all_patterns
    
    def analyze_general_patterns(self, code: str) -> List[Tuple[str, float]]:
        """ì¼ë°˜ ë””ìì¸ íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        pattern_indicators = {
            'Singleton': [r'private static .* _instance', r'public static .* Instance'],
            'Factory': [r'Create\w+', r'Factory', r'Build\w+'],
            'Repository': [r'IRepository', r'Repository<', r'DbContext'],
            'Observer': [r'INotify', r'EventHandler', r'event .*EventArgs'],
            'Command': [r'ICommand', r'Execute\(', r'CanExecute'],
            'Strategy': [r'IStrategy', r'Strategy', r'Algorithm'],
            'Decorator': [r'IDecorator', r'Wrapper', r'decorator'],
            'Adapter': [r'IAdapter', r'Adapter', r'Convert'],
            'Mediator': [r'IMediator', r'Mediator', r'IRequest'],
            'Builder': [r'Builder', r'With\w+', r'Build\(']
        }
        
        for pattern_name, indicators in pattern_indicators.items():
            confidence = 0
            for indicator in indicators:
                if re.search(indicator, code):
                    confidence += 0.33
            
            if confidence > 0.3:
                patterns.append((pattern_name, min(confidence, 1.0)))
        
        return patterns
    
    def analyze_architecture_patterns(self, code: str) -> List[Tuple[str, float]]:
        """ì•„í‚¤í…ì²˜ íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        # Clean Architecture íŒ¨í„´
        if all(term in code for term in ['Domain', 'Application', 'Infrastructure']):
            patterns.append(('CleanArchitecture', 0.9))
        
        # DDD íŒ¨í„´
        if any(term in code for term in ['AggregateRoot', 'ValueObject', 'DomainEvent']):
            patterns.append(('DomainDrivenDesign', 0.8))
        
        # CQRS íŒ¨í„´
        if all(term in code for term in ['Command', 'Query', 'Handler']):
            patterns.append(('CQRS', 0.85))
        
        # Event Sourcing
        if all(term in code for term in ['Event', 'EventStore', 'Aggregate']):
            patterns.append(('EventSourcing', 0.75))
        
        return patterns
    
    def analyze_performance_patterns(self, code: str) -> List[Tuple[str, float]]:
        """ì„±ëŠ¥ íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        # Memory ìµœì í™”
        if 'Span<' in code or 'Memory<' in code:
            patterns.append(('MemoryOptimization', 0.9))
        
        # Object Pooling
        if 'ObjectPool' in code or 'Pool<' in code:
            patterns.append(('ObjectPooling', 0.85))
        
        # Caching
        if 'IMemoryCache' in code or 'Cache' in code:
            patterns.append(('Caching', 0.7))
        
        # Async ìµœì í™”
        if 'ValueTask' in code:
            patterns.append(('AsyncOptimization', 0.8))
        
        return patterns
    
    def analyze_unity_patterns(self, code: str) -> List[Tuple[str, float]]:
        """Unity íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        # DOTS/ECS
        if any(term in code for term in ['IComponentData', 'Entity', 'SystemBase']):
            patterns.append(('Unity_DOTS', 0.9))
        
        # Job System
        if 'IJob' in code or 'JobHandle' in code:
            patterns.append(('Unity_JobSystem', 0.85))
        
        # Object Pooling
        if 'ObjectPool' in code and 'GameObject' in code:
            patterns.append(('Unity_ObjectPool', 0.8))
        
        # Coroutines
        if 'IEnumerator' in code and 'yield return' in code:
            patterns.append(('Unity_Coroutine', 0.75))
        
        return patterns
    
    def analyze_async_patterns(self, code: str) -> List[Tuple[str, float]]:
        """ë¹„ë™ê¸° íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        # Async/Await
        if 'async' in code and 'await' in code:
            patterns.append(('AsyncAwait', 0.9))
        
        # Channels
        if 'Channel<' in code:
            patterns.append(('Channels', 0.85))
        
        # DataFlow
        if 'DataflowBlock' in code or 'ActionBlock' in code:
            patterns.append(('DataFlow', 0.8))
        
        # Reactive
        if 'IObservable' in code or 'Observable.' in code:
            patterns.append(('ReactiveExtensions', 0.75))
        
        return patterns
    
    def analyze_testing_patterns(self, code: str) -> List[Tuple[str, float]]:
        """í…ŒìŠ¤íŒ… íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        # Unit Testing
        if any(term in code for term in ['[Test]', '[Fact]', '[TestMethod]']):
            patterns.append(('UnitTesting', 0.9))
        
        # Mocking
        if 'Mock<' in code or 'Substitute.' in code:
            patterns.append(('Mocking', 0.85))
        
        # BDD
        if any(term in code for term in ['Given', 'When', 'Then']):
            patterns.append(('BDD', 0.7))
        
        return patterns
    
    def extract_keywords(self, code: str) -> Dict[str, int]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°"""
        keywords = defaultdict(int)
        
        # C# í‚¤ì›Œë“œ
        csharp_keywords = [
            'async', 'await', 'Task', 'public', 'private', 'protected',
            'class', 'interface', 'struct', 'enum', 'delegate',
            'override', 'virtual', 'abstract', 'sealed',
            'using', 'namespace', 'static', 'const', 'readonly'
        ]
        
        # Unity í‚¤ì›Œë“œ
        unity_keywords = [
            'GameObject', 'Transform', 'MonoBehaviour', 'Component',
            'Rigidbody', 'Collider', 'Coroutine', 'Prefab'
        ]
        
        # íŒ¨í„´ í‚¤ì›Œë“œ
        pattern_keywords = [
            'Repository', 'Factory', 'Service', 'Manager', 'Controller',
            'Handler', 'Provider', 'Builder', 'Strategy', 'Observer'
        ]
        
        all_keywords = csharp_keywords + unity_keywords + pattern_keywords
        
        for keyword in all_keywords:
            count = len(re.findall(rf'\b{keyword}\b', code))
            if count > 0:
                keywords[keyword.lower()] = count
        
        return dict(keywords)
    
    def update_statistics(self):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì¹´í…Œê³ ë¦¬ ìˆ˜
        cursor.execute('SELECT COUNT(DISTINCT category) FROM code_entries')
        self.stats['total_categories'] = cursor.fetchone()[0]
        
        # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
        cursor.execute("SELECT page_count * page_size FROM pragma_page_count(), pragma_page_size()")
        self.stats['index_size'] = cursor.fetchone()[0]
        
        conn.close()
    
    def generate_learning_report(self):
        """í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.learning_results_dir / f"indexing_report_{timestamp}.md"
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ğŸ” AutoCI ê³ ê¸‰ ì¸ë±ì‹± ë³´ê³ ì„œ\n\n")
            f.write(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ì²˜ë¦¬ ì‹œê°„**: {self.stats['processing_time']:.2f}ì´ˆ\n\n")
            
            f.write("## ğŸ“Š ì „ì²´ í†µê³„\n\n")
            f.write(f"- **ì´ íŒŒì¼ ìˆ˜**: {self.stats['total_files']}ê°œ\n")
            f.write(f"- **ì´ íŒ¨í„´ ìˆ˜**: {self.stats['total_patterns']}ê°œ\n")
            f.write(f"- **ì¹´í…Œê³ ë¦¬ ìˆ˜**: {self.stats['total_categories']}ê°œ\n")
            f.write(f"- **ì¸ë±ìŠ¤ í¬ê¸°**: {self.stats['index_size'] / 1024 / 1024:.2f} MB\n\n")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            f.write("## ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n\n")
            cursor.execute('''
                SELECT category, COUNT(*) as count 
                FROM code_entries 
                GROUP BY category 
                ORDER BY count DESC
            ''')
            for category, count in cursor.fetchall():
                f.write(f"- **{category}**: {count}ê°œ\n")
            f.write("\n")
            
            # ì£¼ìš” íŒ¨í„´
            f.write("## ğŸ¯ ì£¼ìš” ë””ìì¸ íŒ¨í„´\n\n")
            cursor.execute('''
                SELECT pattern_name, pattern_type, COUNT(*) as count, AVG(confidence) as avg_conf
                FROM patterns 
                GROUP BY pattern_name, pattern_type
                ORDER BY count DESC
                LIMIT 20
            ''')
            for pattern, ptype, count, confidence in cursor.fetchall():
                f.write(f"- **{pattern}** ({ptype}): {count}ê°œ ì‚¬ìš© (ì‹ ë¢°ë„: {confidence:.2f})\n")
            f.write("\n")
            
            # ì¸ê¸° í‚¤ì›Œë“œ
            f.write("## ğŸ”‘ ì¸ê¸° í‚¤ì›Œë“œ Top 20\n\n")
            cursor.execute('''
                SELECT keyword, SUM(frequency) as total_freq
                FROM keywords
                GROUP BY keyword
                ORDER BY total_freq DESC
                LIMIT 20
            ''')
            for keyword, freq in cursor.fetchall():
                f.write(f"- **{keyword}**: {freq}íšŒ\n")
            f.write("\n")
            
            # ë³µì¡ë„ ë¶„ì„
            f.write("## ğŸ“ˆ ë³µì¡ë„ ë¶„ì„\n\n")
            cursor.execute('''
                SELECT 
                    AVG(complexity) as avg_complexity,
                    MIN(complexity) as min_complexity,
                    MAX(complexity) as max_complexity
                FROM code_entries
            ''')
            avg_comp, min_comp, max_comp = cursor.fetchone()
            f.write(f"- **í‰ê·  ë³µì¡ë„**: {avg_comp:.2f}\n")
            f.write(f"- **ìµœì†Œ ë³µì¡ë„**: {min_comp}\n")
            f.write(f"- **ìµœëŒ€ ë³µì¡ë„**: {max_comp}\n\n")
            
            # í’ˆì§ˆ ì ìˆ˜ ë¶„í¬
            f.write("## â­ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬\n\n")
            cursor.execute('''
                SELECT 
                    CASE 
                        WHEN quality_score >= 90 THEN '90-100 (ìš°ìˆ˜)'
                        WHEN quality_score >= 80 THEN '80-89 (ì–‘í˜¸)'
                        WHEN quality_score >= 70 THEN '70-79 (ë³´í†µ)'
                        ELSE '70 ë¯¸ë§Œ (ê°œì„  í•„ìš”)'
                    END as range,
                    COUNT(*) as count
                FROM code_entries
                GROUP BY range
                ORDER BY quality_score DESC
            ''')
            for range_name, count in cursor.fetchall():
                f.write(f"- **{range_name}**: {count}ê°œ\n")
            f.write("\n")
            
            # í•™ìŠµ ê¶Œì¥ì‚¬í•­
            f.write("## ğŸ’¡ í•™ìŠµ ê¶Œì¥ì‚¬í•­\n\n")
            
            # ë¶€ì¡±í•œ íŒ¨í„´ ì°¾ê¸°
            cursor.execute('''
                SELECT pattern_type, COUNT(DISTINCT pattern_name) as variety
                FROM patterns
                GROUP BY pattern_type
                ORDER BY variety ASC
                LIMIT 5
            ''')
            weak_patterns = cursor.fetchall()
            
            if weak_patterns:
                f.write("### ë³´ê°•ì´ í•„ìš”í•œ íŒ¨í„´ ì˜ì—­:\n\n")
                for ptype, variety in weak_patterns:
                    f.write(f"- **{ptype}**: {variety}ê°œ íŒ¨í„´ë§Œ ë°œê²¬ë¨\n")
                f.write("\n")
            
            f.write("### ì¶”ì²œ í•™ìŠµ ì†ŒìŠ¤:\n\n")
            f.write("- **Architecture**: dotnet/aspnetcore, ardalis/CleanArchitecture\n")
            f.write("- **Performance**: dotnet/runtime (Span/Memory ì˜ˆì œ)\n")
            f.write("- **Unity Advanced**: Unity-Technologies/DOTS-Samples\n")
            f.write("- **Testing**: xunit/xunit, nunit/nunit\n")
        
        # í•™ìŠµ ê²°ê³¼ DBì— ì €ì¥
        session_id = hashlib.md5(timestamp.encode()).hexdigest()[:8]
        cursor.execute('''
            INSERT INTO learning_results 
            (session_id, timestamp, patterns_learned, improvements, metrics)
            VALUES (?, ?, ?, ?, ?)
        ''', (session_id, datetime.now(), self.stats['total_patterns'], 
              json.dumps({"weak_patterns": weak_patterns}),
              json.dumps(self.stats)))
        
        conn.commit()
        conn.close()
        
        print(f"ğŸ“„ í•™ìŠµ ë³´ê³ ì„œ ìƒì„±ë¨: {report_file}")
    
    def query_patterns(self, pattern_type: str = None, min_confidence: float = 0.5) -> List[Dict]:
        """íŒ¨í„´ ì¿¼ë¦¬"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if pattern_type:
            cursor.execute('''
                SELECT p.pattern_name, p.confidence, c.template_name, c.category
                FROM patterns p
                JOIN code_entries c ON p.code_id = c.id
                WHERE p.pattern_type = ? AND p.confidence >= ?
                ORDER BY p.confidence DESC
            ''', (pattern_type, min_confidence))
        else:
            cursor.execute('''
                SELECT p.pattern_name, p.pattern_type, p.confidence, c.template_name
                FROM patterns p
                JOIN code_entries c ON p.code_id = c.id
                WHERE p.confidence >= ?
                ORDER BY p.confidence DESC
                LIMIT 50
            ''', (min_confidence,))
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'pattern': row[0],
                'type': row[1] if not pattern_type else pattern_type,
                'confidence': row[2],
                'template': row[3]
            })
        
        conn.close()
        return results

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Advanced Indexing System")
    parser.add_argument('--index', action='store_true', help='ì „ì²´ ë°ì´í„° ì¸ë±ì‹±')
    parser.add_argument('--query', type=str, help='íŒ¨í„´ ì¿¼ë¦¬')
    parser.add_argument('--stats', action='store_true', help='í†µê³„ í‘œì‹œ')
    
    args = parser.parse_args()
    
    indexer = AdvancedIndexer()
    
    if args.index:
        indexer.index_all_data()
    
    elif args.query:
        results = indexer.query_patterns(pattern_type=args.query)
        print(f"\nğŸ” '{args.query}' íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼:")
        for r in results[:10]:
            print(f"  - {r['pattern']} (ì‹ ë¢°ë„: {r['confidence']:.2f}) - {r['template']}")
    
    elif args.stats:
        indexer.update_statistics()
        print("\nğŸ“Š ì¸ë±ì‹± ì‹œìŠ¤í…œ í†µê³„:")
        for key, value in indexer.stats.items():
            print(f"  - {key}: {value}")
    
    else:
        print("ì‚¬ìš©ë²•: python advanced_indexer.py [--index|--query PATTERN|--stats]")

if __name__ == "__main__":
    main()