#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ
C# ì „ë¬¸ ë‚´ìš© í¬ë¡¤ë§ + ì‹¤ì‹œê°„ í•™ìŠµ
"""

import os
import sys
import json
import time
import threading
import requests
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import hashlib
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import schedule

class CSharpKnowledgeCrawler:
    """C# ì „ë¬¸ ì§€ì‹ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, data_dir: str = "learning_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db_path = self.data_dir / "csharp_knowledge.db"
        self.init_database()
        
        # í¬ë¡¤ë§ ëŒ€ìƒ
        self.sources = {
            "microsoft_docs": [
                "https://docs.microsoft.com/en-us/dotnet/csharp/",
                "https://docs.microsoft.com/en-us/dotnet/api/",
                "https://docs.microsoft.com/en-us/aspnet/core/",
                "https://docs.microsoft.com/en-us/dotnet/framework/"
            ],
            "unity_docs": [
                "https://docs.unity3d.com/ScriptReference/",
                "https://docs.unity3d.com/Manual/",
                "https://learn.unity.com/"
            ],
            "github_repos": [
                "https://api.github.com/search/repositories?q=language:csharp+stars:>1000",
                "https://api.github.com/search/code?q=extension:cs+size:>1000"
            ],
            "stackoverflow": [
                "https://api.stackexchange.com/2.3/questions?order=desc&sort=activity&tagged=c%23",
                "https://api.stackexchange.com/2.3/questions?order=desc&sort=votes&tagged=unity3d"
            ]
        }
        
        self.crawl_stats = {
            "total_documents": 0,
            "today_crawled": 0,
            "last_crawl": None,
            "errors": 0,
            "knowledge_updates": 0
        }
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_base (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    title TEXT NOT NULL,
                    content TEXT NOT NULL,
                    category TEXT,
                    difficulty REAL DEFAULT 0.5,
                    quality_score REAL DEFAULT 0.0,
                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    hash TEXT UNIQUE
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS learning_sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_start TIMESTAMP,
                    session_end TIMESTAMP,
                    documents_processed INTEGER,
                    knowledge_gained REAL,
                    model_updates INTEGER
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS code_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern TEXT NOT NULL,
                    usage_count INTEGER DEFAULT 1,
                    success_rate REAL DEFAULT 0.5,
                    context TEXT,
                    learned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
    
    async def crawl_microsoft_docs(self):
        """Microsoft ë¬¸ì„œ í¬ë¡¤ë§"""
        print("ğŸ“š Microsoft C# ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘...")
        
        async with aiohttp.ClientSession() as session:
            for url in self.sources["microsoft_docs"]:
                try:
                    await self._crawl_url(session, url, "microsoft_docs")
                except Exception as e:
                    print(f"âŒ Microsoft ë¬¸ì„œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    self.crawl_stats["errors"] += 1
    
    async def crawl_unity_docs(self):
        """Unity ë¬¸ì„œ í¬ë¡¤ë§"""
        print("ğŸ® Unity ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘...")
        
        async with aiohttp.ClientSession() as session:
            for url in self.sources["unity_docs"]:
                try:
                    await self._crawl_url(session, url, "unity_docs")
                except Exception as e:
                    print(f"âŒ Unity ë¬¸ì„œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    self.crawl_stats["errors"] += 1
    
    async def crawl_github_repos(self):
        """GitHub C# ë¦¬í¬ì§€í† ë¦¬ í¬ë¡¤ë§"""
        print("ğŸ’» GitHub C# ì½”ë“œ í¬ë¡¤ë§ ì‹œì‘...")
        
        async with aiohttp.ClientSession() as session:
            for url in self.sources["github_repos"]:
                try:
                    await self._crawl_github_api(session, url)
                except Exception as e:
                    print(f"âŒ GitHub í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    self.crawl_stats["errors"] += 1
    
    async def crawl_stackoverflow(self):
        """StackOverflow ì§ˆë¬¸/ë‹µë³€ í¬ë¡¤ë§"""
        print("â“ StackOverflow C# ì§ˆë¬¸ í¬ë¡¤ë§ ì‹œì‘...")
        
        async with aiohttp.ClientSession() as session:
            for url in self.sources["stackoverflow"]:
                try:
                    await self._crawl_stackoverflow_api(session, url)
                except Exception as e:
                    print(f"âŒ StackOverflow í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    self.crawl_stats["errors"] += 1
    
    async def _crawl_url(self, session: aiohttp.ClientSession, url: str, source: str):
        """URL í¬ë¡¤ë§"""
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” BeautifulSoup ë“± ì‚¬ìš©)
                    title = f"Document from {url}"
                    
                    # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í•´ì‹œ
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    self._save_knowledge(source, title, content, content_hash)
                    
        except Exception as e:
            print(f"URL í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
    
    async def _crawl_github_api(self, session: aiohttp.ClientSession, url: str):
        """GitHub API í¬ë¡¤ë§"""
        headers = {
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'AutoCI-Learning-Bot'
        }
        
        try:
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if 'items' in data:
                        for item in data['items'][:10]:  # ìƒìœ„ 10ê°œë§Œ
                            repo_name = item.get('full_name', 'Unknown')
                            description = item.get('description', '')
                            
                            # ì½”ë“œ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                            if 'contents_url' in item:
                                await self._crawl_repo_contents(session, item['contents_url'], repo_name)
                            
        except Exception as e:
            print(f"GitHub API í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    async def _crawl_stackoverflow_api(self, session: aiohttp.ClientSession, url: str):
        """StackOverflow API í¬ë¡¤ë§"""
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if 'items' in data:
                        for item in data['items'][:20]:  # ìƒìœ„ 20ê°œ ì§ˆë¬¸
                            title = item.get('title', 'Untitled')
                            body = item.get('body', '')
                            tags = item.get('tags', [])
                            
                            content = f"Title: {title}\nTags: {', '.join(tags)}\nBody: {body}"
                            content_hash = hashlib.md5(content.encode()).hexdigest()
                            
                            self._save_knowledge("stackoverflow", title, content, content_hash)
                            
        except Exception as e:
            print(f"StackOverflow API í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    async def _crawl_repo_contents(self, session: aiohttp.ClientSession, contents_url: str, repo_name: str):
        """ë¦¬í¬ì§€í† ë¦¬ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            # contents_urlì—ì„œ ì‹¤ì œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê°„ì†Œí™”)
            files_url = contents_url.replace('{+path}', '')
            
            async with session.get(files_url) as response:
                if response.status == 200:
                    files = await response.json()
                    
                    for file_info in files[:5]:  # ìƒìœ„ 5ê°œ íŒŒì¼ë§Œ
                        if file_info.get('name', '').endswith('.cs'):
                            file_content = f"Repository: {repo_name}\nFile: {file_info.get('name')}\nPath: {file_info.get('path')}"
                            content_hash = hashlib.md5(file_content.encode()).hexdigest()
                            
                            self._save_knowledge("github", f"{repo_name}/{file_info.get('name')}", file_content, content_hash)
                            
        except Exception as e:
            print(f"ë¦¬í¬ì§€í† ë¦¬ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    def _save_knowledge(self, source: str, title: str, content: str, content_hash: str):
        """ì§€ì‹ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ì¤‘ë³µ ì²´í¬
                existing = conn.execute("SELECT id FROM knowledge_base WHERE hash = ?", (content_hash,)).fetchone()
                
                if not existing:
                    # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                    category = self._classify_content(content)
                    quality_score = self._assess_quality(content)
                    
                    conn.execute("""
                        INSERT INTO knowledge_base (source, title, content, category, quality_score, hash)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (source, title, content, category, quality_score, content_hash))
                    
                    self.crawl_stats["total_documents"] += 1
                    self.crawl_stats["today_crawled"] += 1
                    self.crawl_stats["knowledge_updates"] += 1
                    
                    print(f"âœ… ìƒˆë¡œìš´ ì§€ì‹ ì €ì¥: {title[:50]}...")
                    
        except sqlite3.IntegrityError:
            # ì¤‘ë³µ ë¬¸ì„œ - ë¬´ì‹œ
            pass
        except Exception as e:
            print(f"âŒ ì§€ì‹ ì €ì¥ ì‹¤íŒ¨: {e}")
            self.crawl_stats["errors"] += 1
    
    def _classify_content(self, content: str) -> str:
        """ë‚´ìš© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ['unity', 'gameobject', 'transform', 'monobehaviour']):
            return 'unity'
        elif any(keyword in content_lower for keyword in ['async', 'await', 'task', 'thread']):
            return 'async_programming'
        elif any(keyword in content_lower for keyword in ['class', 'interface', 'inheritance', 'polymorphism']):
            return 'oop'
        elif any(keyword in content_lower for keyword in ['linq', 'query', 'select', 'where']):
            return 'linq'
        elif any(keyword in content_lower for keyword in ['performance', 'optimization', 'memory', 'gc']):
            return 'performance'
        else:
            return 'general'
    
    def _assess_quality(self, content: str) -> float:
        """ë‚´ìš© í’ˆì§ˆ í‰ê°€"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        if len(content) > 1000:
            score += 0.2
        elif len(content) > 500:
            score += 0.1
        
        # ì½”ë“œ ì˜ˆì œ í¬í•¨ ì—¬ë¶€
        if any(keyword in content for keyword in ['```', 'class ', 'public ', 'private ']):
            score += 0.2
        
        # ì„¤ëª… í’ˆì§ˆ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        if any(keyword in content.lower() for keyword in ['example', 'usage', 'how to', 'tutorial']):
            score += 0.1
        
        return min(score, 1.0)
    
    def get_crawl_stats(self) -> Dict:
        """í¬ë¡¤ë§ í†µê³„ ë°˜í™˜"""
        return self.crawl_stats.copy()

class ContinuousLearningAI:
    """24ì‹œê°„ ì—°ì† í•™ìŠµ AI"""
    
    def __init__(self):
        self.crawler = CSharpKnowledgeCrawler()
        self.learning_active = True
        self.learning_thread = None
        self.crawl_thread = None
        
        # í•™ìŠµ ìƒíƒœ
        self.learning_stats = {
            "sessions_completed": 0,
            "total_learning_time": 0,
            "knowledge_base_size": 0,
            "last_update": None,
            "learning_rate": 0.001,
            "model_accuracy": 0.0
        }
        
        # ê°€ìƒì˜ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ (ì‹¤ì œë¡œëŠ” PyTorch/TensorFlow ëª¨ë¸)
        self.model_weights = {
            "korean_language": {},
            "csharp_knowledge": {},
            "unity_expertise": {},
            "conversation_patterns": {}
        }
        
        print("ğŸ§  24ì‹œê°„ ì—°ì† í•™ìŠµ AI ì´ˆê¸°í™”ë¨")
    
    def start_continuous_learning(self):
        """ì—°ì† í•™ìŠµ ì‹œì‘"""
        print("ğŸš€ 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œì‘!")
        
        # í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì •
        schedule.every(1).hours.do(self._run_crawling_cycle)
        schedule.every(30).minutes.do(self._run_learning_cycle)
        schedule.every(6).hours.do(self._save_learning_progress)
        schedule.every().day.at("03:00").do(self._daily_maintenance)
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        self.learning_thread = threading.Thread(target=self._continuous_learning_loop, daemon=True)
        self.crawl_thread = threading.Thread(target=self._continuous_crawling_loop, daemon=True)
        
        self.learning_thread.start()
        self.crawl_thread.start()
        
        print("âœ… ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")
    
    def _continuous_learning_loop(self):
        """ì—°ì† í•™ìŠµ ë£¨í”„"""
        while self.learning_active:
            try:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ ì²´í¬
            except Exception as e:
                print(f"âŒ í•™ìŠµ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
    
    def _continuous_crawling_loop(self):
        """ì—°ì† í¬ë¡¤ë§ ë£¨í”„"""
        while self.learning_active:
            try:
                # ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
                asyncio.run(self._run_async_crawling())
                time.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸°
            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(1800)  # 30ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
    
    async def _run_async_crawling(self):
        """ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰"""
        tasks = [
            self.crawler.crawl_microsoft_docs(),
            self.crawler.crawl_unity_docs(),
            self.crawler.crawl_github_repos(),
            self.crawler.crawl_stackoverflow()
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    def _run_crawling_cycle(self):
        """í¬ë¡¤ë§ ì‚¬ì´í´ ì‹¤í–‰"""
        print("ğŸ”„ ì •ê¸° í¬ë¡¤ë§ ì‚¬ì´í´ ì‹œì‘...")
        start_time = time.time()
        
        try:
            # ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self._run_async_crawling())
            loop.close()
            
            elapsed = time.time() - start_time
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
    
    def _run_learning_cycle(self):
        """í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰"""
        print("ğŸ§  ì •ê¸° í•™ìŠµ ì‚¬ì´í´ ì‹œì‘...")
        start_time = time.time()
        
        try:
            # 1. ìƒˆë¡œìš´ ì§€ì‹ ë¡œë“œ
            new_knowledge = self._load_new_knowledge()
            
            # 2. ëª¨ë¸ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            updates = self._update_model_weights(new_knowledge)
            
            # 3. í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸
            self.learning_stats["sessions_completed"] += 1
            self.learning_stats["total_learning_time"] += time.time() - start_time
            self.learning_stats["knowledge_base_size"] = len(new_knowledge)
            self.learning_stats["last_update"] = datetime.now().isoformat()
            
            print(f"âœ… í•™ìŠµ ì™„ë£Œ: {updates}ê°œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸")
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
    
    def _load_new_knowledge(self) -> List[Dict]:
        """ìƒˆë¡œìš´ ì§€ì‹ ë¡œë“œ"""
        try:
            with sqlite3.connect(self.crawler.db_path) as conn:
                # ìµœê·¼ 1ì‹œê°„ ë‚´ ì¶”ê°€ëœ ì§€ì‹
                one_hour_ago = (datetime.now() - timedelta(hours=1)).isoformat()
                
                cursor = conn.execute("""
                    SELECT title, content, category, quality_score 
                    FROM knowledge_base 
                    WHERE crawled_at > ? 
                    ORDER BY quality_score DESC 
                    LIMIT 100
                """, (one_hour_ago,))
                
                return [{"title": row[0], "content": row[1], "category": row[2], "quality": row[3]} 
                       for row in cursor.fetchall()]
                
        except Exception as e:
            print(f"âŒ ì§€ì‹ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []
    
    def _update_model_weights(self, knowledge_list: List[Dict]) -> int:
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
        updates = 0
        
        for knowledge in knowledge_list:
            category = knowledge["category"]
            quality = knowledge["quality"]
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜
            if category not in self.model_weights["csharp_knowledge"]:
                self.model_weights["csharp_knowledge"][category] = 0.0
            
            # í’ˆì§ˆì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
            self.model_weights["csharp_knowledge"][category] += quality * self.learning_stats["learning_rate"]
            updates += 1
            
            # ëª¨ë¸ ì •í™•ë„ í–¥ìƒ ì‹œë®¬ë ˆì´ì…˜
            self.learning_stats["model_accuracy"] = min(
                self.learning_stats["model_accuracy"] + 0.001,
                1.0
            )
        
        return updates
    
    def _save_learning_progress(self):
        """í•™ìŠµ ì§„í–‰ìƒí™© ì €ì¥"""
        try:
            progress_file = self.crawler.data_dir / "learning_progress.json"
            
            progress_data = {
                "timestamp": datetime.now().isoformat(),
                "learning_stats": self.learning_stats,
                "crawl_stats": self.crawler.get_crawl_stats(),
                "model_weights": self.model_weights
            }
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            print("ğŸ’¾ í•™ìŠµ ì§„í–‰ìƒí™© ì €ì¥ë¨")
            
        except Exception as e:
            print(f"âŒ ì§„í–‰ìƒí™© ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _daily_maintenance(self):
        """ì¼ì¼ ìœ ì§€ë³´ìˆ˜"""
        print("ğŸ”§ ì¼ì¼ ìœ ì§€ë³´ìˆ˜ ì‹œì‘...")
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            with sqlite3.connect(self.crawler.db_path) as conn:
                # ì˜¤ë˜ëœ ì €í’ˆì§ˆ ë°ì´í„° ì‚­ì œ
                conn.execute("""
                    DELETE FROM knowledge_base 
                    WHERE quality_score < 0.3 
                    AND crawled_at < datetime('now', '-7 days')
                """)
                
                # ì¤‘ë³µ ì œê±°
                conn.execute("""
                    DELETE FROM knowledge_base 
                    WHERE id NOT IN (
                        SELECT MIN(id) FROM knowledge_base GROUP BY hash
                    )
                """)
            
            # 2. í†µê³„ ì´ˆê¸°í™”
            self.crawler.crawl_stats["today_crawled"] = 0
            self.crawler.crawl_stats["errors"] = 0
            
            print("âœ… ì¼ì¼ ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ìœ ì§€ë³´ìˆ˜ ì˜¤ë¥˜: {e}")
    
    def get_learning_status(self) -> Dict:
        """í•™ìŠµ ìƒíƒœ ë°˜í™˜"""
        return {
            "learning_active": self.learning_active,
            "learning_stats": self.learning_stats,
            "crawl_stats": self.crawler.get_crawl_stats(),
            "knowledge_categories": list(self.model_weights["csharp_knowledge"].keys()),
            "model_size": sum(len(weights) for weights in self.model_weights.values())
        }
    
    def stop_learning(self):
        """í•™ìŠµ ì¤‘ì§€"""
        print("ğŸ›‘ ì—°ì† í•™ìŠµ ì¤‘ì§€...")
        self.learning_active = False
        
        if self.learning_thread and self.learning_thread.is_alive():
            self.learning_thread.join(timeout=5)
        
        if self.crawl_thread and self.crawl_thread.is_alive():
            self.crawl_thread.join(timeout=5)
        
        self._save_learning_progress()
        print("âœ… ì—°ì† í•™ìŠµ ì¤‘ì§€ë¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì—°ì† í•™ìŠµ AI ì´ˆê¸°í™”
    learning_ai = ContinuousLearningAI()
    
    try:
        # ì—°ì† í•™ìŠµ ì‹œì‘
        learning_ai.start_continuous_learning()
        
        print("\nğŸ“Š ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (Ctrl+Cë¡œ ì¢…ë£Œ)")
        print("-" * 50)
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë£¨í”„
        while True:
            status = learning_ai.get_learning_status()
            
            print(f"\rğŸ§  ì„¸ì…˜: {status['learning_stats']['sessions_completed']} | "
                  f"ğŸ“š ì§€ì‹: {status['crawl_stats']['total_documents']} | "
                  f"ğŸ¯ ì •í™•ë„: {status['learning_stats']['model_accuracy']:.3f} | "
                  f"ğŸ”„ í™œì„±: {'âœ…' if status['learning_active'] else 'âŒ'}", end="")
            
            time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ ì—…ë°ì´íŠ¸
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
    except Exception as e:
        print(f"\nâŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
    finally:
        learning_ai.stop_learning()
        print("ğŸ‘‹ AutoCI ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 