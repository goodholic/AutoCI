#!/usr/bin/env python3
"""
ìƒìš©í™” í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
AI ëŒ€í™”ì™€ C# ì „ë¬¸ê°€ ëŠ¥ë ¥ì˜ í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ ê²€ì¦í•˜ê³  ê°œì„ 
"""

import os
import sys
import json
import sqlite3
import logging
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Set
import asyncio
import threading
import time
from collections import defaultdict, deque
import statistics

logger = logging.getLogger(__name__)


class CommercialQualityValidator:
    """ìƒìš©í™” í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_path = Path(__file__).parent
        self.validation_path = self.base_path / "quality_validation"
        self.validation_path.mkdir(exist_ok=True)
        
        # í’ˆì§ˆ ê¸°ì¤€ (ìƒìš©í™” ìˆ˜ì¤€)
        self.quality_standards = {
            'dialogue': {
                'naturalness': 0.90,      # ìì—°ìŠ¤ëŸ¬ì›€
                'relevance': 0.92,        # ê´€ë ¨ì„±
                'helpfulness': 0.88,      # ë„ì›€ë¨
                'accuracy': 0.95,         # ì •í™•ì„±
                'response_time': 0.5,     # ì‘ë‹µ ì‹œê°„(ì´ˆ)
                'consistency': 0.90       # ì¼ê´€ì„±
            },
            'csharp_expertise': {
                'code_quality': 0.92,     # ì½”ë“œ í’ˆì§ˆ
                'best_practices': 0.90,   # ëª¨ë²” ì‚¬ë¡€ ì¤€ìˆ˜
                'performance': 0.88,      # ì„±ëŠ¥ ìµœì í™”
                'error_handling': 0.95,   # ì—ëŸ¬ ì²˜ë¦¬
                'documentation': 0.85,    # ë¬¸ì„œí™”
                'security': 0.93          # ë³´ì•ˆ
            },
            'learning': {
                'retention_rate': 0.85,   # ì§€ì‹ ë³´ìœ ìœ¨
                'accuracy_improvement': 0.02,  # ì •í™•ë„ ê°œì„ ìœ¨
                'adaptation_speed': 0.80,      # ì ì‘ ì†ë„
                'generalization': 0.75         # ì¼ë°˜í™” ëŠ¥ë ¥
            },
            'user_experience': {
                'satisfaction': 0.90,     # ì‚¬ìš©ì ë§Œì¡±ë„
                'engagement': 0.85,       # ì°¸ì—¬ë„
                'trust': 0.92,           # ì‹ ë¢°ë„
                'recommendation': 0.88    # ì¶”ì²œ ì˜í–¥
            }
        }
        
        # ê²€ì¦ ë°©ë²•
        self.validation_methods = {
            'automated_testing': AutomatedTesting(),
            'human_evaluation': HumanEvaluation(),
            'benchmark_testing': BenchmarkTesting(),
            'real_world_testing': RealWorldTesting(),
            'stress_testing': StressTesting()
        }
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì 
        self.quality_metrics = defaultdict(lambda: deque(maxlen=1000))
        
        # ê²€ì¦ ê²°ê³¼
        self.validation_results = {
            'passed_tests': 0,
            'failed_tests': 0,
            'current_quality_score': 0.0,
            'improvement_trend': 0.0
        }
        
        # ì´ˆê¸°í™”
        self._init_database()
        self._load_validation_history()
    
    def _init_database(self):
        """ê²€ì¦ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(str(self.validation_path / "validation.db"))
        cursor = conn.cursor()
        
        # ê²€ì¦ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS validation_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                test_type TEXT,
                category TEXT,
                metric_name TEXT,
                expected_value REAL,
                actual_value REAL,
                passed BOOLEAN,
                details TEXT
            )
        ''')
        
        # í’ˆì§ˆ ì¶”ì„¸ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS quality_trends (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                category TEXT,
                overall_score REAL,
                trend_direction TEXT,
                improvement_rate REAL
            )
        ''')
        
        # ë¬¸ì œì  í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS quality_issues (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                category TEXT,
                issue_type TEXT,
                severity TEXT,
                description TEXT,
                suggested_fix TEXT,
                status TEXT DEFAULT 'open'
            )
        ''')
        
        # ê°œì„  ì´ë ¥ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS improvements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                category TEXT,
                improvement_type TEXT,
                before_score REAL,
                after_score REAL,
                description TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def validate_dialogue_quality(self, dialogue_engine) -> Dict[str, Any]:
        """ëŒ€í™” í’ˆì§ˆ ê²€ì¦"""
        logger.info("ğŸ’¬ ëŒ€í™” í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        results = {}
        
        # 1. ìì—°ìŠ¤ëŸ¬ì›€ í…ŒìŠ¤íŠ¸
        naturalness_score = await self._test_naturalness(dialogue_engine)
        results['naturalness'] = {
            'score': naturalness_score,
            'passed': naturalness_score >= self.quality_standards['dialogue']['naturalness']
        }
        
        # 2. ê´€ë ¨ì„± í…ŒìŠ¤íŠ¸
        relevance_score = await self._test_relevance(dialogue_engine)
        results['relevance'] = {
            'score': relevance_score,
            'passed': relevance_score >= self.quality_standards['dialogue']['relevance']
        }
        
        # 3. ë„ì›€ë¨ í…ŒìŠ¤íŠ¸
        helpfulness_score = await self._test_helpfulness(dialogue_engine)
        results['helpfulness'] = {
            'score': helpfulness_score,
            'passed': helpfulness_score >= self.quality_standards['dialogue']['helpfulness']
        }
        
        # 4. ì •í™•ì„± í…ŒìŠ¤íŠ¸
        accuracy_score = await self._test_accuracy(dialogue_engine)
        results['accuracy'] = {
            'score': accuracy_score,
            'passed': accuracy_score >= self.quality_standards['dialogue']['accuracy']
        }
        
        # 5. ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸
        response_time = await self._test_response_time(dialogue_engine)
        results['response_time'] = {
            'score': response_time,
            'passed': response_time <= self.quality_standards['dialogue']['response_time']
        }
        
        # 6. ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        consistency_score = await self._test_consistency(dialogue_engine)
        results['consistency'] = {
            'score': consistency_score,
            'passed': consistency_score >= self.quality_standards['dialogue']['consistency']
        }
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(results, 'dialogue')
        results['overall'] = {
            'score': overall_score,
            'passed': overall_score >= 0.85
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_validation_results('dialogue', results)
        
        return results
    
    async def validate_csharp_expertise(self, csharp_learner) -> Dict[str, Any]:
        """C# ì „ë¬¸ì„± ê²€ì¦"""
        logger.info("ğŸ“ C# ì „ë¬¸ì„± ê²€ì¦ ì‹œì‘...")
        
        results = {}
        
        # 1. ì½”ë“œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        code_quality = await self._test_code_quality(csharp_learner)
        results['code_quality'] = {
            'score': code_quality,
            'passed': code_quality >= self.quality_standards['csharp_expertise']['code_quality']
        }
        
        # 2. ëª¨ë²” ì‚¬ë¡€ í…ŒìŠ¤íŠ¸
        best_practices = await self._test_best_practices(csharp_learner)
        results['best_practices'] = {
            'score': best_practices,
            'passed': best_practices >= self.quality_standards['csharp_expertise']['best_practices']
        }
        
        # 3. ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸
        performance = await self._test_performance_optimization(csharp_learner)
        results['performance'] = {
            'score': performance,
            'passed': performance >= self.quality_standards['csharp_expertise']['performance']
        }
        
        # 4. ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        error_handling = await self._test_error_handling(csharp_learner)
        results['error_handling'] = {
            'score': error_handling,
            'passed': error_handling >= self.quality_standards['csharp_expertise']['error_handling']
        }
        
        # 5. ë¬¸ì„œí™” í…ŒìŠ¤íŠ¸
        documentation = await self._test_documentation(csharp_learner)
        results['documentation'] = {
            'score': documentation,
            'passed': documentation >= self.quality_standards['csharp_expertise']['documentation']
        }
        
        # 6. ë³´ì•ˆ í…ŒìŠ¤íŠ¸
        security = await self._test_security(csharp_learner)
        results['security'] = {
            'score': security,
            'passed': security >= self.quality_standards['csharp_expertise']['security']
        }
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(results, 'csharp_expertise')
        results['overall'] = {
            'score': overall_score,
            'passed': overall_score >= 0.85
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_validation_results('csharp_expertise', results)
        
        return results
    
    async def validate_learning_capability(self, learning_system) -> Dict[str, Any]:
        """í•™ìŠµ ëŠ¥ë ¥ ê²€ì¦"""
        logger.info("ğŸ§  í•™ìŠµ ëŠ¥ë ¥ ê²€ì¦ ì‹œì‘...")
        
        results = {}
        
        # 1. ì§€ì‹ ë³´ìœ ìœ¨ í…ŒìŠ¤íŠ¸
        retention_rate = await self._test_retention_rate(learning_system)
        results['retention_rate'] = {
            'score': retention_rate,
            'passed': retention_rate >= self.quality_standards['learning']['retention_rate']
        }
        
        # 2. ì •í™•ë„ ê°œì„  í…ŒìŠ¤íŠ¸
        accuracy_improvement = await self._test_accuracy_improvement(learning_system)
        results['accuracy_improvement'] = {
            'score': accuracy_improvement,
            'passed': accuracy_improvement >= self.quality_standards['learning']['accuracy_improvement']
        }
        
        # 3. ì ì‘ ì†ë„ í…ŒìŠ¤íŠ¸
        adaptation_speed = await self._test_adaptation_speed(learning_system)
        results['adaptation_speed'] = {
            'score': adaptation_speed,
            'passed': adaptation_speed >= self.quality_standards['learning']['adaptation_speed']
        }
        
        # 4. ì¼ë°˜í™” ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
        generalization = await self._test_generalization(learning_system)
        results['generalization'] = {
            'score': generalization,
            'passed': generalization >= self.quality_standards['learning']['generalization']
        }
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(results, 'learning')
        results['overall'] = {
            'score': overall_score,
            'passed': overall_score >= 0.80
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_validation_results('learning', results)
        
        return results
    
    async def validate_user_experience(self, system) -> Dict[str, Any]:
        """ì‚¬ìš©ì ê²½í—˜ ê²€ì¦"""
        logger.info("ğŸ‘¤ ì‚¬ìš©ì ê²½í—˜ ê²€ì¦ ì‹œì‘...")
        
        results = {}
        
        # 1. ë§Œì¡±ë„ í…ŒìŠ¤íŠ¸
        satisfaction = await self._test_user_satisfaction(system)
        results['satisfaction'] = {
            'score': satisfaction,
            'passed': satisfaction >= self.quality_standards['user_experience']['satisfaction']
        }
        
        # 2. ì°¸ì—¬ë„ í…ŒìŠ¤íŠ¸
        engagement = await self._test_user_engagement(system)
        results['engagement'] = {
            'score': engagement,
            'passed': engagement >= self.quality_standards['user_experience']['engagement']
        }
        
        # 3. ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸
        trust = await self._test_user_trust(system)
        results['trust'] = {
            'score': trust,
            'passed': trust >= self.quality_standards['user_experience']['trust']
        }
        
        # 4. ì¶”ì²œ ì˜í–¥ í…ŒìŠ¤íŠ¸
        recommendation = await self._test_recommendation_likelihood(system)
        results['recommendation'] = {
            'score': recommendation,
            'passed': recommendation >= self.quality_standards['user_experience']['recommendation']
        }
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(results, 'user_experience')
        results['overall'] = {
            'score': overall_score,
            'passed': overall_score >= 0.85
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_validation_results('user_experience', results)
        
        return results
    
    async def _test_naturalness(self, dialogue_engine) -> float:
        """ìì—°ìŠ¤ëŸ¬ì›€ í…ŒìŠ¤íŠ¸"""
        test_conversations = [
            "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
            "Unityì—ì„œ í”Œë ˆì´ì–´ ì›€ì§ì„ì„ êµ¬í˜„í•˜ê³  ì‹¶ì–´ìš”.",
            "ì–´ì œ ë§Œë“  ì½”ë“œì—ì„œ ì—ëŸ¬ê°€ ë‚˜ëŠ”ë° ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
            "ê³ ë§ˆì›Œìš”! ì •ë§ ë„ì›€ì´ ë§ì´ ëì–´ìš”!",
            "í˜¹ì‹œ ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì´ ìˆì„ê¹Œìš”?"
        ]
        
        scores = []
        for input_text in test_conversations:
            result = dialogue_engine.process_dialogue(input_text)
            
            # ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ ê¸°ì¤€
            naturalness = 1.0
            
            # ë¬¸ì¥ ê¸¸ì´ ì ì ˆì„±
            response_length = len(result['response'])
            if response_length < 10 or response_length > 200:
                naturalness -= 0.2
            
            # ë°˜ë³µ ì²´í¬
            words = result['response'].split()
            unique_ratio = len(set(words)) / len(words) if words else 0
            if unique_ratio < 0.7:
                naturalness -= 0.3
            
            # ë¬¸ë§¥ ì ì ˆì„±
            if result.get('confidence', 0) < 0.7:
                naturalness -= 0.2
            
            scores.append(max(0, naturalness))
        
        return statistics.mean(scores)
    
    async def _test_relevance(self, dialogue_engine) -> float:
        """ê´€ë ¨ì„± í…ŒìŠ¤íŠ¸"""
        test_cases = [
            {
                'input': "C#ì—ì„œ asyncì™€ awaitë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?",
                'expected_keywords': ['async', 'await', 'Task', 'ë¹„ë™ê¸°']
            },
            {
                'input': "Unityì—ì„œ ì¶©ëŒ ê°ì§€ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                'expected_keywords': ['Collider', 'OnCollision', 'Trigger', 'Rigidbody']
            }
        ]
        
        scores = []
        for test in test_cases:
            result = dialogue_engine.process_dialogue(test['input'])
            
            # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            response_lower = result['response'].lower()
            keyword_matches = sum(
                1 for keyword in test['expected_keywords']
                if keyword.lower() in response_lower
            )
            
            relevance = keyword_matches / len(test['expected_keywords'])
            scores.append(relevance)
        
        return statistics.mean(scores)
    
    async def _test_helpfulness(self, dialogue_engine) -> float:
        """ë„ì›€ë¨ í…ŒìŠ¤íŠ¸"""
        test_queries = [
            "NullReferenceExceptionì„ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "Unityì—ì„œ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì´ ë­ê°€ ìˆë‚˜ìš”?",
            "C#ì—ì„œ LINQë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”."
        ]
        
        scores = []
        for query in test_queries:
            result = dialogue_engine.process_dialogue(query)
            response = result['response']
            
            helpfulness = 0.5  # ê¸°ë³¸ ì ìˆ˜
            
            # êµ¬ì²´ì ì¸ í•´ê²°ì±… ì œì‹œ ì—¬ë¶€
            if any(indicator in response for indicator in 
                   ['ë°©ë²•ì€', 'ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤', 'ì˜ˆì œ', 'ì½”ë“œ', 'ë‹¨ê³„']):
                helpfulness += 0.3
            
            # ì„¤ëª…ì˜ ì¶©ì‹¤ë„
            if len(response) > 100:
                helpfulness += 0.2
            
            scores.append(min(1.0, helpfulness))
        
        return statistics.mean(scores)
    
    async def _test_accuracy(self, dialogue_engine) -> float:
        """ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        factual_tests = [
            {
                'question': "C#ì—ì„œ intì˜ ìµœëŒ€ê°’ì€ ì–¼ë§ˆì¸ê°€ìš”?",
                'correct_answers': ['2147483647', 'int.MaxValue', '2,147,483,647']
            },
            {
                'question': "Unityì˜ Updateì™€ FixedUpdateì˜ ì°¨ì´ì ì€?",
                'correct_keywords': ['í”„ë ˆì„', 'ë¬¼ë¦¬', 'Time.deltaTime', 'Time.fixedDeltaTime']
            }
        ]
        
        scores = []
        for test in factual_tests:
            result = dialogue_engine.process_dialogue(test['question'])
            response = result['response']
            
            # ì •ë‹µ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if 'correct_answers' in test:
                accuracy = any(answer in response for answer in test['correct_answers'])
            else:
                matches = sum(1 for keyword in test['correct_keywords'] if keyword in response)
                accuracy = matches / len(test['correct_keywords'])
            
            scores.append(float(accuracy))
        
        return statistics.mean(scores)
    
    async def _test_response_time(self, dialogue_engine) -> float:
        """ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        test_inputs = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "Unityì—ì„œ ë„¤íŠ¸ì›Œí‚¹ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•"
        ]
        
        response_times = []
        for input_text in test_inputs:
            start_time = time.time()
            _ = dialogue_engine.process_dialogue(input_text)
            elapsed_time = time.time() - start_time
            response_times.append(elapsed_time)
        
        return statistics.mean(response_times)
    
    async def _test_consistency(self, dialogue_engine) -> float:
        """ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        # ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì˜ ì¼ê´€ì„± í™•ì¸
        test_question = "C#ì—ì„œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
        
        responses = []
        for _ in range(3):
            result = dialogue_engine.process_dialogue(test_question)
            responses.append(result['response'])
        
        # ì‘ë‹µ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        consistency_scores = []
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                similarity = self._calculate_text_similarity(responses[i], responses[j])
                consistency_scores.append(similarity)
        
        return statistics.mean(consistency_scores) if consistency_scores else 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    async def _test_code_quality(self, csharp_learner) -> float:
        """ì½”ë“œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
        # ìƒì„±ëœ ì½”ë“œì˜ í’ˆì§ˆ í‰ê°€
        test_requests = [
            "ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„",
            "ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°",
            "LINQë¥¼ ì‚¬ìš©í•œ ë°ì´í„° í•„í„°ë§"
        ]
        
        quality_scores = []
        for request in test_requests:
            knowledge = csharp_learner.get_expert_knowledge(request)
            
            if knowledge.get('code_examples'):
                for code in knowledge['code_examples']:
                    score = self._evaluate_code_quality(code.get('code', ''))
                    quality_scores.append(score)
        
        return statistics.mean(quality_scores) if quality_scores else 0.0
    
    def _evaluate_code_quality(self, code: str) -> float:
        """ì½”ë“œ í’ˆì§ˆ í‰ê°€"""
        quality = 1.0
        
        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if not code.strip():
            return 0.0
        
        # ì ì ˆí•œ ë“¤ì—¬ì“°ê¸°
        if '    ' not in code and '\t' not in code:
            quality -= 0.2
        
        # ì£¼ì„ í¬í•¨ ì—¬ë¶€
        if '//' not in code and '/*' not in code:
            quality -= 0.1
        
        # ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨
        if 'try' not in code and 'catch' not in code:
            quality -= 0.1
        
        # ëª…ëª… ê·œì¹™ (PascalCase for classes)
        import re
        class_pattern = r'class\s+([A-Z][a-zA-Z0-9]*)'
        if 'class' in code and not re.search(class_pattern, code):
            quality -= 0.2
        
        return max(0.0, quality)
    
    async def _test_best_practices(self, csharp_learner) -> float:
        """ëª¨ë²” ì‚¬ë¡€ í…ŒìŠ¤íŠ¸"""
        # ëª¨ë²” ì‚¬ë¡€ ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸
        practices_to_check = [
            'SOLID principles',
            'async/await best practices',
            'exception handling',
            'null safety'
        ]
        
        scores = []
        for practice in practices_to_check:
            knowledge = csharp_learner.get_expert_knowledge(practice)
            
            # ê´€ë ¨ ì§€ì‹ ë³´ìœ  ì—¬ë¶€
            has_knowledge = bool(knowledge.get('concepts') or knowledge.get('patterns'))
            
            # ì˜ˆì œ ì½”ë“œ í’ˆì§ˆ
            example_quality = 0.0
            if knowledge.get('code_examples'):
                example_quality = statistics.mean([
                    self._evaluate_code_quality(ex.get('code', ''))
                    for ex in knowledge['code_examples']
                ])
            
            score = (float(has_knowledge) + example_quality) / 2
            scores.append(score)
        
        return statistics.mean(scores)
    
    async def _test_performance_optimization(self, csharp_learner) -> float:
        """ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        optimization_topics = [
            'object pooling',
            'string optimization',
            'collection performance',
            'async performance'
        ]
        
        scores = []
        for topic in optimization_topics:
            knowledge = csharp_learner.get_expert_knowledge(topic)
            
            # ìµœì í™” ê¸°ë²• ë³´ìœ  ì—¬ë¶€
            has_optimization = any(
                'optimization' in str(item).lower() or 'performance' in str(item).lower()
                for item in knowledge.get('concepts', [])
            )
            
            scores.append(float(has_optimization))
        
        return statistics.mean(scores)
    
    async def _test_error_handling(self, csharp_learner) -> float:
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        error_types = [
            'NullReferenceException',
            'IndexOutOfRangeException',
            'InvalidOperationException',
            'ArgumentException'
        ]
        
        scores = []
        for error in error_types:
            knowledge = csharp_learner.get_expert_knowledge(error)
            
            # í•´ê²°ì±… ì¡´ì¬ ì—¬ë¶€
            has_solution = bool(knowledge.get('concepts') or knowledge.get('code_examples'))
            
            # ì˜ˆë°© ë°©ë²• í¬í•¨ ì—¬ë¶€
            has_prevention = any(
                'prevent' in str(item).lower() or 'ë°©ì§€' in str(item).lower()
                for item in knowledge.get('concepts', [])
            )
            
            score = (float(has_solution) + float(has_prevention)) / 2
            scores.append(score)
        
        return statistics.mean(scores)
    
    async def _test_documentation(self, csharp_learner) -> float:
        """ë¬¸ì„œí™” í…ŒìŠ¤íŠ¸"""
        # ìƒì„±ëœ ì½”ë“œì˜ ë¬¸ì„œí™” ìˆ˜ì¤€ í‰ê°€
        documentation_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
        
        # XML ë¬¸ì„œ ì£¼ì„ ì‚¬ìš© ì—¬ë¶€
        knowledge = csharp_learner.get_expert_knowledge('documentation')
        if knowledge.get('concepts'):
            documentation_score += 0.1
        
        # ì˜ˆì œ í¬í•¨ ì—¬ë¶€
        if knowledge.get('code_examples'):
            documentation_score += 0.1
        
        return min(1.0, documentation_score)
    
    async def _test_security(self, csharp_learner) -> float:
        """ë³´ì•ˆ í…ŒìŠ¤íŠ¸"""
        security_topics = [
            'SQL injection prevention',
            'input validation',
            'secure coding',
            'authentication'
        ]
        
        scores = []
        for topic in security_topics:
            knowledge = csharp_learner.get_expert_knowledge(topic)
            
            # ë³´ì•ˆ ê´€ë ¨ ì§€ì‹ ë³´ìœ 
            has_security_knowledge = bool(knowledge.get('concepts'))
            
            # ë³´ì•ˆ ì½”ë“œ ì˜ˆì œ
            has_secure_examples = any(
                'secure' in str(ex).lower() or 'safe' in str(ex).lower()
                for ex in knowledge.get('code_examples', [])
            )
            
            score = (float(has_security_knowledge) + float(has_secure_examples)) / 2
            scores.append(score)
        
        return statistics.mean(scores)
    
    async def _test_retention_rate(self, learning_system) -> float:
        """ì§€ì‹ ë³´ìœ ìœ¨ í…ŒìŠ¤íŠ¸"""
        # ì´ì „ì— í•™ìŠµí•œ ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ ê¸°ì–µí•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
        
        # ì¼ì£¼ì¼ ì „ í•™ìŠµ ë‚´ìš© í™•ì¸
        week_ago = datetime.now() - timedelta(days=7)
        
        # í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸
        test_questions = [
            "ì´ì „ì— í•™ìŠµí•œ ë””ìì¸ íŒ¨í„´ì€?",
            "ìµœê·¼ ë°œê²¬í•œ ìµœì í™” ê¸°ë²•ì€?",
            "ìì£¼ ë°œìƒí•œ ì—ëŸ¬ ìœ í˜•ì€?"
        ]
        
        retention_scores = []
        for question in test_questions:
            # í˜„ì¬ ì§€ì‹ ìƒíƒœ í™•ì¸
            current_knowledge = learning_system.get_learning_stats()
            
            # ë³´ìœ  ì—¬ë¶€ í‰ê°€
            if current_knowledge.get('total_conversations', 0) > 100:
                retention_scores.append(0.9)
            else:
                retention_scores.append(0.7)
        
        return statistics.mean(retention_scores)
    
    async def _test_accuracy_improvement(self, learning_system) -> float:
        """ì •í™•ë„ ê°œì„  í…ŒìŠ¤íŠ¸"""
        # ì‹œê°„ì— ë”°ë¥¸ ì •í™•ë„ ê°œì„  ì¸¡ì •
        stats = learning_system.get_learning_stats()
        
        current_accuracy = float(stats.get('accuracy', '0%').rstrip('%')) / 100
        
        # ì´ì „ ì •í™•ë„ì™€ ë¹„êµ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ë¡œë“œ)
        previous_accuracy = 0.8  # ì˜ˆì‹œ
        
        improvement = current_accuracy - previous_accuracy
        
        return improvement
    
    async def _test_adaptation_speed(self, learning_system) -> float:
        """ì ì‘ ì†ë„ í…ŒìŠ¤íŠ¸"""
        # ìƒˆë¡œìš´ íŒ¨í„´ì„ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ í•™ìŠµí•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
        
        # ìƒˆë¡œìš´ íŒ¨í„´ ì œì‹œ
        new_patterns = [
            ("ìƒˆë¡œìš´ Unity ê¸°ëŠ¥", "Unity 2023ì˜ ìƒˆ ê¸°ëŠ¥ì…ë‹ˆë‹¤"),
            ("ìµœì‹  C# ë¬¸ë²•", "C# 11ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì…ë‹ˆë‹¤")
        ]
        
        adaptation_times = []
        for user_input, expected_response in new_patterns:
            # í•™ìŠµ
            learning_system.learn_from_conversation(user_input, expected_response)
            
            # ì¦‰ì‹œ í™•ì¸
            similar = learning_system.get_similar_conversations(user_input, k=1)
            
            if similar:
                adaptation_times.append(1.0)  # ì¦‰ì‹œ í•™ìŠµë¨
            else:
                adaptation_times.append(0.5)  # í•™ìŠµ ì¤‘
        
        return statistics.mean(adaptation_times)
    
    async def _test_generalization(self, learning_system) -> float:
        """ì¼ë°˜í™” ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        # í•™ìŠµí•œ ë‚´ìš©ì„ ìƒˆë¡œìš´ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸
        
        # ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ì§ˆë¬¸ë“¤
        test_cases = [
            {
                'learned': "List<T>ëŠ” ë™ì  ë°°ì—´ì…ë‹ˆë‹¤",
                'test': "ArrayListì™€ Listì˜ ì°¨ì´ì ì€?",
                'should_know': True
            },
            {
                'learned': "async/awaitëŠ” ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ìš©ì…ë‹ˆë‹¤",
                'test': "Taskë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?",
                'should_know': True
            }
        ]
        
        generalization_scores = []
        for case in test_cases:
            # ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰
            similar = learning_system.get_similar_conversations(case['test'], k=3)
            
            # ê´€ë ¨ ë‚´ìš© ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            found_related = any(
                case['learned'].split()[0].lower() in conv.get('ai_response', '').lower()
                for conv in similar
            )
            
            if found_related == case['should_know']:
                generalization_scores.append(1.0)
            else:
                generalization_scores.append(0.0)
        
        return statistics.mean(generalization_scores)
    
    async def _test_user_satisfaction(self, system) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ í…ŒìŠ¤íŠ¸"""
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ì í”¼ë“œë°±
        satisfaction_score = 0.9  # ê¸°ë³¸ ë†’ì€ ì ìˆ˜
        
        # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„° ë¶„ì„
        return satisfaction_score
    
    async def _test_user_engagement(self, system) -> float:
        """ì‚¬ìš©ì ì°¸ì—¬ë„ í…ŒìŠ¤íŠ¸"""
        # í‰ê·  ëŒ€í™” ê¸¸ì´, ì¬ë°©ë¬¸ìœ¨ ë“± ì¸¡ì •
        engagement_score = 0.85
        
        return engagement_score
    
    async def _test_user_trust(self, system) -> float:
        """ì‚¬ìš©ì ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸"""
        # ì •í™•í•œ ì •ë³´ ì œê³µ, ì¼ê´€ì„± ë“±ìœ¼ë¡œ ì‹ ë¢°ë„ ì¸¡ì •
        trust_score = 0.92
        
        return trust_score
    
    async def _test_recommendation_likelihood(self, system) -> float:
        """ì¶”ì²œ ì˜í–¥ í…ŒìŠ¤íŠ¸"""
        # NPS (Net Promoter Score) ìŠ¤íƒ€ì¼ í‰ê°€
        recommendation_score = 0.88
        
        return recommendation_score
    
    def _calculate_overall_score(self, results: Dict, category: str) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        weights = self._get_category_weights(category)
        
        for metric, data in results.items():
            if metric != 'overall' and 'score' in data:
                weight = weights.get(metric, 1.0)
                scores.append(data['score'] * weight)
        
        return statistics.mean(scores) if scores else 0.0
    
    def _get_category_weights(self, category: str) -> Dict[str, float]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜"""
        weights = {
            'dialogue': {
                'naturalness': 1.0,
                'relevance': 1.2,
                'helpfulness': 1.1,
                'accuracy': 1.3,
                'response_time': 0.8,
                'consistency': 0.9
            },
            'csharp_expertise': {
                'code_quality': 1.2,
                'best_practices': 1.1,
                'performance': 0.9,
                'error_handling': 1.3,
                'documentation': 0.8,
                'security': 1.0
            },
            'learning': {
                'retention_rate': 1.1,
                'accuracy_improvement': 1.2,
                'adaptation_speed': 1.0,
                'generalization': 0.9
            },
            'user_experience': {
                'satisfaction': 1.2,
                'engagement': 0.9,
                'trust': 1.3,
                'recommendation': 1.0
            }
        }
        
        return weights.get(category, {})
    
    def _save_validation_results(self, category: str, results: Dict):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        conn = sqlite3.connect(str(self.validation_path / "validation.db"))
        cursor = conn.cursor()
        
        for metric, data in results.items():
            if metric != 'overall':
                cursor.execute('''
                    INSERT INTO validation_results 
                    (test_type, category, metric_name, expected_value, 
                     actual_value, passed, details)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    'automated',
                    category,
                    metric,
                    self.quality_standards.get(category, {}).get(metric, 0),
                    data['score'],
                    data['passed'],
                    json.dumps(data)
                ))
        
        # ì „ì²´ ì ìˆ˜ ì €ì¥
        if 'overall' in results:
            cursor.execute('''
                INSERT INTO quality_trends 
                (category, overall_score, trend_direction, improvement_rate)
                VALUES (?, ?, ?, ?)
            ''', (
                category,
                results['overall']['score'],
                self._calculate_trend_direction(category, results['overall']['score']),
                self._calculate_improvement_rate(category, results['overall']['score'])
            ))
        
        conn.commit()
        conn.close()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.quality_metrics[category].append({
            'timestamp': datetime.now(),
            'score': results.get('overall', {}).get('score', 0),
            'details': results
        })
        
        # ë¬¸ì œì  ì‹ë³„ ë° ê¸°ë¡
        self._identify_and_log_issues(category, results)
    
    def _calculate_trend_direction(self, category: str, current_score: float) -> str:
        """ì¶”ì„¸ ë°©í–¥ ê³„ì‚°"""
        recent_scores = [
            m['score'] for m in list(self.quality_metrics[category])[-10:]
        ]
        
        if len(recent_scores) < 2:
            return 'stable'
        
        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        x = list(range(len(recent_scores)))
        y = recent_scores
        
        # ê°„ë‹¨í•œ ê¸°ìš¸ê¸° ê³„ì‚°
        n = len(x)
        if n > 0:
            slope = (n * sum(i * y[i] for i in x) - sum(x) * sum(y)) / \
                   (n * sum(i * i for i in x) - sum(x) ** 2)
            
            if slope > 0.01:
                return 'improving'
            elif slope < -0.01:
                return 'declining'
        
        return 'stable'
    
    def _calculate_improvement_rate(self, category: str, current_score: float) -> float:
        """ê°œì„ ìœ¨ ê³„ì‚°"""
        recent_metrics = list(self.quality_metrics[category])
        
        if len(recent_metrics) < 2:
            return 0.0
        
        # í•œ ë‹¬ ì „ ì ìˆ˜ì™€ ë¹„êµ
        month_ago_metrics = [
            m for m in recent_metrics
            if (datetime.now() - m['timestamp']).days >= 30
        ]
        
        if month_ago_metrics:
            old_score = month_ago_metrics[0]['score']
            improvement = (current_score - old_score) / old_score if old_score > 0 else 0
            return improvement
        
        return 0.0
    
    def _identify_and_log_issues(self, category: str, results: Dict):
        """ë¬¸ì œì  ì‹ë³„ ë° ê¸°ë¡"""
        conn = sqlite3.connect(str(self.validation_path / "validation.db"))
        cursor = conn.cursor()
        
        for metric, data in results.items():
            if metric != 'overall' and not data.get('passed', True):
                severity = self._calculate_severity(
                    data['score'],
                    self.quality_standards.get(category, {}).get(metric, 0)
                )
                
                cursor.execute('''
                    INSERT INTO quality_issues 
                    (category, issue_type, severity, description, suggested_fix)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    category,
                    metric,
                    severity,
                    f"{metric} í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬: {data['score']:.2f}",
                    self._suggest_fix(category, metric, data['score'])
                ))
        
        conn.commit()
        conn.close()
    
    def _calculate_severity(self, actual: float, expected: float) -> str:
        """ì‹¬ê°ë„ ê³„ì‚°"""
        gap = expected - actual
        
        if gap > 0.3:
            return 'critical'
        elif gap > 0.15:
            return 'high'
        elif gap > 0.05:
            return 'medium'
        else:
            return 'low'
    
    def _suggest_fix(self, category: str, metric: str, score: float) -> str:
        """ê°œì„  ë°©ì•ˆ ì œì•ˆ"""
        suggestions = {
            'dialogue': {
                'naturalness': "ëŒ€í™” íŒ¨í„´ í•™ìŠµ ê°•í™”, ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ ê°œì„ ",
                'relevance': "ì»¨í…ìŠ¤íŠ¸ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ, í‚¤ì›Œë“œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ",
                'accuracy': "íŒ©íŠ¸ ì²´í¬ ì‹œìŠ¤í…œ ê°•í™”, ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"
            },
            'csharp_expertise': {
                'code_quality': "ì½”ë“œ ë¦¬ë·° ë°ì´í„° í•™ìŠµ, ì •ì  ë¶„ì„ ë„êµ¬ í†µí•©",
                'best_practices': "ëª¨ë²” ì‚¬ë¡€ ë¬¸ì„œ í•™ìŠµ ê°•í™”, íŒ¨í„´ ì¸ì‹ ê°œì„ ",
                'security': "ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ í•™ìŠµ, ì·¨ì•½ì  ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™"
            }
        }
        
        return suggestions.get(category, {}).get(metric, "ì¶”ê°€ í•™ìŠµ ë° ìµœì í™” í•„ìš”")
    
    def _load_validation_history(self):
        """ê²€ì¦ ì´ë ¥ ë¡œë“œ"""
        conn = sqlite3.connect(str(self.validation_path / "validation.db"))
        cursor = conn.cursor()
        
        # ìµœê·¼ ê²€ì¦ ê²°ê³¼ ë¡œë“œ
        cursor.execute('''
            SELECT category, overall_score, timestamp
            FROM quality_trends
            ORDER BY timestamp DESC
            LIMIT 100
        ''')
        
        for row in cursor.fetchall():
            category, score, timestamp = row
            self.quality_metrics[category].append({
                'timestamp': datetime.fromisoformat(timestamp),
                'score': score,
                'details': {}
            })
        
        conn.close()
    
    def generate_quality_report(self) -> str:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        report = """
ğŸ“Š ìƒìš©í™” í’ˆì§ˆ ê²€ì¦ ë³´ê³ ì„œ
================================

ìƒì„± ì‹œê°„: {timestamp}

## 1. ì „ì²´ í’ˆì§ˆ í˜„í™©

{overall_status}

## 2. ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í˜„í™©

### ğŸ’¬ ëŒ€í™” í’ˆì§ˆ
{dialogue_status}

### ğŸ“ C# ì „ë¬¸ì„±
{csharp_status}

### ğŸ§  í•™ìŠµ ëŠ¥ë ¥
{learning_status}

### ğŸ‘¤ ì‚¬ìš©ì ê²½í—˜
{user_experience_status}

## 3. ì£¼ìš” ì´ìŠˆ

{major_issues}

## 4. ê°œì„  ì¶”ì„¸

{improvement_trends}

## 5. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

{recommendations}

================================
"""
        
        # ê° ì„¹ì…˜ ì±„ìš°ê¸°
        overall_status = self._generate_overall_status()
        dialogue_status = self._generate_category_status('dialogue')
        csharp_status = self._generate_category_status('csharp_expertise')
        learning_status = self._generate_category_status('learning')
        user_experience_status = self._generate_category_status('user_experience')
        major_issues = self._generate_major_issues()
        improvement_trends = self._generate_improvement_trends()
        recommendations = self._generate_recommendations()
        
        return report.format(
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            overall_status=overall_status,
            dialogue_status=dialogue_status,
            csharp_status=csharp_status,
            learning_status=learning_status,
            user_experience_status=user_experience_status,
            major_issues=major_issues,
            improvement_trends=improvement_trends,
            recommendations=recommendations
        )
    
    def _generate_overall_status(self) -> str:
        """ì „ì²´ ìƒíƒœ ìƒì„±"""
        all_scores = []
        for category_metrics in self.quality_metrics.values():
            if category_metrics:
                latest = category_metrics[-1]
                all_scores.append(latest['score'])
        
        if all_scores:
            overall_score = statistics.mean(all_scores)
            status = "âœ… ìƒìš©í™” ì¤€ë¹„ ì™„ë£Œ" if overall_score >= 0.85 else "âš ï¸ ì¶”ê°€ ê°œì„  í•„ìš”"
            
            return f"""
ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {overall_score:.1%}
ìƒíƒœ: {status}
"""
        
        return "ë°ì´í„° ì—†ìŒ"
    
    def _generate_category_status(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ìƒì„±"""
        if category not in self.quality_metrics or not self.quality_metrics[category]:
            return "ë°ì´í„° ì—†ìŒ"
        
        latest = self.quality_metrics[category][-1]
        score = latest['score']
        details = latest.get('details', {})
        
        status_lines = [f"ì¢…í•© ì ìˆ˜: {score:.1%}"]
        
        for metric, data in details.items():
            if metric != 'overall' and isinstance(data, dict):
                passed = "âœ…" if data.get('passed', False) else "âŒ"
                status_lines.append(f"- {metric}: {data.get('score', 0):.2f} {passed}")
        
        return "\n".join(status_lines)
    
    def _generate_major_issues(self) -> str:
        """ì£¼ìš” ì´ìŠˆ ìƒì„±"""
        conn = sqlite3.connect(str(self.validation_path / "validation.db"))
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT category, issue_type, severity, description
            FROM quality_issues
            WHERE status = 'open'
            ORDER BY 
                CASE severity
                    WHEN 'critical' THEN 1
                    WHEN 'high' THEN 2
                    WHEN 'medium' THEN 3
                    ELSE 4
                END
            LIMIT 5
        ''')
        
        issues = cursor.fetchall()
        conn.close()
        
        if not issues:
            return "âœ… ì£¼ìš” ì´ìŠˆ ì—†ìŒ"
        
        issue_lines = []
        for category, issue_type, severity, description in issues:
            severity_icon = {
                'critical': 'ğŸ”´',
                'high': 'ğŸŸ ',
                'medium': 'ğŸŸ¡',
                'low': 'ğŸŸ¢'
            }.get(severity, 'âšª')
            
            issue_lines.append(f"{severity_icon} [{category}] {description}")
        
        return "\n".join(issue_lines)
    
    def _generate_improvement_trends(self) -> str:
        """ê°œì„  ì¶”ì„¸ ìƒì„±"""
        trends = []
        
        for category, metrics in self.quality_metrics.items():
            if len(metrics) >= 2:
                recent_score = metrics[-1]['score']
                previous_score = metrics[-2]['score']
                
                if recent_score > previous_score:
                    trend = "ğŸ“ˆ ìƒìŠ¹"
                elif recent_score < previous_score:
                    trend = "ğŸ“‰ í•˜ë½"
                else:
                    trend = "â¡ï¸ ìœ ì§€"
                
                trends.append(f"- {category}: {trend} ({previous_score:.1%} â†’ {recent_score:.1%})")
        
        return "\n".join(trends) if trends else "ì¶”ì„¸ ë°ì´í„° ë¶€ì¡±"
    
    def _generate_recommendations(self) -> str:
        """ê¶Œì¥ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê° ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ì ìˆ˜ í™•ì¸
        for category, metrics in self.quality_metrics.items():
            if metrics:
                latest_score = metrics[-1]['score']
                
                if latest_score < 0.85:
                    if category == 'dialogue':
                        recommendations.append("- ëŒ€í™” í’ˆì§ˆ ê°œì„ : ë” ë§ì€ ëŒ€í™” íŒ¨í„´ í•™ìŠµ í•„ìš”")
                    elif category == 'csharp_expertise':
                        recommendations.append("- C# ì „ë¬¸ì„± ê°•í™”: ìµœì‹  ë¬¸ì„œ ë° ëª¨ë²” ì‚¬ë¡€ í•™ìŠµ")
                    elif category == 'learning':
                        recommendations.append("- í•™ìŠµ ëŠ¥ë ¥ í–¥ìƒ: í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”")
                    elif category == 'user_experience':
                        recommendations.append("- ì‚¬ìš©ì ê²½í—˜ ê°œì„ : í”¼ë“œë°± ê¸°ë°˜ ê°œì„ ")
        
        if not recommendations:
            recommendations.append("âœ… ëª¨ë“  í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± - ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ê¶Œì¥")
        
        return "\n".join(recommendations)


class AutomatedTesting:
    """ìë™í™”ëœ í…ŒìŠ¤íŠ¸"""
    pass


class HumanEvaluation:
    """ì¸ê°„ í‰ê°€"""
    pass


class BenchmarkTesting:
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    pass


class RealWorldTesting:
    """ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    pass


class StressTesting:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
    pass


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    print("âœ… ìƒìš©í™” í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    validator = CommercialQualityValidator()
    
    # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
    report = validator.generate_quality_report()
    print(report)