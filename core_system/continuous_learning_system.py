#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ - ì €ì‚¬ì–‘ ìµœì í™” ë²„ì „
C#ê³¼ í•œê¸€ì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
Llama-3.1-8B, CodeLlama-13B, Qwen2.5-Coder-32B ëª¨ë¸ì„ í™œìš©

RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ì— ìµœì í™”ë¨
autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
Cross-platform support for Windows and WSL
"""

import os
import sys
import json
import time
import random
import asyncio
import logging
import gc
import psutil
import platform
from datetime import datetime, timedelta
from pathlib import Path

# Platform-specific path setup
def get_project_root():
    """Get project root path based on platform"""
    if platform.system() == "Windows":
        # Windows: use script's parent directory
        return Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    else:
        # WSL/Linux: use configured path
        return Path("/mnt/d/AutoCI/AutoCI")

# Add project paths
PROJECT_ROOT = get_project_root()
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'modules'))
sys.path.insert(0, str(PROJECT_ROOT / 'modules_active'))
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM
)
try:
    from transformers.utils.quantization_config import BitsAndBytesConfig
except ImportError:
    try:
        from transformers import BitsAndBytesConfig
    except ImportError:
        BitsAndBytesConfig = None
        
try:
    from transformers.pipelines import pipeline
except ImportError:
    try:
        from transformers import pipeline
    except ImportError:
        pipeline = None

# ì •ë³´ ìˆ˜ì§‘ê¸° ì„í¬íŠ¸
try:
    from modules.intelligent_information_gatherer import get_information_gatherer
    INFORMATION_GATHERER_AVAILABLE = True
    print("ğŸŒ ì§€ëŠ¥í˜• ì •ë³´ ìˆ˜ì§‘ê¸° í™œì„±í™”!")
except ImportError:
    INFORMATION_GATHERER_AVAILABLE = False
    print("âš ï¸ ì •ë³´ ìˆ˜ì§‘ê¸°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from modules.ai_model_controller import AIModelController
    MODEL_CONTROLLER_AVAILABLE = True
    print("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ í™œì„±í™”!")
except ImportError:
    MODEL_CONTROLLER_AVAILABLE = False
    print("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ë¡œê¹… ì„¤ì •
log_file = PROJECT_ROOT / 'continuous_learning.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(str(log_file), encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from modules.pytorch_deep_learning_module import AutoCIPyTorchLearningSystem
    PYTORCH_AVAILABLE = True
    print("ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆ í™œì„±í™”!")
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ ë¹Œë” ì„í¬íŠ¸
try:
    from modules.instruction_response_dataset_builder import get_dataset_builder
    DATASET_BUILDER_AVAILABLE = True
    print("ğŸ“š ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ ë¹Œë” í™œì„±í™”!")
except ImportError:
    DATASET_BUILDER_AVAILABLE = False
    print("âš ï¸ ë°ì´í„°ì…‹ ë¹Œë”ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# í•™ìŠµ í’ˆì§ˆ ëª¨ë‹ˆí„° ì„í¬íŠ¸
try:
    from modules.learning_quality_monitor import get_quality_monitor
    QUALITY_MONITOR_AVAILABLE = True
    print("ğŸ“Š í•™ìŠµ í’ˆì§ˆ ëª¨ë‹ˆí„° í™œì„±í™”!")
except ImportError:
    QUALITY_MONITOR_AVAILABLE = False
    print("âš ï¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# Hugging Face í† í°
HF_TOKEN = "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"

# PyTorch í’ˆì§ˆ í‰ê°€ ëª¨ë¸
class QualityAssessmentModel(nn.Module):
    """ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸"""
    def __init__(self, input_dim=768, hidden_dim=256):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU()
        )
        self.quality_head = nn.Linear(64, 1)
        
    def forward(self, x):
        features = self.encoder(x)
        quality_score = torch.sigmoid(self.quality_head(features))
        return quality_score

# Experience Replay Buffer
class ExperienceReplayBuffer:
    """í•™ìŠµ ê²½í—˜ì„ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©í•˜ëŠ” ë²„í¼"""
    def __init__(self, capacity=10000, prioritized=True):
        self.buffer = deque(maxlen=capacity)
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.prioritized = prioritized
        self.alpha = 0.6  # prioritization ê°•ë„
        self.beta = 0.4   # importance sampling
        self.epsilon = 1e-6
        self.pos = 0
        
    def add(self, experience, td_error=None):
        """ê²½í—˜ ì¶”ê°€"""
        max_priority = self.priorities.max() if self.buffer else 1.0
        
        if len(self.buffer) < len(self.priorities):
            self.buffer.append(experience)
        else:
            self.buffer[self.pos] = experience
            
        # Priority ì„¤ì •
        priority = max_priority if td_error is None else (abs(td_error) + self.epsilon) ** self.alpha
        self.priorities[self.pos] = priority
        
        self.pos = (self.pos + 1) % len(self.priorities)
        
    def sample(self, batch_size):
        """ë°°ì¹˜ ìƒ˜í”Œë§"""
        if len(self.buffer) == 0:
            return None, None, None
            
        if self.prioritized:
            # Priority-based sampling
            probs = self.priorities[:len(self.buffer)]
            probs /= probs.sum()
            
            indices = np.random.choice(len(self.buffer), batch_size, p=probs)
            samples = [self.buffer[idx] for idx in indices]
            
            # Importance sampling weights
            weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
            weights /= weights.max()
            
            return samples, weights, indices
        else:
            # Uniform sampling
            indices = np.random.choice(len(self.buffer), batch_size)
            samples = [self.buffer[idx] for idx in indices]
            return samples, np.ones(batch_size), indices
            
    def update_priorities(self, indices, td_errors):
        """Priority ì—…ë°ì´íŠ¸"""
        for idx, td_error in zip(indices, td_errors):
            priority = (abs(td_error) + self.epsilon) ** self.alpha
            self.priorities[idx] = priority
            
    def __len__(self):
        return len(self.buffer)

# ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬
class MultiTaskLearningNetwork(nn.Module):
    """5ê°€ì§€ í•µì‹¬ ì£¼ì œì— íŠ¹í™”ëœ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬"""
    def __init__(self, input_dim=768, hidden_dim=512):
        super().__init__()
        # ê³µìœ  ì¸ì½”ë”
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # íƒœìŠ¤í¬ë³„ ì „ë¬¸í™” í—¤ë“œ
        self.task_heads = nn.ModuleDict({
            'csharp': nn.Sequential(
                nn.Linear(hidden_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            ),
            'korean': nn.Sequential(
                nn.Linear(hidden_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            ),
            'godot': nn.Sequential(
                nn.Linear(hidden_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            ),
            'socketio': nn.Sequential(
                nn.Linear(hidden_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            ),
            'ai_optimization': nn.Sequential(
                nn.Linear(hidden_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 128)
            )
        })
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = nn.Linear(128, 1)
        
    def forward(self, x, task):
        # ê³µìœ  íŠ¹ì§• ì¶”ì¶œ
        shared_features = self.shared_encoder(x)
        
        # íƒœìŠ¤í¬ë³„ íŠ¹ì§• ì¶”ì¶œ
        if task in self.task_heads:
            task_features = self.task_heads[task](shared_features)
        else:
            # ê¸°ë³¸ íƒœìŠ¤í¬ (ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬)
            task_features = self.task_heads['godot'](shared_features)
            
        # ìµœì¢… ì¶œë ¥
        output = self.output_layer(task_features)
        return torch.sigmoid(output)

# ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ
class RealtimeFeedbackSystem:
    """ë‹µë³€ ìƒì„± ì¤‘ ì‹¤ì‹œê°„ìœ¼ë¡œ í’ˆì§ˆì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ê°œì„ """
    def __init__(self, quality_threshold=0.7):
        self.quality_threshold = quality_threshold
        self.feedback_history = deque(maxlen=100)
        self.improvement_strategies = {
            'low_quality': self._improve_low_quality,
            'incomplete': self._improve_incomplete,
            'off_topic': self._improve_off_topic,
            'language_mix': self._improve_language_mix
        }
        
    def analyze_partial_response(self, partial_response, expected_language='korean'):
        """ë¶€ë¶„ ì‘ë‹µ ë¶„ì„"""
        issues = []
        
        # í’ˆì§ˆ ì²´í¬
        if len(partial_response) < 50:
            issues.append('incomplete')
            
        # ì–¸ì–´ í˜¼ìš© ì²´í¬
        has_korean = any('\u3131' <= char <= '\u3163' or '\uac00' <= char <= '\ud7a3' for char in partial_response)
        has_english = any('a' <= char.lower() <= 'z' for char in partial_response)
        
        if expected_language == 'korean' and has_english and not has_korean:
            issues.append('language_mix')
        elif expected_language == 'english' and has_korean:
            issues.append('language_mix')
            
        # ì½”ë“œ ì™„ì„±ë„ ì²´í¬
        if '```' in partial_response and partial_response.count('```') % 2 != 0:
            issues.append('incomplete')
            
        return issues
        
    def _improve_low_quality(self, response, context):
        """ë‚®ì€ í’ˆì§ˆ ê°œì„ """
        prompt_addon = "\n\në” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”."
        return prompt_addon
        
    def _improve_incomplete(self, response, context):
        """ë¶ˆì™„ì „í•œ ë‹µë³€ ê°œì„ """
        prompt_addon = "\n\në‹µë³€ì„ ì™„ì„±í•´ì£¼ì„¸ìš”. ì½”ë“œ ì˜ˆì œê°€ ìˆë‹¤ë©´ ì™„ì „í•œ í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
        return prompt_addon
        
    def _improve_off_topic(self, response, context):
        """ì£¼ì œ ë²—ì–´ë‚¨ ê°œì„ """
        prompt_addon = f"\n\n{context.get('topic', 'ì£¼ì œ')}ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        return prompt_addon
        
    def _improve_language_mix(self, response, context):
        """ì–¸ì–´ í˜¼ìš© ê°œì„ """
        expected_lang = context.get('language', 'korean')
        if expected_lang == 'korean':
            prompt_addon = "\n\ní•œêµ­ì–´ë¡œ ì¼ê´€ë˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        else:
            prompt_addon = "\n\nPlease answer consistently in English."
        return prompt_addon
        
    def get_improvement_prompt(self, issues, response, context):
        """ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = []
        for issue in issues:
            if issue in self.improvement_strategies:
                prompt = self.improvement_strategies[issue](response, context)
                prompts.append(prompt)
                
        return " ".join(prompts)
        
    def record_feedback(self, response, quality_score, issues):
        """í”¼ë“œë°± ê¸°ë¡"""
        self.feedback_history.append({
            'timestamp': datetime.now().isoformat(),
            'quality_score': quality_score,
            'issues': issues,
            'response_length': len(response)
        })

@dataclass
class LearningTopic:
    """í•™ìŠµ ì£¼ì œ"""
    id: str
    category: str
    topic: str
    difficulty: int  # 1-5
    korean_keywords: List[str]
    python_concepts: List[str]
    godot_integration: Optional[str] = None
    
@dataclass
class LearningSession:
    """í•™ìŠµ ì„¸ì…˜"""
    session_id: str
    start_time: datetime
    topics_covered: List[str]
    questions_asked: int
    successful_answers: int
    models_used: Dict[str, int]
    knowledge_gained: List[Dict[str, Any]]
    
class ContinuousLearningSystem:
    def __init__(self, models_dir: str = None, learning_dir: str = None, max_memory_gb: float = 32.0):
        # Platform-specific default paths
        if models_dir is None:
            models_dir = str(PROJECT_ROOT / "models")
        if learning_dir is None:
            learning_dir = str(PROJECT_ROOT / "continuous_learning")
        
        self.models_dir = Path(models_dir)
        self.learning_dir = Path(learning_dir)
        self.learning_dir.mkdir(exist_ok=True, parents=True)
        
        # ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if MODEL_CONTROLLER_AVAILABLE:
            self.model_controller = AIModelController()
            print("ğŸ¯ AI ëª¨ë¸ ì¡°ì¢…ê¶Œ í™•ë³´ ì™„ë£Œ!")
            logger.info("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            self.model_controller = None
            logger.warning("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • (ì €ì‚¬ì–‘ ìµœì í™”)
        self.max_memory_gb = max_memory_gb
        self.memory_threshold = 0.70  # 70% ì‚¬ìš© ì‹œ ëª¨ë¸ ì–¸ë¡œë“œ (ë” ë³´ìˆ˜ì )
        self.currently_loaded_model = None
        self.model_cache = {}  # ì–¸ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì €ì¥
        
        # ì‹¤íŒ¨í•œ ëª¨ë¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (í˜„ì¬ ì„¸ì…˜ ë™ì•ˆë§Œ ìœ íš¨)
        self.failed_models_blacklist = set()  # ë¡œë”© ì‹¤íŒ¨í•œ ëª¨ë¸ ì¶”ì 
        
        # ğŸ†• PyTorch ê¸°ë°˜ ê°œì„  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ”§ PyTorch ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í’ˆì§ˆ í‰ê°€ ëª¨ë¸
        self.quality_assessor = QualityAssessmentModel().to(self.device)
        self.quality_optimizer = torch.optim.Adam(self.quality_assessor.parameters(), lr=0.001)
        
        # Experience Replay Buffer
        self.experience_buffer = ExperienceReplayBuffer(capacity=10000, prioritized=True)
        logger.info("ğŸ“¦ Experience Replay Buffer ì´ˆê¸°í™” ì™„ë£Œ (ìš©ëŸ‰: 10,000)")
        
        # ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬
        self.multitask_network = MultiTaskLearningNetwork().to(self.device)
        self.multitask_optimizer = torch.optim.Adam(self.multitask_network.parameters(), lr=0.0003)
        
        # ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ
        self.feedback_system = RealtimeFeedbackSystem(quality_threshold=0.7)
        logger.info("ğŸ”„ ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ í™œì„±í™”")
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        self.quality_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.quality_optimizer, mode='max', patience=10, factor=0.5
        )
        self.multitask_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.multitask_optimizer, mode='max', patience=10, factor=0.5
        )
        
        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        self.questions_dir = self.learning_dir / "questions"
        self.answers_dir = self.learning_dir / "answers"
        self.knowledge_base_dir = self.learning_dir / "knowledge_base"
        self.progress_dir = self.learning_dir / "progress"
        
        for dir in [self.questions_dir, self.answers_dir, self.knowledge_base_dir, self.progress_dir]:
            dir.mkdir(exist_ok=True)
            
        # ëª¨ë¸ ì •ë³´ (ì‹¤ì œ ë¡œë”©ì€ í•„ìš”í•  ë•Œë§Œ)
        self.available_models = {}
        self.load_model_info()
        
        # í•™ìŠµ ì£¼ì œ ì •ì˜
        self.learning_topics = self._initialize_learning_topics()
        
        # í˜„ì¬ ì„¸ì…˜
        self.current_session = None
        
        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_base = self._load_knowledge_base()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        self.memory_usage_history = []
        
        # í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ
        self.learning_progress = self._load_learning_progress()
        
        # ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì
        try:
            from modules.progressive_learning_manager import ProgressiveLearningManager
            self.progressive_manager = ProgressiveLearningManager(self.learning_dir)
            logger.info("ğŸ“ˆ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì í™œì„±í™”")
        except:
            self.progressive_manager = None
            logger.warning("âš ï¸ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì¶”ì  ì‹œìŠ¤í…œ
        self.question_history = self._load_question_history()
        
        # ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ ë¹Œë”
        self.dataset_builder = None
        if DATASET_BUILDER_AVAILABLE:
            self.dataset_builder = get_dataset_builder()
            logger.info("ğŸ“š ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ ë¹Œë” ì—°ê²°ë¨")
        
        # í•™ìŠµ í’ˆì§ˆ ëª¨ë‹ˆí„°
        self.quality_monitor = None
        if QUALITY_MONITOR_AVAILABLE:
            self.quality_monitor = get_quality_monitor()
            logger.info("ğŸ“Š í•™ìŠµ í’ˆì§ˆ ëª¨ë‹ˆí„° ì—°ê²°ë¨")
        self.question_similarity_threshold = 0.85  # ìœ ì‚¬ë„ ì„ê³„ê°’
        self.max_history_size = 10000  # ìµœëŒ€ íˆìŠ¤í† ë¦¬ í¬ê¸°
        
        # PyTorch ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if PYTORCH_AVAILABLE:
            try:
                self.pytorch_system = AutoCIPyTorchLearningSystem(base_path=str(Path(__file__).parent.parent))
                logger.info("ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                
                # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
                pytorch_models_dir = Path(self.models_dir).parent / "pytorch_models"
                if pytorch_models_dir.exists():
                    latest_dl_model = sorted(pytorch_models_dir.glob("deep_learning_model_*.pth"))
                    latest_rl_model = sorted(pytorch_models_dir.glob("rl_agent_*.pth"))
                    
                    if latest_dl_model:
                        self.pytorch_system.load_models(dl_path=str(latest_dl_model[-1]))
                    if latest_rl_model:
                        self.pytorch_system.load_models(rl_path=str(latest_rl_model[-1]))
                        
            except Exception as e:
                self.pytorch_system = None
                logger.error(f"PyTorch ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        else:
            self.pytorch_system = None
            
        # ì €ì¥ëœ PyTorch ëª¨ë¸ ë¡œë“œ
        self.load_pytorch_models()
        
    def _initialize_learning_topics(self) -> List[LearningTopic]:
        """5ê°€ì§€ í•µì‹¬ í•™ìŠµ ì£¼ì œ ì´ˆê¸°í™” (DeepSeek-coder ìµœì í™”)"""
        topics = [
            # 1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° (ë³€í˜•ëœ Godotìš©)
            LearningTopic("csharp_basics", "C# í”„ë¡œê·¸ë˜ë°", "C# ê¸°ì´ˆ ë¬¸ë²•", 1,
                         ["ë³€ìˆ˜", "ë°ì´í„°íƒ€ì…", "ë©”ì„œë“œ", "í´ë˜ìŠ¤"],
                         ["variable", "datatype", "method", "class"],
                         "Godot C# ê¸°ì´ˆ"),
            LearningTopic("csharp_advanced", "C# í”„ë¡œê·¸ë˜ë°", "C# ê³ ê¸‰ ê¸°ëŠ¥", 3,
                         ["ë¸ë¦¬ê²Œì´íŠ¸", "ì´ë²¤íŠ¸", "LINQ", "async/await"],
                         ["delegate", "event", "LINQ", "async"],
                         "Godot C# ê³ ê¸‰ ê¸°ëŠ¥"),
            # 2ï¸âƒ£ í•œê¸€ í”„ë¡œê·¸ë˜ë° ìš©ì–´
            LearningTopic("korean_terms_basic", "í•œê¸€ ìš©ì–´", "í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ìš©ì–´", 1,
                         ["ë³€ìˆ˜", "í•¨ìˆ˜", "í´ë˜ìŠ¤", "ê°ì²´", "ìƒì†"],
                         ["variable", "function", "class", "object", "inheritance"],
                         "í•œ-ì˜ ìš©ì–´ ë§¤í•‘"),
            LearningTopic("korean_terms_advanced", "í•œê¸€ ìš©ì–´", "ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ìš©ì–´", 3,
                         ["ë‹¤í˜•ì„±", "ìº¡ìŠí™”", "ì¶”ìƒí™”", "ì¸í„°í˜ì´ìŠ¤"],
                         ["polymorphism", "encapsulation", "abstraction", "interface"],
                         "ì „ë¬¸ ìš©ì–´ ì´í•´"),
            # 3ï¸âƒ£ ë³€í˜•ëœ Godot ì—”ì§„
            LearningTopic("godot_basics", "ë³€í˜•ëœ Godot", "Godot ê¸°ì´ˆ", 2,
                         ["ë…¸ë“œ", "ì”¬", "ì‹œê·¸ë„", "ìŠ¤í¬ë¦½íŠ¸"],
                         ["Node", "Scene", "Signal", "Script"],
                         "Godot ê¸°ë³¸ êµ¬ì¡°"),
            LearningTopic("godot_advanced", "ë³€í˜•ëœ Godot", "Godot ê³ ê¸‰", 4,
                         ["ì»¤ìŠ¤í…€ë…¸ë“œ", "ì…°ì´ë”", "ë¬¼ë¦¬ì—”ì§„", "ìµœì í™”"],
                         ["CustomNode", "Shader", "Physics2D/3D", "Optimization"],
                         "Godot í™•ì¥ ê°œë°œ"),
            # 4ï¸âƒ£ Socket.IO ë„¤íŠ¸ì›Œí‚¹
            LearningTopic("socketio_basic", "Socket.IO", "ì‹¤ì‹œê°„ í†µì‹  ê¸°ì´ˆ", 3,
                         ["ì†Œì¼“", "ì´ë²¤íŠ¸", "ë£¸", "ë„¤ì„ìŠ¤í˜ì´ìŠ¤"],
                         ["Socket", "Event", "Room", "Namespace"],
                         "Socket.IO ê¸°ë³¸ í†µì‹ "),
            LearningTopic("socketio_advanced", "Socket.IO", "ê³ ê¸‰ ì‹¤ì‹œê°„ í†µì‹ ", 5,
                         ["ë¸Œë¡œë“œìºìŠ¤íŠ¸", "ë¯¸ë“¤ì›¨ì–´", "í´ëŸ¬ìŠ¤í„°ë§", "Redis"],
                         ["Broadcast", "Middleware", "Clustering", "Redis"],
                         "Socket.IO ê³ ê¸‰ ê¸°ëŠ¥"),
            # 5ï¸âƒ£ AI ìµœì í™”
            LearningTopic("ai_optimization_basic", "AI ìµœì í™”", "AI ì½”ë“œ ìƒì„± ê¸°ì´ˆ", 3,
                         ["í”„ë¡¬í”„íŠ¸", "ì»¨í…ìŠ¤íŠ¸", "í† í°", "ì‘ë‹µ"],
                         ["Prompt", "Context", "Token", "Response"],
                         "AI ê¸°ë°˜ ì½”ë“œ ìƒì„±"),
            LearningTopic("ai_optimization_advanced", "AI ìµœì í™”", "AI ê³ ê¸‰ ìµœì í™”", 5,
                         ["íŒŒì¸íŠœë‹", "í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§", "ì»¨í…ìŠ¤íŠ¸ê´€ë¦¬", "ì²´ì´ë‹"],
                         ["FineTuning", "PromptEngineering", "ContextManagement", "Chaining"],
                         "AI ì„±ëŠ¥ ìµœì í™”"),
            # 6ï¸âƒ£ ë³€í˜•ëœ Godot ì „ë¬¸ê°€ í•™ìŠµ
            LearningTopic("godot_expert_architecture", "Godot ì „ë¬¸ê°€", "ë³€í˜•ëœ Godot ì•„í‚¤í…ì²˜", 5,
                         ["ì»¤ìŠ¤í…€ì—”ì§„", "ë Œë”íŒŒì´í”„ë¼ì¸", "ì”¬ì‹œìŠ¤í…œ", "ë¦¬ì†ŒìŠ¤ê´€ë¦¬"],
                         ["CustomEngine", "RenderPipeline", "SceneSystem", "ResourceManager"],
                         "Godot í•µì‹¬ êµ¬ì¡°"),
            LearningTopic("godot_expert_csharp", "Godot ì „ë¬¸ê°€", "C# ê³ ê¸‰ í†µí•©", 5,
                         ["GDExtension", "NativeCall", "ë©”ëª¨ë¦¬ê´€ë¦¬", "ì„±ëŠ¥ìµœì í™”"],
                         ["GDExtension", "NativeCall", "MemoryManagement", "Performance"],
                         "C# ê³ ê¸‰ ê²Œì„ ê°œë°œ"),
            # 7ï¸âƒ£ Godot ì—”ì§„ ì¡°ì‘ (ê°€ìƒ ì…ë ¥)
            LearningTopic("godot_manipulation_basic", "Godot ì¡°ì‘", "ê¸°ë³¸ ì—ë””í„° ì¡°ì‘", 2,
                         ["ë…¸ë“œìƒì„±", "ì”¬êµ¬ì„±", "ì†ì„±ì„¤ì •", "ìŠ¤í¬ë¦½íŠ¸ì—°ê²°"],
                         ["NodeCreation", "SceneSetup", "PropertyConfig", "ScriptAttach"],
                         "ì—ë””í„° ê¸°ë³¸ ì¡°ì‘"),
            LearningTopic("godot_manipulation_advanced", "Godot ì¡°ì‘", "ê³ ê¸‰ ìë™í™” ì¡°ì‘", 4,
                         ["ë³µì¡í•œì”¬êµ¬ì„±", "ì• ë‹ˆë©”ì´ì…˜ì„¤ì •", "ë¬¼ë¦¬ì„¤ì •", "ìµœì í™”ì‘ì—…"],
                         ["ComplexScene", "AnimationSetup", "PhysicsConfig", "Optimization"],
                         "ìë™í™” ì›Œí¬í”Œë¡œìš°"),
        ]
        
        # ëª¨ë“  ì£¼ì œì— DeepSeek-coder ìš°ì„  íƒœê·¸ ì¶”ê°€
        for topic in topics:
            topic.godot_integration = f"[DeepSeek ìš°ì„ ] {topic.godot_integration}"
            
        return topics
        
    def _load_knowledge_base(self) -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "csharp_patterns": {},
            "korean_translations": {},
            "godot_integrations": {},
            "common_errors": {},
            "best_practices": {}
        }
        
    def _save_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, indent=2, ensure_ascii=False)
    
    def _load_question_history(self) -> List[Dict[str, Any]]:
        """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        history_file = self.learning_dir / "question_history.json"
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return []
        return []
    
    def _save_question_history(self):
        """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì €ì¥"""
        history_file = self.learning_dir / "question_history.json"
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.question_history) > self.max_history_size:
            self.question_history = self.question_history[-self.max_history_size:]
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.question_history, f, indent=2, ensure_ascii=False)
    
    def _calculate_question_similarity(self, q1: str, q2: str) -> float:
        """ë‘ ì§ˆë¬¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ìì¹´ë“œ ìœ ì‚¬ë„)"""
        # í•œê¸€ê³¼ ì˜ì–´ ëª¨ë‘ ì²˜ë¦¬
        import re
        
        # ë‹¨ì–´ ì¶”ì¶œ (í•œê¸€, ì˜ì–´, ìˆ«ì)
        pattern = r'[\wê°€-í£]+'
        words1 = set(re.findall(pattern, q1.lower()))
        words2 = set(re.findall(pattern, q2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        # ìì¹´ë“œ ìœ ì‚¬ë„
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _is_question_too_similar(self, new_question: str) -> bool:
        """ìƒˆ ì§ˆë¬¸ì´ ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ë„ˆë¬´ ìœ ì‚¬í•œì§€ í™•ì¸"""
        # ìµœê·¼ 100ê°œ ì§ˆë¬¸ë§Œ ë¹„êµ (ì„±ëŠ¥ ìµœì í™”)
        recent_questions = self.question_history[-100:] if len(self.question_history) > 100 else self.question_history
        
        for hist_q in recent_questions:
            similarity = self._calculate_question_similarity(new_question, hist_q.get('question', ''))
            if similarity > self.question_similarity_threshold:
                return True
        return False
    
    def _add_to_question_history(self, question: Dict[str, Any]):
        """ì§ˆë¬¸ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.question_history.append({
            'question': question['question'],
            'type': question['type'],
            'topic': question['topic'],
            'timestamp': datetime.now().isoformat(),
            'keywords': question.get('keywords', [])
        })
        self._save_question_history()
            
    def _load_learning_progress(self) -> Dict[str, Any]:
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        progress_file = self.progress_dir / "learning_progress.json"
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
                logger.info(f"ê¸°ì¡´ í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {progress.get('total_hours', 0)}ì‹œê°„")
                return progress
        return {
            "total_hours": 0,
            "total_questions": 0,
            "total_successful": 0,
            "topics_progress": {},
            "last_session_id": None,
            "last_save_time": None,
            "sessions_completed": []
        }
        
    def _save_learning_progress(self):
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ì €ì¥"""
        if self.current_session:
            # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
            session_duration = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
            
            self.learning_progress["total_hours"] += session_duration
            self.learning_progress["total_questions"] += self.current_session.questions_asked
            self.learning_progress["total_successful"] += self.current_session.successful_answers
            self.learning_progress["last_session_id"] = self.current_session.session_id
            self.learning_progress["last_save_time"] = datetime.now().isoformat()
            
            # ì£¼ì œë³„ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            if self.current_session.topics_covered:
                for topic in self.current_session.topics_covered:
                    if topic not in self.learning_progress["topics_progress"]:
                        self.learning_progress["topics_progress"][topic] = {
                            "questions_asked": 0,
                            "successful_answers": 0,
                            "last_studied": None
                        }
                    self.learning_progress["topics_progress"][topic]["questions_asked"] += 1
                    self.learning_progress["topics_progress"][topic]["last_studied"] = datetime.now().isoformat()
        
        progress_file = self.progress_dir / "learning_progress.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.learning_progress, f, indent=2, ensure_ascii=False)
        logger.info(f"í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {self.learning_progress['total_hours']:.1f}ì‹œê°„")
            
    def load_model_info(self):
        """ëª¨ë¸ ì •ë³´ë§Œ ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ì€ í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)"""
        models_info_file = self.models_dir / "installed_models.json"
        if not models_info_file.exists():
            logger.error("ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. install_llm_models.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        with open(models_info_file, 'r', encoding='utf-8') as f:
            installed_models = json.load(f)
            
        self.available_models = installed_models
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.available_models.keys())}")
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB)"""
        return psutil.virtual_memory().used / (1024**3)
        
    def get_memory_usage_percent(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜ (%)"""
        return psutil.virtual_memory().percent
        
    def check_memory_safety(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì•ˆì „í•œì§€ í™•ì¸"""
        current_usage = self.get_memory_usage()
        
        # í˜„ì¬ ì‚¬ìš©ëŸ‰ì´ ìµœëŒ€ í—ˆìš©ëŸ‰ì˜ 85%ë¥¼ ë„˜ìœ¼ë©´ ìœ„í—˜
        return current_usage < (self.max_memory_gb * self.memory_threshold)
        
    def unload_current_model(self):
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.currently_loaded_model:
            logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ {self.currently_loaded_model} ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.currently_loaded_model = None
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
    def load_model(self, model_name: str) -> bool:
        """í•„ìš” ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if model_name == self.currently_loaded_model:
            return True  # ì´ë¯¸ ë¡œë“œë¨
            
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬
        if model_name in self.failed_models_blacklist:
            logger.warning(f"ğŸš« {model_name}ì€(ëŠ”) ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆìŠµë‹ˆë‹¤. ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
            
        if model_name not in self.available_models:
            logger.error(f"ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False
            
        # ë©”ëª¨ë¦¬ í™•ì¸ í›„ í•„ìš”ì‹œ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
        if not self.check_memory_safety() and self.currently_loaded_model:
            self.unload_current_model()
            
        try:
            print(f"ğŸ”„ {model_name} ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            logger.info(f"{model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            info = self.available_models[model_name]
            model_id = info['model_id']
            print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_id}")
            
            # RTX 2080 8GB ìµœì í™” ëª¨ë¸ë§Œ í—ˆìš©
            rtx_2080_optimized = {
                "bitnet-b1.58-2b": {"max_vram": 1, "device": "cpu"},
                "gemma-4b": {"max_vram": 4, "device": "cuda:0"},
                "phi3-mini": {"max_vram": 6, "device": "cuda:0"},
                "deepseek-coder-7b": {"max_vram": 6, "device": "cuda:0"},
                "mistral-7b": {"max_vram": 7, "device": "cuda:0"}
            }
            
            if model_name not in rtx_2080_optimized:
                logger.warning(f"âŒ {model_name}ì€ RTX 2080 8GBì— ìµœì í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {', '.join(rtx_2080_optimized.keys())}")
                logger.info("ğŸ’¡ install_llm_models_rtx2080.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì í™”ëœ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
                return False
            
            model_config = rtx_2080_optimized[model_name]
            logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {model_name} (VRAM: {model_config['max_vram']}GB)")
            
            # AutoTokenizerì™€ AutoModelForCausalLMì„ ì§ì ‘ ì‚¬ìš©
            from transformers import AutoTokenizer, AutoModelForCausalLM
            try:
                from transformers import BitsAndBytesConfig
            except ImportError:
                from transformers.utils.quantization_config import BitsAndBytesConfig
            
            hf_token = os.getenv('HF_TOKEN', None)
            
            # ëª¨ë¸ë³„ ìµœì  ì„¤ì •
            device_map = model_config['device']
            torch_dtype = torch.float32 if device_map == "cpu" else torch.float16
            quantization_config = None
            
            # 4bit ì–‘ìí™” ì„¤ì • (GPU ëª¨ë¸ìš©)
            if device_map != "cpu" and info.get('quantization') == '4bit':
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ìš© í† í¬ë‚˜ì´ì € ê²½ë¡œ ì²˜ë¦¬
            tokenizer_path = model_id
            if model_name in ["deepseek-coder-7b", "llama-3.1-8b"]:
                # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ì€ í† í¬ë‚˜ì´ì € í´ë” ì‚¬ìš©
                tokenizer_path = info.get('tokenizer_path', model_id)
                if not tokenizer_path.startswith('./'):
                    tokenizer_path = f"./{tokenizer_path}"
                print(f"ğŸ“ í† í¬ë‚˜ì´ì € ê²½ë¡œ: {tokenizer_path}")
            
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path, 
                token=hf_token,
                trust_remote_code=True,
                local_files_only=model_id.startswith('./models/')  # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            if device_map == "cpu":
                model_kwargs["device_map"] = "cpu"
            else:
                model_kwargs["device_map"] = {"": 0}  # GPU 0 ì‚¬ìš©
                if quantization_config:
                    model_kwargs["quantization_config"] = quantization_config
            
            if hf_token and not model_id.startswith('./models/'):
                model_kwargs["token"] = hf_token
            
            # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ë¡œë“œ
            if model_id.startswith('./models/'):
                model_kwargs["local_files_only"] = True
                print(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            else:
                print(f"ğŸ”„ Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            
            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ë˜í¼
            def optimized_generate(prompt):
                try:
                    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=1024,
                        padding=True
                    )
                    
                    if device_map != "cpu":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs.get('input_ids'),
                            attention_mask=inputs.get('attention_mask'),
                            max_new_tokens=300,  # ë” ìƒì„¸í•œ ë‹µë³€ì„ ìœ„í•´ ì¦ê°€ (ê¸°ì¡´ 150)
                            temperature=0.6,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ê°ì†Œ (ê¸°ì¡´ 0.7)
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(
                        outputs[0][inputs.get('input_ids').shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return [{"generated_text": response}]
                    
                except Exception as e:
                    logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    return [{"generated_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}]
            
            pipe = optimized_generate
            
            self.model_cache[model_name] = {
                "pipeline": pipe,
                "features": info['features'],
                "info": info
            }
            
            self.currently_loaded_model = model_name
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            memory_usage = self.get_memory_usage()
            self.memory_usage_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": model_name,
                "memory_gb": memory_usage,
                "memory_percent": self.get_memory_usage_percent()
            })
            
            logger.info(f"âœ“ {model_name} ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬: {memory_usage:.1f}GB)")
            return True
            
        except Exception as e:
            logger.error(f"âœ— {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨í•œ ëª¨ë¸ì„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            self.failed_models_blacklist.add(model_name)
            logger.warning(f"âš ï¸ {model_name}ì„(ë¥¼) ì´ë²ˆ ì„¸ì…˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
            return False
                
    def _get_quantization_config(self, quantization: str):
        """ì–‘ìí™” ì„¤ì • ë°˜í™˜ (RTX 2080 8GB ìµœì í™”)"""
        if quantization == "4bit":
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True
            )
        elif quantization == "8bit":
            return BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True,  # RTX 2080 8GB í•„ìˆ˜
                llm_int8_has_fp16_weight=False  # ë©”ëª¨ë¦¬ ì ˆì•½
            )
        return None
        
    def generate_question(self, topic: LearningTopic) -> Dict[str, Any]:
        """ê°œì„ ëœ ì§ˆë¬¸ ìƒì„± - ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ êµ¬ì¶•ì— ìµœì í™”"""
        
        # ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ ë¹Œë”ì˜ ê³ í’ˆì§ˆ í…œí”Œë¦¿ ìš°ì„  ì‚¬ìš©
        if self.dataset_builder and hasattr(topic, 'category'):
            # Godot ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_map = {
                "Godot ê¸°ì´ˆ": "concept_explanation",
                "Godot ê³ ê¸‰": "code_generation",
                "Godot ì „ë¬¸ê°€ ì•„í‚¤í…ì²˜": "architecture",
                "Godot ì „ë¬¸ê°€ C#": "optimization",
                "Godot ì¡°ì‘ ê³ ê¸‰": "debugging"
            }
            
            builder_category = category_map.get(topic.category)
            if builder_category:
                template_question = self.dataset_builder.generate_instruction_from_template(builder_category)
                if template_question:
                    return {
                        'question': template_question,
                        'type': 'template_based',
                        'topic': topic.topic,
                        'difficulty': topic.difficulty_level,
                        'language': 'korean' if any(k in template_question for k in ['í•´', 'ì¤˜', 'ë‚˜', 'ì„']) else 'english',
                        'category': topic.category,
                        'quality_priority': True  # í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ì€ ìš°ì„ ìˆœìœ„ ë†’ìŒ
                    }
        
        # ê¸°ì¡´ ì§ˆë¬¸ íƒ€ì…ë“¤ (í´ë°±)
        question_types = [
            "explain",      # ê°œë… ì„¤ëª…
            "example",      # ì˜ˆì œ ì½”ë“œ
            "translate",    # í•œê¸€-ì˜ì–´ ë²ˆì—­
            "error",        # ì˜¤ë¥˜ ìˆ˜ì •
            "optimize",     # ìµœì í™”
            "integrate",    # Godot í†µí•©
            "compare",      # ë¹„êµ ë¶„ì„
            "implement",    # êµ¬í˜„ ê³¼ì œ
            "debug",        # ë””ë²„ê¹…
            "refactor",     # ë¦¬íŒ©í† ë§
            "design",       # ì„¤ê³„ íŒ¨í„´
            "test"          # í…ŒìŠ¤íŠ¸ ì‘ì„±
        ]
        
        # "Godot ì „ë¬¸ê°€" ì£¼ì œì¸ ê²½ìš°, ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ìƒì„±
        if topic.category == "Godot ì „ë¬¸ê°€":
            try:
                with open("collected_godot_docs.json", "r", encoding="utf-8") as f:
                    docs_data = json.load(f)
                
                if docs_data:
                    doc = random.choice(docs_data)
                    # ë‹¤ì–‘í•œ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ í…œí”Œë¦¿
                    doc_templates = [
                        f"{doc['title']}ì˜ í•µì‹¬ ê°œë…ì„ ì‹¤ì œ ê²Œì„ ê°œë°œ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                        f"{doc['title']}ë¥¼ ì‚¬ìš©í•œ ìµœì í™”ëœ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                        f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ {doc['title']}ì˜ ê³ ê¸‰ í™œìš©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”: \n\n{doc['content'][:300]}",
                        f"{doc['title']}ì™€ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ë¬¸ì œì ê³¼ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                        f"{doc['title']}ë¥¼ í™œìš©í•œ ì‹¤ë¬´ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”."
                    ]
                    question_text = random.choice(doc_templates)
                    return {
                        "id": f"{topic.id}_from_doc_{int(time.time())}",
                        "topic": topic.topic,
                        "type": "doc_based_qna",
                        "language": "korean",
                        "difficulty": topic.difficulty,
                        "question": question_text,
                        "keywords": topic.korean_keywords
                    }
            except FileNotFoundError:
                pass

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì§ˆë¬¸ íƒ€ì… ì„ íƒ (ë” ë‹¤ì–‘í•œ ë¶„í¬)
        type_weights = {
            "example": 0.20,    # 20%
            "explain": 0.15,    # 15%
            "implement": 0.15,  # 15%
            "integrate": 0.10,  # 10%
            "optimize": 0.10,   # 10%
            "error": 0.08,      # 8%
            "debug": 0.08,      # 8%
            "compare": 0.05,    # 5%
            "design": 0.05,     # 5%
            "translate": 0.02,  # 2%
            "refactor": 0.01,   # 1%
            "test": 0.01        # 1%
        }
        
        question_type = random.choices(
            list(type_weights.keys()),
            weights=list(type_weights.values())
        )[0]
        
        # í‚¤ì›Œë“œ ì¡°í•© ìƒì„± (1-3ê°œ)
        num_keywords = random.randint(1, min(3, len(topic.korean_keywords)))
        selected_keywords = random.sample(topic.korean_keywords, num_keywords)
        keyword_str = ", ".join(selected_keywords)
        
        num_concepts = random.randint(1, min(3, len(topic.python_concepts)))
        selected_concepts = random.sample(topic.python_concepts, num_concepts)
        concept_str = ", ".join(selected_concepts)
        
        # ë‚œì´ë„ë³„ ì»¨í…ìŠ¤íŠ¸
        difficulty_contexts = {
            "basic": ["ê¸°ì´ˆì ì¸", "ê°„ë‹¨í•œ", "ì…ë¬¸ìë¥¼ ìœ„í•œ", "ê¸°ë³¸ì ì¸"],
            "intermediate": ["ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ”", "ì¤‘ê¸‰ ìˆ˜ì¤€ì˜", "ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í• ", "í”„ë¡œë•ì…˜ ë ˆë²¨ì˜"],
            "advanced": ["ê³ ê¸‰", "ìµœì í™”ëœ", "ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì˜", "ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜"],
            "expert": ["ì•„í‚¤í…ì²˜ ë ˆë²¨ì˜", "ì—”í„°í”„ë¼ì´ì¦ˆê¸‰", "ìµœì²¨ë‹¨", "í˜ì‹ ì ì¸"]
        }
        
        difficulty_context = random.choice(difficulty_contexts.get(topic.difficulty, ["ì ì ˆí•œ"]))
        
        # ë‹¤ì–‘í•œ ì§ˆë¬¸ í…œí”Œë¦¿ (ê° íƒ€ì…ë³„ë¡œ 3-5ê°œì”©)
        templates = {
            "explain": {
                "korean": [
                    f"{topic.topic}ì˜ {keyword_str} ê°œë…ì„ {difficulty_context} ì˜ˆì œì™€ í•¨ê»˜ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} {topic.topic} ì‚¬ìš©ë²•ì„ {keyword_str}ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ì—ì„œ {keyword_str}ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ {topic.topic}ì˜ {keyword_str}ë¥¼ ë¹„ìœ ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ì™€ {keyword_str}ì˜ ê´€ê³„ë¥¼ ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Explain {difficulty_context} {topic.topic} focusing on {concept_str} with practical examples.",
                    f"Deep dive into {topic.topic} architecture, especially {concept_str} components.",
                    f"How does {concept_str} work in {topic.topic}? Include implementation details.",
                    f"Explain the relationship between {topic.topic} and {concept_str} with real-world scenarios."
                ]
            },
            "example": {
                "korean": [
                    f"{topic.topic}ì„ í™œìš©í•œ {difficulty_context} {keyword_str} êµ¬í˜„ ì˜ˆì œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                    f"{keyword_str}ë¥¼ ì‚¬ìš©í•˜ì—¬ {topic.topic} ê¸°ë°˜ì˜ ì‹¤ìš©ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} ìˆ˜ì¤€ì˜ {topic.topic} í”„ë¡œì íŠ¸ì—ì„œ {keyword_str}ë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš”.",
                    f"{topic.topic}ê³¼ {keyword_str}ë¥¼ ê²°í•©í•œ ì°½ì˜ì ì¸ ì½”ë“œ ì˜ˆì œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.",
                    f"ì‹¤ì œ ê²Œì„/ì•±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” {topic.topic}ì˜ {keyword_str} í™œìš© ì˜ˆì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Create a {difficulty_context} example using {topic.topic} with {concept_str}.",
                    f"Build a practical application demonstrating {concept_str} in {topic.topic}.",
                    f"Show creative usage of {topic.topic} combining {concept_str} features.",
                    f"Implement a real-world scenario using {topic.topic} and {concept_str}."
                ]
            },
            "implement": {
                "korean": [
                    f"{topic.topic}ì„ ì‚¬ìš©í•˜ì—¬ {keyword_str} ê¸°ëŠ¥ì„ ê°€ì§„ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} {keyword_str} ì‹œìŠ¤í…œì„ {topic.topic} ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„í•˜ê³  êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ê³¼ {keyword_str}ë¥¼ í™œìš©í•œ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.",
                    f"ì£¼ì–´ì§„ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° {topic.topic}ë¡œ {keyword_str} ëª¨ë“ˆì„ ê°œë°œí•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Implement a {difficulty_context} system using {topic.topic} with {concept_str} features.",
                    f"Design and build a {concept_str} module using {topic.topic} best practices.",
                    f"Create a complete mini-project showcasing {topic.topic} and {concept_str}."
                ]
            },
            "optimize": {
                "korean": [
                    f"{topic.topic}ì—ì„œ {keyword_str} ê´€ë ¨ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} í™˜ê²½ì—ì„œ {topic.topic}ì˜ {keyword_str} ë³‘ëª©í˜„ìƒì„ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ì„¸ìš”.",
                    f"{topic.topic} ì½”ë“œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì‹¤í–‰ ì†ë„ë¥¼ {keyword_str} ê´€ì ì—ì„œ ê°œì„ í•´ì£¼ì„¸ìš”.",
                    f"í”„ë¡œíŒŒì¼ë§ì„ í†µí•´ {topic.topic}ì˜ {keyword_str} ì„±ëŠ¥ ë¬¸ì œë¥¼ ì°¾ê³  í•´ê²°í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Optimize {topic.topic} performance focusing on {concept_str} bottlenecks.",
                    f"Profile and improve {concept_str} usage in {topic.topic} applications.",
                    f"Reduce memory footprint and execution time for {topic.topic} with {concept_str}."
                ]
            },
            "debug": {
                "korean": [
                    f"{topic.topic}ì—ì„œ ë°œìƒí•˜ëŠ” {keyword_str} ê´€ë ¨ ë²„ê·¸ë¥¼ ì°¾ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
                    f"ë‹¤ìŒ {topic.topic} ì½”ë“œì˜ {keyword_str} ë¶€ë¶„ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ë””ë²„ê¹…í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} {topic.topic} í”„ë¡œì íŠ¸ì˜ {keyword_str} ì˜¤ë¥˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì í•˜ê³  í•´ê²°í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Debug {concept_str} related issues in {topic.topic} code.",
                    f"Find and fix bugs in {topic.topic} implementation focusing on {concept_str}.",
                    f"Systematically trace and resolve {concept_str} errors in {topic.topic}."
                ]
            },
            "compare": {
                "korean": [
                    f"{topic.topic}ê³¼ ë‹¤ë¥¸ ê¸°ìˆ ì˜ {keyword_str} êµ¬í˜„ ë°©ì‹ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ì—ì„œ {keyword_str}ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”.",
                    f"ì„±ëŠ¥, ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ ê´€ì ì—ì„œ {topic.topic}ì˜ {keyword_str} ì ‘ê·¼ë²•ë“¤ì„ í‰ê°€í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Compare different approaches to {concept_str} in {topic.topic}.",
                    f"Analyze pros and cons of various {concept_str} implementations in {topic.topic}.",
                    f"Evaluate {topic.topic} {concept_str} patterns from performance and maintainability perspectives."
                ]
            },
            "design": {
                "korean": [
                    f"{topic.topic}ì„ í™œìš©í•œ {keyword_str} ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ {topic.topic} ê¸°ë°˜ {keyword_str} ì„¤ê³„ íŒ¨í„´ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.",
                    f"í™•ì¥ ê°€ëŠ¥í•œ {topic.topic} {keyword_str} êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³  ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Design a scalable architecture for {concept_str} using {topic.topic}.",
                    f"Propose design patterns for {topic.topic} based {concept_str} systems.",
                    f"Create architectural diagrams for {concept_str} implementation in {topic.topic}."
                ]
            },
            "test": {
                "korean": [
                    f"{topic.topic}ì˜ {keyword_str} ê¸°ëŠ¥ì„ ê²€ì¦í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} {topic.topic} í”„ë¡œì íŠ¸ì˜ {keyword_str} ë¶€ë¶„ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                    f"TDD ë°©ì‹ìœ¼ë¡œ {topic.topic}ì˜ {keyword_str} ëª¨ë“ˆì„ ê°œë°œí•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Write comprehensive tests for {concept_str} functionality in {topic.topic}.",
                    f"Implement unit tests for {topic.topic} {concept_str} module.",
                    f"Develop {concept_str} features in {topic.topic} using TDD approach."
                ]
            },
            "refactor": {
                "korean": [
                    f"ë ˆê±°ì‹œ {topic.topic} ì½”ë“œë¥¼ {keyword_str} íŒ¨í„´ì„ ì‚¬ìš©í•´ ë¦¬íŒ©í† ë§í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ì˜ {keyword_str} ë¶€ë¶„ì„ ë” ê¹”ë”í•˜ê³  ìœ ì§€ë³´ìˆ˜í•˜ê¸° ì‰½ê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.",
                    f"SOLID ì›ì¹™ì— ë”°ë¼ {topic.topic}ì˜ {keyword_str} êµ¬í˜„ì„ ë¦¬íŒ©í† ë§í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Refactor legacy {topic.topic} code using {concept_str} patterns.",
                    f"Improve {concept_str} implementation in {topic.topic} following SOLID principles.",
                    f"Clean up and modernize {topic.topic} {concept_str} codebase."
                ]
            },
            "error": {
                "korean": [
                    f"{topic.topic}ì—ì„œ {keyword_str} ì‚¬ìš© ì‹œ ë°œìƒí•˜ëŠ” ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ì±…ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                    f"{difficulty_context} ê°œë°œìê°€ {topic.topic}ì˜ {keyword_str}ì—ì„œ ìì£¼ ì‹¤ìˆ˜í•˜ëŠ” ë¶€ë¶„ê³¼ ì˜ˆë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic} {keyword_str} ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í•´ì„í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ë°©ë²•ì„ ê°€ì´ë“œí•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Common {concept_str} errors in {topic.topic} and how to fix them.",
                    f"Troubleshooting guide for {topic.topic} {concept_str} issues.",
                    f"Error handling best practices for {concept_str} in {topic.topic}."
                ]
            },
            "integrate": {
                "korean": [
                    f"Godot ì—”ì§„ì—ì„œ {topic.topic}ì˜ {keyword_str} ê°œë…ì„ C#ìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ê³¼ Godotì˜ {keyword_str}ë¥¼ ì—°ë™í•˜ëŠ” ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
                    f"Unityì—ì„œ Godotë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë•Œ {topic.topic}ì˜ {keyword_str}ë¥¼ ì–´ë–»ê²Œ ë³€í™˜í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš”.",
                    f"Godot 4.xì—ì„œ {topic.topic} ê¸°ë°˜ {keyword_str} ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Integrate {concept_str} from {topic.topic} into Godot with C#.",
                    f"Build a bridge between {topic.topic} {concept_str} and Godot systems.",
                    f"Migrate {concept_str} patterns from Unity to Godot using {topic.topic} principles."
                ]
            },
            "translate": {
                "korean": [
                    f"{concept_str} ìš©ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  {topic.topic} ë§¥ë½ì—ì„œì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    f"ë‹¤ìŒ ì˜ë¬¸ {topic.topic} ë¬¸ì„œë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  {keyword_str} ìš©ì–´ë¥¼ í•´ì„¤í•´ì£¼ì„¸ìš”.",
                    f"{topic.topic}ì˜ {concept_str}ì— í•´ë‹¹í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ìš©ì–´ì™€ ì‹¤ë¬´ ì‚¬ìš© ì˜ˆë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”."
                ],
                "english": [
                    f"Translate {keyword_str} to English and explain in {topic.topic} context.",
                    f"Create a glossary of {topic.topic} terms translating {keyword_str}.",
                    f"Professional translation of {keyword_str} with {topic.topic} usage examples."
                ]
            }
        }
        
        # ì–¸ì–´ ì„ íƒ (ë‚œì´ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •)
        language_weight = {
            "basic": 0.8,      # ê¸°ì´ˆëŠ” í•œê¸€ 80%
            "intermediate": 0.7, # ì¤‘ê¸‰ì€ í•œê¸€ 70%
            "advanced": 0.6,    # ê³ ê¸‰ì€ í•œê¸€ 60%
            "expert": 0.5       # ì „ë¬¸ê°€ëŠ” í•œê¸€ 50%
        }
        
        language = "korean" if random.random() < language_weight.get(topic.difficulty, 0.7) else "english"
        
        # í…œí”Œë¦¿ ì„ íƒ
        question_templates = templates.get(question_type, templates["example"])[language]
        question_text = random.choice(question_templates)
        
        # ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (30% í™•ë¥ )
        if random.random() < 0.3:
            scenarios = [
                "ì‹¤ì‹œê°„ ë©€í‹°í”Œë ˆì´ì–´ ê²Œì„",
                "ëª¨ë°”ì¼ ì•±",
                "ì›¹ ì„œë¹„ìŠ¤ ë°±ì—”ë“œ",
                "AI ì±—ë´‡",
                "ë°ì´í„° ë¶„ì„ ë„êµ¬",
                "ìë™í™” ìŠ¤í¬ë¦½íŠ¸",
                "ê²Œì„ ì—”ì§„ í”ŒëŸ¬ê·¸ì¸",
                "IoT ë””ë°”ì´ìŠ¤",
                "ë¸”ë¡ì²´ì¸ dApp",
                "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤"
            ]
            scenario = random.choice(scenarios)
            question_text = f"[{scenario} ê°œë°œ ìƒí™©] " + question_text
        
        # ì¤‘ë³µ í™•ì¸ ë° ì¬ìƒì„± (ìµœëŒ€ 5íšŒ ì‹œë„)
        max_attempts = 5
        for attempt in range(max_attempts):
            if not self._is_question_too_similar(question_text):
                break
            
            # ì¬ìƒì„± ì‹œ ë‹¤ë¥¸ í…œí”Œë¦¿, í‚¤ì›Œë“œ ì¡°í•©, ì‹œë‚˜ë¦¬ì˜¤ ì‚¬ìš©
            logger.info(f"ğŸ”„ ìœ ì‚¬í•œ ì§ˆë¬¸ ê°ì§€, ì¬ìƒì„± ì¤‘... (ì‹œë„ {attempt + 1}/{max_attempts})")
            
            # ë‹¤ë¥¸ ì§ˆë¬¸ íƒ€ì… ì„ íƒ
            if attempt == 1:
                question_type = random.choices(
                    list(type_weights.keys()),
                    weights=list(type_weights.values())
                )[0]
            
            # ë‹¤ë¥¸ í‚¤ì›Œë“œ ì¡°í•©
            num_keywords = random.randint(1, min(3, len(topic.korean_keywords)))
            selected_keywords = random.sample(topic.korean_keywords, num_keywords)
            keyword_str = ", ".join(selected_keywords)
            
            num_concepts = random.randint(1, min(3, len(topic.python_concepts)))
            selected_concepts = random.sample(topic.python_concepts, num_concepts)
            concept_str = ", ".join(selected_concepts)
            
            # ë‹¤ë¥¸ ë‚œì´ë„ ì»¨í…ìŠ¤íŠ¸
            difficulty_context = random.choice(difficulty_contexts.get(topic.difficulty, ["ì ì ˆí•œ"]))
            
            # ë‹¤ë¥¸ í…œí”Œë¦¿ ì„ íƒ
            question_templates = templates.get(question_type, templates["example"])[language]
            question_text = random.choice(question_templates)
            
            # ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ (í™•ë¥  ë†’ì„)
            if random.random() < 0.5:  # 50%ë¡œ ì¦ê°€
                scenario = random.choice(scenarios)
                question_text = f"[{scenario} ê°œë°œ ìƒí™©] " + question_text
        
        question_data = {
            "id": f"{topic.id}_{question_type}_{int(time.time())}",
            "topic": topic.topic,
            "type": question_type,
            "language": language,
            "difficulty": topic.difficulty,
            "question": question_text,
            "keywords": topic.korean_keywords if language == "korean" else topic.python_concepts,
            "context": {
                "scenario": scenario if 'scenario' in locals() else None,
                "difficulty_context": difficulty_context,
                "keyword_combination": selected_keywords if language == "korean" else selected_concepts
            }
        }
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self._add_to_question_history(question_data)
        
        return question_data
        
    def select_model_for_question(self, question: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ (5ëŒ€ í•µì‹¬ ì£¼ì œ DeepSeek-coder ìµœì í™”)"""
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ ê³ ë ¤
        rtx_2080_models = {
            "deepseek-coder-7b": {"priority": 10, "specialties": ["code", "csharp", "godot", "korean", "socketio"], "vram": 6},
            "phi3-mini": {"priority": 8, "specialties": ["reasoning", "math", "python"], "vram": 6},
            "llama-3.1-8b": {"priority": 7, "specialties": ["general", "korean", "python"], "vram": 7},
            "gemma-4b": {"priority": 6, "specialties": ["general", "korean"], "vram": 4},
            "mistral-7b": {"priority": 4, "specialties": ["general", "fast"], "vram": 7}
        }
        
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ í•„í„°ë§ (ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œì™¸)
        available_optimized = []
        for model_name in self.available_models:
            if (model_name in rtx_2080_models and 
                self.available_models[model_name].get('rtx_2080_optimized', False) and
                model_name not in self.failed_models_blacklist):  # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬
                available_optimized.append(model_name)
        
        if not available_optimized:
            logger.warning("âŒ RTX 2080 ìµœì í™” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            if self.failed_models_blacklist:
                logger.info(f"ğŸš« ë¸”ë™ë¦¬ìŠ¤íŠ¸ ëª¨ë¸: {', '.join(self.failed_models_blacklist)}")
            logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ DeepSeek-coder 6.7Bë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            # Return a default model if available
            if self.available_models:
                return list(self.available_models.keys())[0]
            return "deepseek-coder-7b"  # fallback model name
            
        # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¹´í…Œê³ ë¦¬ í™•ì¸
        core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "ë³€í˜•ëœ Godot", "Socket.IO", "AI ìµœì í™”"]
        topic_category = question.get("category", "")
        is_core_topic = topic_category in core_categories
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ RTX 2080 ìµœì í™”ì´ê³  ì í•©í•˜ë©´ ê³„ì† ì‚¬ìš©
        if (self.currently_loaded_model and 
            self.currently_loaded_model in available_optimized and
            self._is_model_suitable(self.currently_loaded_model, question)):
            return self.currently_loaded_model
            
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„ (5ê°€ì§€ í•µì‹¬ ì£¼ì œ í¬í•¨)
        question_features = set()
        question_text = question.get("question", "").lower()
        topic_text = question.get("topic", "").lower()
        
        # 1ï¸âƒ£ Python í”„ë¡œê·¸ë˜ë° íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['code', 'programming', 'script', 'function', 'class', 'method', 'python', 'py']):
            question_features.add('python')
            question_features.add('code')
        
        # 2ï¸âƒ£ í•œê¸€ í”„ë¡œê·¸ë˜ë° ìš©ì–´ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['korean', 'í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­', 'ìš©ì–´', 'ê°œë…', 'ì–¸ì–´']):
            question_features.add('korean')
        
        # 3ï¸âƒ£ Godot ì—”ì§„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['godot', 'node', 'scene', 'ë…¸ë“œ', 'ì”¬']):
            question_features.add('godot')
        
        # 4ï¸âƒ£ Socket.IO ë„¤íŠ¸ì›Œí‚¹ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['socket', 'socketio', 'realtime', 'ì†Œì¼“', 'ì‹¤ì‹œê°„']):
            question_features.add('socketio')
        
        # 5ï¸âƒ£ AI ìµœì í™” íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['ai', 'optimize', 'prompt', 'ìµœì í™”', 'í”„ë¡¬í”„íŠ¸']):
            question_features.add('ai_optimization')
        
        # ëª¨ë¸ ì ìˆ˜ ê³„ì‚°
        model_scores = []
        for model_name in available_optimized:
            model_info = rtx_2080_models[model_name]
            score = model_info['priority']
            
            # íŠ¹ì„± ë§¤ì¹­ ì ìˆ˜
            for feature in question_features:
                if feature in model_info['specialties']:
                    score += 5
            
            # ëª¨ë¸ë³„ íŠ¹ë³„ ë³´ë„ˆìŠ¤
            if model_name == "llama-3.1-8b" and 'korean' in question_features:
                score += 20  # í•œêµ­ì–´ íŠ¹í™” ë³´ë„ˆìŠ¤
            elif model_name == "gemma-4b" and 'korean' in question_features:
                score += 12  # í•œêµ­ì–´ íŠ¹í™” ë³´ë„ˆìŠ¤
            elif model_name == "deepseek-coder-7b":
                if 'code' in question_features or 'python' in question_features:
                    score += 10  # ì½”ë“œ íŠ¹í™” ë³´ë„ˆìŠ¤
                if is_core_topic:
                    score += 15  # DeepSeek-coderì— í•µì‹¬ ì£¼ì œ ë³´ë„ˆìŠ¤
            
            model_scores.append((model_name, score))
        
        # ì ìˆ˜ê°€ ë†’ì€ ëª¨ë¸ ì„ íƒ
        model_scores.sort(key=lambda x: x[1], reverse=True)
        
        if model_scores:
            selected_model = model_scores[0][0]
            logger.info(f"ì„ íƒëœ ëª¨ë¸: {selected_model} (ì ìˆ˜: {model_scores[0][1]})")
            return selected_model
        else:
            logger.error("ì í•©í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return list(self.available_models.keys())[0] if self.available_models else "deepseek-coder-7b"
    
    def _is_model_suitable(self, model_name: str, question: Dict[str, Any]) -> bool:
        """ëª¨ë¸ì´ ì§ˆë¬¸ì— ì í•©í•œì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ ì í•©ì„± ê²€ì‚¬
        question_text = question.get("question", "").lower()
        
        if model_name == "deepseek-coder-7b":
            # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì— ë§¤ìš° ì í•©
            return any(word in question_text for word in ['code', 'function', 'class', 'godot', 'csharp'])
        elif model_name == "llama-3.1-8b":
            # í•œêµ­ì–´ ì§ˆë¬¸ì— ì í•©
            return any(word in question_text for word in ['í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­'])
        
        return True  # ê¸°ë³¸ì ìœ¼ë¡œ ì í•©í•˜ë‹¤ê³  ê°€ì •
    
    async def ask_and_learn(self, question: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """ì§ˆë¬¸í•˜ê³  ë‹µë³€ í•™ìŠµ"""
        try:
            # ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ìºì‹œëœ ì •ë³´ í™•ì¸
            from modules.shared_knowledge_base import get_shared_knowledge_base
            shared_kb = get_shared_knowledge_base()
            
            # ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ìºì‹œ ê²€ìƒ‰
            question_keywords = question.get('question', '').split()[:3]  # ì²« 3ë‹¨ì–´ë¡œ ê²€ìƒ‰
            search_keyword = ' '.join(question_keywords)
            
            cached_info = await shared_kb.get_cached_search(search_keyword)
            if cached_info:
                logger.info(f"ğŸ“š ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ ë°œê²¬: {search_keyword}")
                # ìºì‹œëœ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë” ë‚˜ì€ ë‹µë³€ ìƒì„± ê°€ëŠ¥
            
            # ëª¨ë¸ ë¡œë“œ (ì¬ì‹œë„ í¬í•¨)
            load_success = False
            for attempt in range(3):
                try:
                    if self.load_model(model_name):
                        load_success = True
                        break
                except Exception as load_error:
                    logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹œë„ {attempt + 1}/3 ì‹¤íŒ¨: {str(load_error)}")
                    if attempt < 2:
                        await asyncio.sleep(5)
                        gc.collect()
            
            if not load_success:
                return {
                    "success": False,
                    "error": f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨ (ëª¨ë“  ì‹œë„ ì‹¤íŒ¨)"
                }
            
            # íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            pipe = self.model_cache[model_name]["pipeline"]
            
            # ë‹µë³€ ìƒì„±
            logger.info(f"ì§ˆë¬¸: {question['question'][:100]}...")
            
            start_time = time.time()
            answer = None
            generation_error = None
            
            # ë‹µë³€ ìƒì„± (íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ í¬í•¨)
            for gen_attempt in range(2):
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì • (60ì´ˆ)
                    response = await asyncio.wait_for(
                        asyncio.to_thread(lambda: pipe(question['question'])),
                        timeout=60.0
                    )
                    answer = response[0]['generated_text'] if response else "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
                    if answer and answer != "ë‹µë³€ ìƒì„± ì‹¤íŒ¨":
                        break
                except asyncio.TimeoutError:
                    logger.warning(f"ë‹µë³€ ìƒì„± íƒ€ì„ì•„ì›ƒ (60ì´ˆ) - ì‹œë„ {gen_attempt + 1}/2")
                    generation_error = "timeout"
                except Exception as gen_error:
                    logger.warning(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜ - ì‹œë„ {gen_attempt + 1}/2: {str(gen_error)}")
                    generation_error = str(gen_error)
                
                if gen_attempt == 0:
                    await asyncio.sleep(5)
            
            generation_time = time.time() - start_time
            
            if not answer or answer == "ë‹µë³€ ìƒì„± ì‹¤íŒ¨":
                return {
                    "success": False,
                    "error": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {generation_error}"
                }
            logger.info(f"ë‹µë³€ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            
            # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ê°œì„ ëœ PyTorch ëª¨ë¸ ì‚¬ìš©)
            try:
                quality_score = self._evaluate_answer_quality(question, answer, model_name)
            except Exception as eval_error:
                logger.warning(f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(eval_error)}")
                quality_score = 0.5  # ê¸°ë³¸ê°’
            
            # ì‹¤ì‹œê°„ í”¼ë“œë°±ìœ¼ë¡œ ë‹µë³€ ê°œì„  ì‹œë„
            if quality_score < 0.7:
                logger.info(f"ğŸ’¡ í’ˆì§ˆì´ ë‚®ì•„ ê°œì„  ì‹œë„ ì¤‘... (í˜„ì¬: {quality_score:.2f})")
                improved_answer = self._improve_answer_with_feedback(question, answer, model_name)
                if improved_answer != answer:
                    answer = improved_answer
                    quality_score = self._evaluate_answer_quality(question, answer, model_name)
                    logger.info(f"âœ¨ ê°œì„  í›„ í’ˆì§ˆ: {quality_score:.2f}")
            
            # ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            try:
                task_category = self._get_task_category(question['topic'])
                if task_category:
                    self._update_multitask_network(question, answer, quality_score, task_category)
            except Exception as mt_error:
                logger.warning(f"ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(mt_error)}")
            
            # ë‹µë³€ ì €ì¥
            answer_data = {
                "question_id": question['id'],
                "question": question['question'],
                "answer": answer,
                "model": model_name,
                "quality_score": quality_score,
                "generation_time": generation_time,
                "timestamp": datetime.now().isoformat(),
                "topic": question['topic'],
                "language": question['language']
            }
            
            # ë‹µë³€ ë””ë ‰í† ë¦¬ì— ì €ì¥
            today = datetime.now().strftime("%Y%m%d")
            answer_dir = self.answers_dir / today
            answer_dir.mkdir(exist_ok=True)
            
            answer_file = answer_dir / f"{question['id']}.json"
            with open(answer_file, 'w', encoding='utf-8') as f:
                json.dump(answer_data, f, indent=2, ensure_ascii=False)
            
            # ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_knowledge_base(question, answer, quality_score)
            
            # ê³ í’ˆì§ˆ ë‹µë³€ì€ ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì— ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¡œ ì €ì¥
            if quality_score > 0.8:
                await shared_kb.save_best_practice(
                    topic=question.get('topic', 'general'),
                    practice={
                        "question": question['question'],
                        "answer": answer,
                        "model": model_name,
                        "quality_score": quality_score,
                        "language": question.get('language', 'ko')
                    }
                )
                logger.info(f"ğŸ“š ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì €ì¥: {question['topic']}")
            
            # PyTorch í•™ìŠµ ë°ì´í„°ë¡œ ì¶”ê°€
            if self.pytorch_system and quality_score > 0.6:
                experience = {
                    'question': question['question'],
                    'answer': answer,
                    'quality_score': quality_score,
                    'topic': question['topic']
                }
                
                # ê°•í™”í•™ìŠµ ì—…ë°ì´íŠ¸
                state = {'quality_score': quality_score, 'topic': question['topic']}
                action = 'generate_answer'
                reward = quality_score
                next_state = {'quality_score': quality_score + 0.1, 'topic': question['topic']}
                
                self.pytorch_system.reinforcement_learning_step(state, action, reward, next_state)
            
            # ì„¸ì…˜ ì—…ë°ì´íŠ¸
            if self.current_session:
                self.current_session.questions_asked += 1
                if quality_score > 0.7:
                    self.current_session.successful_answers += 1
                
                if model_name not in self.current_session.models_used:
                    self.current_session.models_used[model_name] = 0
                self.current_session.models_used[model_name] += 1
                
                self.current_session.knowledge_gained.append({
                    "topic": question['topic'],
                    "quality": quality_score,
                    "timestamp": datetime.now().isoformat()
                })
            
            return {
                "success": True,
                "answer": answer,
                "quality_score": quality_score,
                "model": model_name,
                "generation_time": generation_time
            }
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            
            # í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ í™•ë³´
            try:
                self.unload_current_model()
                gc.collect()
            except:
                pass
            
            return {
                "success": False,
                "error": str(e)
            }
    
    def _update_knowledge_base(self, question: Dict[str, Any], answer: str, quality_score: float):
        """ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        if quality_score < 0.6:
            return  # í’ˆì§ˆì´ ë‚®ì€ ë‹µë³€ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
        
        topic = question['topic']
        category = question.get('category', 'general')
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
        if category not in self.knowledge_base:
            self.knowledge_base[category] = {}
        
        if topic not in self.knowledge_base[category]:
            self.knowledge_base[category][topic] = []
        
        # ì§€ì‹ í•­ëª© ì¶”ê°€
        knowledge_item = {
            "question": question['question'],
            "answer": answer,
            "quality_score": quality_score,
            "timestamp": datetime.now().isoformat(),
            "keywords": question.get('keywords', [])
        }
        
        self.knowledge_base[category][topic].append(knowledge_item)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
        if len(self.knowledge_base[category][topic]) % 10 == 0:
            self._save_knowledge_base()
    
    def calculate_question_diversity_metrics(self) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self.question_history:
            return {
                "total_questions": 0,
                "unique_questions": 0,
                "diversity_score": 0.0,
                "type_distribution": {},
                "topic_distribution": {},
                "keyword_distribution": {},
                "avg_similarity": 0.0
            }
        
        # ê¸°ë³¸ í†µê³„
        total_questions = len(self.question_history)
        unique_questions = len(set(q['question'] for q in self.question_history))
        
        # íƒ€ì…ë³„ ë¶„í¬
        type_counts = {}
        topic_counts = {}
        keyword_counts = {}
        
        for q in self.question_history:
            # íƒ€ì… ë¶„í¬
            q_type = q.get('type', 'unknown')
            type_counts[q_type] = type_counts.get(q_type, 0) + 1
            
            # ì£¼ì œ ë¶„í¬
            q_topic = q.get('topic', 'unknown')
            topic_counts[q_topic] = topic_counts.get(q_topic, 0) + 1
            
            # í‚¤ì›Œë“œ ë¶„í¬
            for keyword in q.get('keywords', []):
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
        import math
        
        def calculate_entropy(counts):
            total = sum(counts.values())
            if total == 0:
                return 0.0
            entropy = 0.0
            for count in counts.values():
                if count > 0:
                    prob = count / total
                    entropy -= prob * math.log2(prob)
            return entropy
        
        type_entropy = calculate_entropy(type_counts)
        topic_entropy = calculate_entropy(topic_counts)
        keyword_entropy = calculate_entropy(keyword_counts)
        
        # ì •ê·œí™”ëœ ë‹¤ì–‘ì„± ì ìˆ˜ (0-1)
        max_type_entropy = math.log2(len(type_counts)) if len(type_counts) > 1 else 1
        max_topic_entropy = math.log2(len(topic_counts)) if len(topic_counts) > 1 else 1
        max_keyword_entropy = math.log2(len(keyword_counts)) if len(keyword_counts) > 1 else 1
        
        type_diversity = type_entropy / max_type_entropy if max_type_entropy > 0 else 0
        topic_diversity = topic_entropy / max_topic_entropy if max_topic_entropy > 0 else 0
        keyword_diversity = keyword_entropy / max_keyword_entropy if max_keyword_entropy > 0 else 0
        
        # ì „ì²´ ë‹¤ì–‘ì„± ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        diversity_score = (0.3 * type_diversity + 0.3 * topic_diversity + 0.4 * keyword_diversity)
        
        # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ìµœê·¼ 50ê°œ ì§ˆë¬¸)
        recent_questions = self.question_history[-50:] if len(self.question_history) > 50 else self.question_history
        similarities = []
        
        for i in range(len(recent_questions)):
            for j in range(i + 1, min(i + 10, len(recent_questions))):  # ê° ì§ˆë¬¸ê³¼ ë‹¤ìŒ 10ê°œë§Œ ë¹„êµ
                sim = self._calculate_question_similarity(
                    recent_questions[i]['question'],
                    recent_questions[j]['question']
                )
                similarities.append(sim)
        
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
        
        return {
            "total_questions": total_questions,
            "unique_questions": unique_questions,
            "uniqueness_ratio": unique_questions / total_questions if total_questions > 0 else 0,
            "diversity_score": round(diversity_score, 3),
            "type_distribution": type_counts,
            "topic_distribution": topic_counts,
            "keyword_distribution": dict(sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:20]),  # Top 20
            "avg_similarity": round(avg_similarity, 3),
            "type_entropy": round(type_entropy, 3),
            "topic_entropy": round(topic_entropy, 3),
            "keyword_entropy": round(keyword_entropy, 3)
        }
    
    def log_diversity_metrics(self):
        """ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ì„ ë¡œê·¸ì— ì¶œë ¥"""
        metrics = self.calculate_question_diversity_metrics()
        
        logger.info("ğŸ“Š ì§ˆë¬¸ ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­:")
        logger.info(f"  - ì´ ì§ˆë¬¸ ìˆ˜: {metrics['total_questions']}")
        logger.info(f"  - ê³ ìœ  ì§ˆë¬¸ ìˆ˜: {metrics['unique_questions']} ({metrics['uniqueness_ratio']:.1%})")
        logger.info(f"  - ë‹¤ì–‘ì„± ì ìˆ˜: {metrics['diversity_score']:.3f} (0-1)")
        logger.info(f"  - í‰ê·  ìœ ì‚¬ë„: {metrics['avg_similarity']:.3f}")
        logger.info(f"  - íƒ€ì… ì—”íŠ¸ë¡œí”¼: {metrics['type_entropy']:.3f}")
        logger.info(f"  - ì£¼ì œ ì—”íŠ¸ë¡œí”¼: {metrics['topic_entropy']:.3f}")
        logger.info(f"  - í‚¤ì›Œë“œ ì—”íŠ¸ë¡œí”¼: {metrics['keyword_entropy']:.3f}")
        
        # íƒ€ì…ë³„ ë¶„í¬ ìƒìœ„ 5ê°œ
        if metrics['type_distribution']:
            logger.info("  - ì§ˆë¬¸ íƒ€ì… ë¶„í¬ (ìƒìœ„ 5ê°œ):")
            for q_type, count in sorted(metrics['type_distribution'].items(), 
                                       key=lambda x: x[1], reverse=True)[:5]:
                logger.info(f"    â€¢ {q_type}: {count}íšŒ")
    
    async def continuous_learning_loop(self, duration_hours: float = 24):
        """ì—°ì† í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ {duration_hours}ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œì‘!")
        
        # ì„¸ì…˜ ì‹œì‘
        self.current_session = LearningSession(
            session_id=f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            start_time=datetime.now(),
            topics_covered=[],
            questions_asked=0,
            successful_answers=0,
            models_used={},
            knowledge_gained=[]
        )
        
        end_time = datetime.now() + timedelta(hours=duration_hours)
        
        # PyTorch í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        pytorch_experiences = []
        
        # í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        if self.quality_monitor:
            self.quality_monitor.start_monitoring(
                model_name="AutoCI Learning System",
                dataset_size=len(self.knowledge_base)
            )
        
        # í•™ìŠµ ì „ ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜
        if self.dataset_builder and len(self.knowledge_base) > 0:
            logger.info("ğŸ“š ê¸°ì¡´ ì§€ì‹ì„ ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
            kb_path = self.knowledge_base_dir / "knowledge_base.json"
            if kb_path.exists():
                imported = self.dataset_builder.import_from_existing_knowledge(kb_path)
                logger.info(f"âœ… {imported}ê°œì˜ ê³ í’ˆì§ˆ ë°ì´í„° ì„í¬íŠ¸ ì™„ë£Œ")
        
        try:
            while datetime.now() < end_time:
                # í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self.quality_monitor and self.quality_monitor.should_stop:
                    logger.warning("ğŸ›‘ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ê¸° ì¢…ë£Œ!")
                    break
                # ë©”ëª¨ë¦¬ ì²´í¬ ë° ê´€ë¦¬
                try:
                    memory_usage = self.get_memory_usage_percent()
                    if memory_usage > 85:
                        logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_usage:.1f}%")
                        self.unload_current_model()
                        gc.collect()
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        await asyncio.sleep(10)
                    elif memory_usage > 95:
                        logger.critical(f"ë©”ëª¨ë¦¬ ìœ„í—˜ ìˆ˜ì¤€: {memory_usage:.1f}%")
                        # ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ
                        self.model_cache.clear()
                        self.currently_loaded_model = None
                        gc.collect()
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        await asyncio.sleep(30)
                except Exception as mem_error:
                    logger.error(f"ë©”ëª¨ë¦¬ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(mem_error)}")
                    # ì•ˆì „ì„ ìœ„í•´ ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                
                # í•™ìŠµ ì£¼ì œ ì„ íƒ
                topic = random.choice(self.learning_topics)
                
                # ì§ˆë¬¸ ìƒì„±
                question = self.generate_question(topic)
                question['category'] = topic.category
                
                # ëª¨ë¸ ì„ íƒ
                model_name = self.select_model_for_question(question)
                if not model_name:
                    logger.error("ì í•©í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    await asyncio.sleep(10)
                    continue
                
                # í•™ìŠµ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                result = None
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        result = await self.ask_and_learn(question, model_name)
                        break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    except Exception as e:
                        logger.warning(f"í•™ìŠµ ì‹œë„ {retry + 1}/{max_retries} ì‹¤íŒ¨: {str(e)}")
                        if retry < max_retries - 1:
                            await asyncio.sleep(10)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                            # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹œë„
                            alternative_models = [m for m in self.available_models if m != model_name and m not in self.failed_models_blacklist]
                            if alternative_models:
                                model_name = random.choice(alternative_models)
                                logger.info(f"ëŒ€ì²´ ëª¨ë¸ë¡œ ì¬ì‹œë„: {model_name}")
                        else:
                            result = {"success": False, "error": str(e)}
                
                if result and result['success']:
                    logger.info(f"âœ“ í•™ìŠµ ì„±ê³µ! í’ˆì§ˆ: {result['quality_score']:.2f}")
                    
                    # ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ì§€ì‹œ-ì‘ë‹µ ë°ì´í„°ì…‹ì— ì¶”ê°€
                    if self.dataset_builder and result['quality_score'] > 0.7:
                        self.dataset_builder.add_instruction_response_pair(
                            instruction=question['question'],
                            output=result['answer'],
                            category=topic.category,
                            difficulty=topic.difficulty_level,
                            source="continuous_learning",
                            verified=result['quality_score'] > 0.8
                        )
                        logger.info("ğŸ“š ê³ í’ˆì§ˆ ì§€ì‹œ-ì‘ë‹µ ìŒ ë°ì´í„°ì…‹ì— ì¶”ê°€ë¨")
                    
                    # PyTorch í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘
                    if result['quality_score'] > 0.6:
                        pytorch_experiences.append({
                            'question': question['question'],
                            'answer': result['answer'],
                            'quality_score': result['quality_score'],
                            'topic': topic.topic
                        })
                    
                    # ì£¼ì œ ì»¤ë²„ë¦¬ì§€ ì—…ë°ì´íŠ¸
                    if topic.id not in self.current_session.topics_covered:
                        self.current_session.topics_covered.append(topic.id)
                    
                    # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸
                    if self.quality_monitor:
                        self.quality_monitor.update_metrics(
                            epoch=self.current_session.questions_asked // 100,
                            iteration=self.current_session.questions_asked,
                            train_loss=1.0 - result['quality_score'],  # í’ˆì§ˆ ì ìˆ˜ë¥¼ lossë¡œ ë³€í™˜
                            validation_loss=None  # ì‹¤ì œ validation ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        )
                else:
                    logger.error(f"âœ— í•™ìŠµ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
                # PyTorch ë°°ì¹˜ í•™ìŠµ (100ê°œ ê²½í—˜ë§ˆë‹¤)
                if len(pytorch_experiences) >= 100 and self.pytorch_system:
                    try:
                        logger.info("ğŸ§  PyTorch ë°°ì¹˜ í•™ìŠµ ì‹œì‘...")
                        self.pytorch_system.train_on_experience(pytorch_experiences, epochs=5)
                        pytorch_experiences = []  # ë¦¬ì…‹
                    except Exception as pt_error:
                        logger.error(f"PyTorch í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(pt_error)}")
                        # ì¼ë¶€ ê²½í—˜ë§Œ ì‚­ì œí•˜ì—¬ ë©”ëª¨ë¦¬ í™•ë³´
                        pytorch_experiences = pytorch_experiences[-50:]
                
                # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë°°ì¹˜ í•™ìŠµ (32ê°œ ê²½í—˜ë§ˆë‹¤)
                if len(self.experience_buffer) >= 32 and self.current_session.questions_asked % 5 == 0:
                    try:
                        self.train_quality_assessor_batch()
                    except Exception as qa_error:
                        logger.error(f"í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(qa_error)}")
                
                # ì§„í–‰ ìƒí™© ì €ì¥ (10ë¶„ë§ˆë‹¤)
                if self.current_session.questions_asked % 10 == 0:
                    try:
                        self._save_learning_progress()
                        # ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ì¶œë ¥ (10ê°œ ì§ˆë¬¸ë§ˆë‹¤)
                        self.log_diversity_metrics()
                        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ìƒíƒœ ì¶œë ¥
                        if self.failed_models_blacklist:
                            logger.info(f"ğŸš« í˜„ì¬ ë¸”ë™ë¦¬ìŠ¤íŠ¸: {', '.join(self.failed_models_blacklist)}")
                        # PyTorch ëª¨ë¸ ì €ì¥ (50ê°œ ì§ˆë¬¸ë§ˆë‹¤)
                        if self.current_session.questions_asked % 50 == 0:
                            self.save_pytorch_models()
                    except Exception as save_error:
                        logger.error(f"ì§„í–‰ ìƒí™© ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(save_error)}")
                
                # ëŒ€ê¸° ì‹œê°„ (ë„ˆë¬´ ë¹ ë¥¸ ë°˜ë³µ ë°©ì§€)
                await asyncio.sleep(random.uniform(5, 15))
                
                # ì£¼ê¸°ì  í—¬ìŠ¤ì²´í¬ (1ì‹œê°„ë§ˆë‹¤)
                if self.current_session.questions_asked > 0 and self.current_session.questions_asked % 60 == 0:
                    elapsed_hours = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
                    remaining_hours = (end_time - datetime.now()).total_seconds() / 3600
                    success_rate = self.current_session.successful_answers / max(1, self.current_session.questions_asked)
                    
                    logger.info("\n" + "="*60)
                    logger.info("ğŸ¥ ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬")
                    logger.info(f"â±ï¸  ê²½ê³¼ ì‹œê°„: {elapsed_hours:.1f}ì‹œê°„ / ë‚¨ì€ ì‹œê°„: {remaining_hours:.1f}ì‹œê°„")
                    logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: {self.current_session.questions_asked}ê°œ ì§ˆë¬¸ / {self.current_session.successful_answers}ê°œ ì„±ê³µ")
                    logger.info(f"âœ… ì„±ê³µë¥ : {success_rate:.1%}")
                    logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {self.get_memory_usage():.1f}GB / {self.max_memory_gb}GB")
                    logger.info(f"ğŸ¤– í˜„ì¬ ëª¨ë¸: {self.currently_loaded_model}")
                    logger.info(f"ğŸ“š ì»¤ë²„ëœ ì£¼ì œ: {len(self.current_session.topics_covered)}ê°œ")
                    logger.info("="*60 + "\n")
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (2ì‹œê°„ë§ˆë‹¤)
                if self.current_session.questions_asked > 0 and self.current_session.questions_asked % 120 == 0:
                    logger.info("ğŸ§¹ ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
                    try:
                        # í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œ ë° ì¬ë¡œë“œ
                        current_model = self.currently_loaded_model
                        if current_model:
                            self.unload_current_model()
                            gc.collect()
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            await asyncio.sleep(10)
                            
                            # ëª¨ë¸ ì¬ë¡œë“œ
                            logger.info(f"ğŸ”„ {current_model} ëª¨ë¸ ì¬ë¡œë“œ...")
                            self.load_model(current_model)
                        
                        # Experience buffer í¬ê¸° ì œí•œ
                        if len(self.experience_buffer) > 5000:
                            logger.info("ğŸ“¦ Experience buffer í¬ê¸° ì¡°ì •...")
                            # ê°€ì¥ ì˜¤ë˜ëœ ê²½í—˜ ì œê±°
                            while len(self.experience_buffer) > 3000:
                                self.experience_buffer.buffer.popleft()
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íˆìŠ¤í† ë¦¬ ì •ë¦¬
                        if len(self.memory_usage_history) > 1000:
                            self.memory_usage_history = self.memory_usage_history[-500:]
                        
                        logger.info("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                    except Exception as cleanup_error:
                        logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(cleanup_error)}")
                
        except KeyboardInterrupt:
            logger.info("í•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"í•™ìŠµ ë£¨í”„ ì˜¤ë¥˜: {str(e)}")
            logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì§„í–‰ ìƒí™© ì €ì¥
            try:
                self._save_learning_progress()
                self._save_knowledge_base()
                logger.info("ì˜¤ë¥˜ ë°œìƒ í›„ ì§„í–‰ ìƒí™© ì €ì¥ ì™„ë£Œ")
            except Exception as save_error:
                logger.error(f"ì§„í–‰ ìƒí™© ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(save_error)}")
        finally:
            # ë§ˆì§€ë§‰ PyTorch í•™ìŠµ
            if pytorch_experiences and self.pytorch_system:
                logger.info("ğŸ§  ìµœì¢… PyTorch ë°°ì¹˜ í•™ìŠµ...")
                self.pytorch_system.train_on_experience(pytorch_experiences, epochs=5)
            
            # ì„¸ì…˜ ì¢…ë£Œ
            session_duration = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
            logger.info(f"ğŸ“Š í•™ìŠµ ì„¸ì…˜ ì™„ë£Œ!")
            logger.info(f"- ì´ ì‹œê°„: {session_duration:.1f}ì‹œê°„")
            logger.info(f"- ì§ˆë¬¸ ìˆ˜: {self.current_session.questions_asked}")
            logger.info(f"- ì„±ê³µì ì¸ ë‹µë³€: {self.current_session.successful_answers}")
            logger.info(f"- ì»¤ë²„ëœ ì£¼ì œ: {len(self.current_session.topics_covered)}")
            
            # ìµœì¢… ì €ì¥
            self._save_learning_progress()
            self._save_knowledge_base()
            
            # ìµœì¢… ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ì¶œë ¥
            logger.info("\nğŸ¯ ìµœì¢… ì§ˆë¬¸ ë‹¤ì–‘ì„± ë¶„ì„:")
            self.log_diversity_metrics()
            
            # ì„¸ì…˜ ë³´ê³ ì„œ ìƒì„±
            self._generate_session_report()
            
            # ë°ì´í„°ì…‹ íë ˆì´ì…˜ ë° ë‚´ë³´ë‚´ê¸°
            if self.dataset_builder:
                logger.info("ğŸ“š ë°ì´í„°ì…‹ íë ˆì´ì…˜ ì‹œì‘...")
                curated_count = self.dataset_builder.curate_dataset(min_quality=0.7)
                if curated_count > 0:
                    # í•™ìŠµìš© í¬ë§·ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
                    training_path = self.dataset_builder.export_for_training(format="alpaca")
                    logger.info(f"âœ… í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {training_path}")
                    
                    # ë°ì´í„°ì…‹ ê²€ì¦
                    validation_results = self.dataset_builder.validate_dataset()
                    logger.info(f"ğŸ“Š ë°ì´í„°ì…‹ ê²€ì¦ ê²°ê³¼:")
                    logger.info(f"  - ì´ íë ˆì´ì…˜ëœ ë°ì´í„°: {validation_results['total_curated']}")
                    logger.info(f"  - ì¹´í…Œê³ ë¦¬ ë¶„í¬: {validation_results['category_distribution']}")
                    logger.info(f"  - ê²€ì¦ í†µê³¼: {'âœ…' if validation_results['validation_passed'] else 'âŒ'}")
                
                self.dataset_builder.save_stats()
            
            # í•™ìŠµ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
            if self.quality_monitor:
                logger.info("ğŸ“Š í•™ìŠµ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
                report_path = self.quality_monitor.export_report()
                logger.info(f"âœ… í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
                
                # í•™ìŠµ ìš”ì•½ ì¶œë ¥
                summary = self.quality_monitor.get_learning_summary()
                logger.info(f"ğŸ“ˆ í•™ìŠµ ìš”ì•½:")
                logger.info(f"  - ì´ ë°˜ë³µ: {summary['total_iterations']}")
                logger.info(f"  - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {summary['average_quality_score']:.3f}")
                logger.info(f"  - ê³¼ì í•© ê°ì§€: {'âš ï¸ ì˜ˆ' if summary['overfitting_detected'] else 'âœ… ì•„ë‹ˆì˜¤'}")
            
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” (ì„¸ì…˜ ì¢…ë£Œ ì‹œ)
            if self.failed_models_blacklist:
                logger.info(f"ğŸ”„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”: {', '.join(self.failed_models_blacklist)}")
                self.failed_models_blacklist.clear()
    
    def _evaluate_answer_quality(self, question: Dict[str, Any], answer: str, model_name: str) -> float:
        """ê°œì„ ëœ ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ì˜ˆì‹œ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì„ë² ë”© ì‚¬ìš©)
            # ì—¬ê¸°ì„œëŠ” ê¸¸ì´, í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ë“±ì„ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
            features = self._extract_features(question, answer)
            
            # PyTorch í…ì„œë¡œ ë³€í™˜
            feature_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì‹¤í–‰
            self.quality_assessor.eval()
            with torch.no_grad():
                quality_score = self.quality_assessor(feature_tensor).item()
            
            # ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹± í‰ê°€
            heuristic_score = self._heuristic_evaluation(question, answer)
            
            # ìµœì¢… ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            final_score = 0.7 * quality_score + 0.3 * heuristic_score
            
            # Experience Bufferì— ì €ì¥
            experience = {
                'question': question,
                'answer': answer,
                'model': model_name,
                'quality_score': final_score,
                'features': features
            }
            self.experience_buffer.add(experience)
            
            return final_score
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _extract_features(self, question: Dict[str, Any], answer: str) -> List[float]:
        """ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        # 1. ë‹µë³€ ê¸¸ì´ (ì •ê·œí™”)
        features.append(min(len(answer) / 1000.0, 1.0))
        
        # 2. í‚¤ì›Œë“œ í¬í•¨ ë¹„ìœ¨
        keywords = question.get('keywords', [])
        keyword_count = sum(1 for keyword in keywords if keyword.lower() in answer.lower())
        features.append(keyword_count / max(len(keywords), 1))
        
        # 3. ì½”ë“œ ë¸”ë¡ ì¡´ì¬ ì—¬ë¶€
        features.append(1.0 if '```' in answer else 0.0)
        
        # 4. ì™„ì„±ë„ (ì½”ë“œ ë¸”ë¡ì´ ì˜¬ë°”ë¥´ê²Œ ë‹«í˜”ëŠ”ì§€)
        code_blocks = answer.count('```')
        features.append(1.0 if code_blocks % 2 == 0 else 0.0)
        
        # 5. ì–¸ì–´ ì¼ê´€ì„±
        question_lang = question.get('language', 'korean')
        has_korean = any('\uac00' <= char <= '\ud7a3' for char in answer)
        has_english = any('a' <= char.lower() <= 'z' for char in answer)
        
        if question_lang == 'korean':
            features.append(1.0 if has_korean and not has_english else 0.5)
        else:
            features.append(1.0 if has_english and not has_korean else 0.5)
        
        # 6. êµ¬ì¡°í™” ìˆ˜ì¤€ (ë²ˆí˜¸, ë¶ˆë¦¿ í¬ì¸íŠ¸ ë“±)
        structured = any(marker in answer for marker in ['1.', '2.', 'â€¢', '-', '*'])
        features.append(1.0 if structured else 0.0)
        
        # 7. ì£¼ì œ ê´€ë ¨ì„± (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        topic_keywords = question.get('topic', '').split()
        topic_match = sum(1 for keyword in topic_keywords if keyword.lower() in answer.lower())
        features.append(min(topic_match / max(len(topic_keywords), 1), 1.0))
        
        # 768ì°¨ì›ìœ¼ë¡œ íŒ¨ë”© (ì‹¤ì œ ì„ë² ë”© í¬ê¸°ì— ë§ì¶¤)
        while len(features) < 768:
            features.append(0.0)
            
        return features[:768]
    
    def _heuristic_evaluation(self, question: Dict[str, Any], answer: str) -> float:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ë‹µë³€ ê¸¸ì´
        if len(answer) > 100:
            score += 0.1
        if len(answer) > 300:
            score += 0.1
            
        # ì½”ë“œ ì˜ˆì œ í¬í•¨
        if '```' in answer and answer.count('```') >= 2:
            score += 0.15
            
        # ì„¤ëª… êµ¬ì¡°í™”
        if any(marker in answer for marker in ['1.', '2.', 'ë‹¨ê³„', 'Step']):
            score += 0.1
            
        # ì£¼ì œë³„ íŠ¹ë³„ í‰ê°€
        topic = question.get('topic', '')
        if 'Godot' in topic and any(godot_term in answer for godot_term in ['Node', 'Scene', 'ë…¸ë“œ', 'ì”¬']):
            score += 0.05
        elif 'C#' in topic and any(csharp_term in answer for csharp_term in ['class', 'method', 'async']):
            score += 0.05
            
        return min(score, 1.0)
    
    def _improve_answer_with_feedback(self, question: Dict[str, Any], answer: str, model_name: str) -> str:
        """ì‹¤ì‹œê°„ í”¼ë“œë°±ì„ í†µí•œ ë‹µë³€ ê°œì„ """
        try:
            # ë‹µë³€ ë¶„ì„
            issues = self.feedback_system.analyze_partial_response(
                answer, 
                expected_language=question.get('language', 'korean')
            )
            
            if not issues:
                return answer
                
            # ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±
            context = {
                'topic': question.get('topic', ''),
                'language': question.get('language', 'korean')
            }
            improvement_prompt = self.feedback_system.get_improvement_prompt(issues, answer, context)
            
            # ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not self.currently_loaded_model or model_name != self.currently_loaded_model:
                if not self.load_model(model_name):
                    return answer
                    
            # ê°œì„ ëœ ë‹µë³€ ìƒì„±
            pipe = self.model_cache[model_name]["pipeline"]
            improved_prompt = f"{question['question']}\n\nê¸°ì¡´ ë‹µë³€:\n{answer}\n\n{improvement_prompt}"
            
            response = pipe(improved_prompt)
            improved_answer = response[0]['generated_text'] if response else answer
            
            # í”¼ë“œë°± ê¸°ë¡
            self.feedback_system.record_feedback(
                improved_answer,
                self._heuristic_evaluation(question, improved_answer),
                issues
            )
            
            return improved_answer
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ê°œì„  ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return answer
    
    def _get_task_category(self, topic: str) -> Optional[str]:
        """ì£¼ì œë¥¼ íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        topic_lower = topic.lower()
        
        if 'c#' in topic_lower or 'csharp' in topic_lower:
            return 'csharp'
        elif 'í•œê¸€' in topic_lower or 'ìš©ì–´' in topic_lower or 'korean' in topic_lower:
            return 'korean'
        elif 'godot' in topic_lower:
            return 'godot'
        elif 'socket' in topic_lower:
            return 'socketio'
        elif 'ai' in topic_lower or 'ìµœì í™”' in topic_lower:
            return 'ai_optimization'
        else:
            return None
    
    def _update_multitask_network(self, question: Dict[str, Any], answer: str, quality_score: float, task_category: str):
        """ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        try:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self._extract_features(question, answer)
            feature_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # íƒ€ê²Ÿ í’ˆì§ˆ ì ìˆ˜
            target = torch.tensor([quality_score], dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # Forward pass
            self.multitask_network.train()
            predicted_quality = self.multitask_network(feature_tensor, task_category)
            
            # Loss ê³„ì‚°
            loss = F.mse_loss(predicted_quality, target)
            
            # Backward pass
            self.multitask_optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.multitask_network.parameters(), 1.0)
            self.multitask_optimizer.step()
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.multitask_scheduler.step(quality_score)
            
            logger.debug(f"ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ - íƒœìŠ¤í¬: {task_category}, Loss: {loss.item():.4f}")
            
        except Exception as e:
            logger.error(f"ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def train_quality_assessor_batch(self):
        """Experience Bufferì—ì„œ ë°°ì¹˜ë¡œ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ"""
        if len(self.experience_buffer) < 32:
            return
            
        try:
            # ë°°ì¹˜ ìƒ˜í”Œë§
            experiences, weights, indices = self.experience_buffer.sample(32)
            if experiences is None:
                return
                
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            features_batch = []
            targets_batch = []
            
            for exp in experiences:
                features_batch.append(exp['features'])
                targets_batch.append(exp['quality_score'])
                
            # í…ì„œë¡œ ë³€í™˜
            features_tensor = torch.tensor(features_batch, dtype=torch.float32).to(self.device)
            targets_tensor = torch.tensor(targets_batch, dtype=torch.float32).unsqueeze(1).to(self.device)
            weights_tensor = torch.tensor(weights, dtype=torch.float32).unsqueeze(1).to(self.device)
            
            # Forward pass
            self.quality_assessor.train()
            predictions = self.quality_assessor(features_tensor)
            
            # Weighted loss
            loss = F.mse_loss(predictions, targets_tensor, reduction='none')
            weighted_loss = (loss * weights_tensor).mean()
            
            # Backward pass
            self.quality_optimizer.zero_grad()
            weighted_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.quality_assessor.parameters(), 1.0)
            self.quality_optimizer.step()
            
            # TD errors for priority update
            td_errors = (predictions - targets_tensor).detach().cpu().numpy().flatten()
            self.experience_buffer.update_priorities(indices, td_errors)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            avg_quality = targets_tensor.mean().item()
            self.quality_scheduler.step(avg_quality)
            
            logger.debug(f"í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ - Loss: {weighted_loss.item():.4f}, Avg Quality: {avg_quality:.2f}")
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def save_pytorch_models(self):
        """PyTorch ëª¨ë¸ë“¤ ì €ì¥"""
        try:
            models_dir = self.learning_dir / "pytorch_models"
            models_dir.mkdir(exist_ok=True)
            
            # í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ì €ì¥
            torch.save({
                'model_state_dict': self.quality_assessor.state_dict(),
                'optimizer_state_dict': self.quality_optimizer.state_dict(),
                'scheduler_state_dict': self.quality_scheduler.state_dict()
            }, models_dir / f"quality_assessor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth")
            
            # ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ì €ì¥
            torch.save({
                'model_state_dict': self.multitask_network.state_dict(),
                'optimizer_state_dict': self.multitask_optimizer.state_dict(),
                'scheduler_state_dict': self.multitask_scheduler.state_dict()
            }, models_dir / f"multitask_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth")
            
            logger.info("ğŸ“ PyTorch ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def load_pytorch_models(self):
        """ì €ì¥ëœ PyTorch ëª¨ë¸ ë¡œë“œ"""
        try:
            models_dir = self.learning_dir / "pytorch_models"
            if not models_dir.exists():
                return
                
            # ìµœì‹  í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ
            quality_models = sorted(models_dir.glob("quality_assessor_*.pth"))
            if quality_models:
                checkpoint = torch.load(quality_models[-1], map_location=self.device)
                self.quality_assessor.load_state_dict(checkpoint['model_state_dict'])
                self.quality_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.quality_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                logger.info(f"âœ… í’ˆì§ˆ í‰ê°€ ëª¨ë¸ ë¡œë“œ: {quality_models[-1].name}")
                
            # ìµœì‹  ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
            multitask_models = sorted(models_dir.glob("multitask_network_*.pth"))
            if multitask_models:
                checkpoint = torch.load(multitask_models[-1], map_location=self.device)
                self.multitask_network.load_state_dict(checkpoint['model_state_dict'])
                self.multitask_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.multitask_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                logger.info(f"âœ… ë©€í‹°íƒœìŠ¤í¬ ë„¤íŠ¸ì›Œí¬ ë¡œë“œ: {multitask_models[-1].name}")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def _generate_session_report(self):
        """ì„¸ì…˜ ë³´ê³ ì„œ ìƒì„±"""
        if not self.current_session:
            return
        
        report = {
            "session_id": self.current_session.session_id,
            "start_time": self.current_session.start_time.isoformat(),
            "end_time": datetime.now().isoformat(),
            "duration_hours": (datetime.now() - self.current_session.start_time).total_seconds() / 3600,
            "questions_asked": self.current_session.questions_asked,
            "successful_answers": self.current_session.successful_answers,
            "success_rate": self.current_session.successful_answers / max(1, self.current_session.questions_asked),
            "topics_covered": self.current_session.topics_covered,
            "models_used": self.current_session.models_used,
            "knowledge_gained": len(self.current_session.knowledge_gained),
            "memory_usage_history": self.memory_usage_history[-100:],  # ìµœê·¼ 100ê°œë§Œ
            "experience_buffer_size": len(self.experience_buffer),
            "feedback_history": list(self.feedback_system.feedback_history)
        }
        
        # PyTorch í†µê³„ ì¶”ê°€
        if self.pytorch_system:
            report['pytorch_stats'] = self.pytorch_system.training_stats
        
        # ì§ˆë¬¸ ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ì¶”ê°€
        report['diversity_metrics'] = self.calculate_question_diversity_metrics()
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = self.learning_dir / f"session_{self.current_session.session_id}_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ì„¸ì…˜ ë³´ê³ ì„œ ì €ì¥: {report_file}")

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ í¬í•¨)"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AutoCI ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ")
    parser.add_argument("duration", type=float, nargs='?', default=24, 
                        help="í•™ìŠµ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)")
    parser.add_argument("memory", type=float, nargs='?', default=32.0,
                        help="ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB, ê¸°ë³¸ê°’: 32)")
    
    args = parser.parse_args()
    
    # ì´ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
    total_start_time = datetime.now()
    target_end_time = total_start_time + timedelta(hours=args.duration)
    restart_count = 0
    max_restarts = 5
    
    logger.info(f"ğŸš€ AutoCI í•™ìŠµ ì‹œì‘ - ëª©í‘œ ì‹œê°„: {args.duration}ì‹œê°„")
    logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ: {args.memory}GB")
    
    while datetime.now() < target_end_time and restart_count < max_restarts:
        try:
            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            learning_system = ContinuousLearningSystem(max_memory_gb=args.memory)
            
            # ë‚¨ì€ ì‹œê°„ ê³„ì‚°
            remaining_hours = (target_end_time - datetime.now()).total_seconds() / 3600
            if remaining_hours <= 0:
                break
            
            logger.info(f"ğŸ“š í•™ìŠµ ì„¸ì…˜ ì‹œì‘ - ë‚¨ì€ ì‹œê°„: {remaining_hours:.1f}ì‹œê°„")
            
            # í•™ìŠµ ì‹¤í–‰
            await learning_system.continuous_learning_loop(duration_hours=remaining_hours)
            
            # ì •ìƒ ì™„ë£Œ
            logger.info("âœ… í•™ìŠµ ì„¸ì…˜ ì •ìƒ ì™„ë£Œ")
            break
            
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            break
            
        except Exception as e:
            restart_count += 1
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œì‘ {restart_count}/{max_restarts}): {str(e)}")
            logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            
            if restart_count < max_restarts:
                wait_time = min(60 * restart_count, 300)  # ìµœëŒ€ 5ë¶„
                logger.info(f"â³ {wait_time}ì´ˆ í›„ ìë™ ì¬ì‹œì‘...")
                await asyncio.sleep(wait_time)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            else:
                logger.critical("âŒ ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜ ì´ˆê³¼ - í•™ìŠµ ì¤‘ë‹¨")
                break
    
    # ìµœì¢… ìš”ì•½
    total_duration = (datetime.now() - total_start_time).total_seconds() / 3600
    logger.info(f"\nğŸ“Š ìµœì¢… í•™ìŠµ ìš”ì•½:")
    logger.info(f"- ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì‹œê°„")
    logger.info(f"- ì¬ì‹œì‘ íšŸìˆ˜: {restart_count}íšŒ")
    logger.info(f"- ìƒíƒœ: {'ì™„ë£Œ' if restart_count == 0 else 'ë¶€ë¶„ ì™„ë£Œ'}")

if __name__ == "__main__":
    asyncio.run(main())