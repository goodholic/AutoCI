#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ - ì €ì‚¬ì–‘ ìµœì í™” ë²„ì „
C#ê³¼ í•œê¸€ì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
Llama-3.1-8B, CodeLlama-13B, Qwen2.5-Coder-32B ëª¨ë¸ì„ í™œìš©

RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ì— ìµœì í™”ë¨
autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import random
import asyncio
import logging
import gc
import psutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM
)
try:
    from transformers.utils.quantization_config import BitsAndBytesConfig
except ImportError:
    try:
        from transformers import BitsAndBytesConfig
    except ImportError:
        BitsAndBytesConfig = None
        
try:
    from transformers.pipelines import pipeline
except ImportError:
    try:
        from transformers import pipeline
    except ImportError:
        pipeline = None

# ì •ë³´ ìˆ˜ì§‘ê¸° ì„í¬íŠ¸
try:
    from modules.intelligent_information_gatherer import get_information_gatherer
    INFORMATION_GATHERER_AVAILABLE = True
    print("ğŸŒ ì§€ëŠ¥í˜• ì •ë³´ ìˆ˜ì§‘ê¸° í™œì„±í™”!")
except ImportError:
    INFORMATION_GATHERER_AVAILABLE = False
    print("âš ï¸ ì •ë³´ ìˆ˜ì§‘ê¸°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from modules.ai_model_controller import AIModelController
    MODEL_CONTROLLER_AVAILABLE = True
    print("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ í™œì„±í™”!")
except ImportError:
    MODEL_CONTROLLER_AVAILABLE = False
    print("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('continuous_learning.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from modules.pytorch_deep_learning_module import AutoCIPyTorchLearningSystem
    PYTORCH_AVAILABLE = True
    print("ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆ í™œì„±í™”!")
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ë”¥ëŸ¬ë‹ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# Hugging Face í† í°
HF_TOKEN = "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"

@dataclass
class LearningTopic:
    """í•™ìŠµ ì£¼ì œ"""
    id: str
    category: str
    topic: str
    difficulty: int  # 1-5
    korean_keywords: List[str]
    python_concepts: List[str]
    godot_integration: Optional[str] = None
    
@dataclass
class LearningSession:
    """í•™ìŠµ ì„¸ì…˜"""
    session_id: str
    start_time: datetime
    topics_covered: List[str]
    questions_asked: int
    successful_answers: int
    models_used: Dict[str, int]
    knowledge_gained: List[Dict[str, Any]]
    
class ContinuousLearningSystem:
    def __init__(self, models_dir: str = "./models", learning_dir: str = "./continuous_learning", max_memory_gb: float = 32.0):
        self.models_dir = Path(models_dir)
        self.learning_dir = Path(learning_dir)
        self.learning_dir.mkdir(exist_ok=True)
        
        # ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if MODEL_CONTROLLER_AVAILABLE:
            self.model_controller = AIModelController()
            print("ğŸ¯ AI ëª¨ë¸ ì¡°ì¢…ê¶Œ í™•ë³´ ì™„ë£Œ!")
            logger.info("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            self.model_controller = None
            logger.warning("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • (ì €ì‚¬ì–‘ ìµœì í™”)
        self.max_memory_gb = max_memory_gb
        self.memory_threshold = 0.70  # 70% ì‚¬ìš© ì‹œ ëª¨ë¸ ì–¸ë¡œë“œ (ë” ë³´ìˆ˜ì )
        self.currently_loaded_model = None
        self.model_cache = {}  # ì–¸ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì €ì¥
        
        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        self.questions_dir = self.learning_dir / "questions"
        self.answers_dir = self.learning_dir / "answers"
        self.knowledge_base_dir = self.learning_dir / "knowledge_base"
        self.progress_dir = self.learning_dir / "progress"
        
        for dir in [self.questions_dir, self.answers_dir, self.knowledge_base_dir, self.progress_dir]:
            dir.mkdir(exist_ok=True)
            
        # ëª¨ë¸ ì •ë³´ (ì‹¤ì œ ë¡œë”©ì€ í•„ìš”í•  ë•Œë§Œ)
        self.available_models = {}
        self.load_model_info()
        
        # í•™ìŠµ ì£¼ì œ ì •ì˜
        self.learning_topics = self._initialize_learning_topics()
        
        # í˜„ì¬ ì„¸ì…˜
        self.current_session = None
        
        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_base = self._load_knowledge_base()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        self.memory_usage_history = []
        
        # í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ
        self.learning_progress = self._load_learning_progress()
        
        # ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì
        try:
            from modules.progressive_learning_manager import ProgressiveLearningManager
            self.progressive_manager = ProgressiveLearningManager(self.learning_dir)
            logger.info("ğŸ“ˆ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì í™œì„±í™”")
        except:
            self.progressive_manager = None
            logger.warning("âš ï¸ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # PyTorch ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if PYTORCH_AVAILABLE:
            try:
                self.pytorch_system = AutoCIPyTorchLearningSystem(base_path=str(Path(__file__).parent.parent))
                logger.info("ğŸ§  PyTorch ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                
                # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
                pytorch_models_dir = Path(self.models_dir).parent / "pytorch_models"
                if pytorch_models_dir.exists():
                    latest_dl_model = sorted(pytorch_models_dir.glob("deep_learning_model_*.pth"))
                    latest_rl_model = sorted(pytorch_models_dir.glob("rl_agent_*.pth"))
                    
                    if latest_dl_model:
                        self.pytorch_system.load_models(dl_path=str(latest_dl_model[-1]))
                    if latest_rl_model:
                        self.pytorch_system.load_models(rl_path=str(latest_rl_model[-1]))
                        
            except Exception as e:
                self.pytorch_system = None
                logger.error(f"PyTorch ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        else:
            self.pytorch_system = None
        
    def _initialize_learning_topics(self) -> List[LearningTopic]:
        """5ê°€ì§€ í•µì‹¬ í•™ìŠµ ì£¼ì œ ì´ˆê¸°í™” (DeepSeek-coder ìµœì í™”)"""
        topics = [
            # 1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° (ë³€í˜•ëœ Godotìš©)
            LearningTopic("csharp_basics", "C# í”„ë¡œê·¸ë˜ë°", "C# ê¸°ì´ˆ ë¬¸ë²•", 1,
                         ["ë³€ìˆ˜", "ë°ì´í„°íƒ€ì…", "ë©”ì„œë“œ", "í´ë˜ìŠ¤"],
                         ["variable", "datatype", "method", "class"],
                         "Godot C# ê¸°ì´ˆ"),
            LearningTopic("csharp_advanced", "C# í”„ë¡œê·¸ë˜ë°", "C# ê³ ê¸‰ ê¸°ëŠ¥", 3,
                         ["ë¸ë¦¬ê²Œì´íŠ¸", "ì´ë²¤íŠ¸", "LINQ", "async/await"],
                         ["delegate", "event", "LINQ", "async"],
                         "Godot C# ê³ ê¸‰ ê¸°ëŠ¥"),
            # 2ï¸âƒ£ í•œê¸€ í”„ë¡œê·¸ë˜ë° ìš©ì–´
            LearningTopic("korean_terms_basic", "í•œê¸€ ìš©ì–´", "í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ìš©ì–´", 1,
                         ["ë³€ìˆ˜", "í•¨ìˆ˜", "í´ë˜ìŠ¤", "ê°ì²´", "ìƒì†"],
                         ["variable", "function", "class", "object", "inheritance"],
                         "í•œ-ì˜ ìš©ì–´ ë§¤í•‘"),
            LearningTopic("korean_terms_advanced", "í•œê¸€ ìš©ì–´", "ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ìš©ì–´", 3,
                         ["ë‹¤í˜•ì„±", "ìº¡ìŠí™”", "ì¶”ìƒí™”", "ì¸í„°í˜ì´ìŠ¤"],
                         ["polymorphism", "encapsulation", "abstraction", "interface"],
                         "ì „ë¬¸ ìš©ì–´ ì´í•´"),
            # 3ï¸âƒ£ ë³€í˜•ëœ Godot ì—”ì§„
            LearningTopic("godot_basics", "ë³€í˜•ëœ Godot", "Godot ê¸°ì´ˆ", 2,
                         ["ë…¸ë“œ", "ì”¬", "ì‹œê·¸ë„", "ìŠ¤í¬ë¦½íŠ¸"],
                         ["Node", "Scene", "Signal", "Script"],
                         "Godot ê¸°ë³¸ êµ¬ì¡°"),
            LearningTopic("godot_advanced", "ë³€í˜•ëœ Godot", "Godot ê³ ê¸‰", 4,
                         ["ì»¤ìŠ¤í…€ë…¸ë“œ", "ì…°ì´ë”", "ë¬¼ë¦¬ì—”ì§„", "ìµœì í™”"],
                         ["CustomNode", "Shader", "Physics2D/3D", "Optimization"],
                         "Godot í™•ì¥ ê°œë°œ"),
            # 4ï¸âƒ£ Socket.IO ë„¤íŠ¸ì›Œí‚¹
            LearningTopic("socketio_basic", "Socket.IO", "ì‹¤ì‹œê°„ í†µì‹  ê¸°ì´ˆ", 3,
                         ["ì†Œì¼“", "ì´ë²¤íŠ¸", "ë£¸", "ë„¤ì„ìŠ¤í˜ì´ìŠ¤"],
                         ["Socket", "Event", "Room", "Namespace"],
                         "Socket.IO ê¸°ë³¸ í†µì‹ "),
            LearningTopic("socketio_advanced", "Socket.IO", "ê³ ê¸‰ ì‹¤ì‹œê°„ í†µì‹ ", 5,
                         ["ë¸Œë¡œë“œìºìŠ¤íŠ¸", "ë¯¸ë“¤ì›¨ì–´", "í´ëŸ¬ìŠ¤í„°ë§", "Redis"],
                         ["Broadcast", "Middleware", "Clustering", "Redis"],
                         "Socket.IO ê³ ê¸‰ ê¸°ëŠ¥"),
            # 5ï¸âƒ£ AI ìµœì í™”
            LearningTopic("ai_optimization_basic", "AI ìµœì í™”", "AI ì½”ë“œ ìƒì„± ê¸°ì´ˆ", 3,
                         ["í”„ë¡¬í”„íŠ¸", "ì»¨í…ìŠ¤íŠ¸", "í† í°", "ì‘ë‹µ"],
                         ["Prompt", "Context", "Token", "Response"],
                         "AI ê¸°ë°˜ ì½”ë“œ ìƒì„±"),
            LearningTopic("ai_optimization_advanced", "AI ìµœì í™”", "AI ê³ ê¸‰ ìµœì í™”", 5,
                         ["íŒŒì¸íŠœë‹", "í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§", "ì»¨í…ìŠ¤íŠ¸ê´€ë¦¬", "ì²´ì´ë‹"],
                         ["FineTuning", "PromptEngineering", "ContextManagement", "Chaining"],
                         "AI ì„±ëŠ¥ ìµœì í™”"),
            # 6ï¸âƒ£ ë³€í˜•ëœ Godot ì „ë¬¸ê°€ í•™ìŠµ
            LearningTopic("godot_expert_architecture", "Godot ì „ë¬¸ê°€", "ë³€í˜•ëœ Godot ì•„í‚¤í…ì²˜", 5,
                         ["ì»¤ìŠ¤í…€ì—”ì§„", "ë Œë”íŒŒì´í”„ë¼ì¸", "ì”¬ì‹œìŠ¤í…œ", "ë¦¬ì†ŒìŠ¤ê´€ë¦¬"],
                         ["CustomEngine", "RenderPipeline", "SceneSystem", "ResourceManager"],
                         "Godot í•µì‹¬ êµ¬ì¡°"),
            LearningTopic("godot_expert_csharp", "Godot ì „ë¬¸ê°€", "C# ê³ ê¸‰ í†µí•©", 5,
                         ["GDExtension", "NativeCall", "ë©”ëª¨ë¦¬ê´€ë¦¬", "ì„±ëŠ¥ìµœì í™”"],
                         ["GDExtension", "NativeCall", "MemoryManagement", "Performance"],
                         "C# ê³ ê¸‰ ê²Œì„ ê°œë°œ"),
            # 7ï¸âƒ£ Godot ì—”ì§„ ì¡°ì‘ (ê°€ìƒ ì…ë ¥)
            LearningTopic("godot_manipulation_basic", "Godot ì¡°ì‘", "ê¸°ë³¸ ì—ë””í„° ì¡°ì‘", 2,
                         ["ë…¸ë“œìƒì„±", "ì”¬êµ¬ì„±", "ì†ì„±ì„¤ì •", "ìŠ¤í¬ë¦½íŠ¸ì—°ê²°"],
                         ["NodeCreation", "SceneSetup", "PropertyConfig", "ScriptAttach"],
                         "ì—ë””í„° ê¸°ë³¸ ì¡°ì‘"),
            LearningTopic("godot_manipulation_advanced", "Godot ì¡°ì‘", "ê³ ê¸‰ ìë™í™” ì¡°ì‘", 4,
                         ["ë³µì¡í•œì”¬êµ¬ì„±", "ì• ë‹ˆë©”ì´ì…˜ì„¤ì •", "ë¬¼ë¦¬ì„¤ì •", "ìµœì í™”ì‘ì—…"],
                         ["ComplexScene", "AnimationSetup", "PhysicsConfig", "Optimization"],
                         "ìë™í™” ì›Œí¬í”Œë¡œìš°"),
        ]
        
        # ëª¨ë“  ì£¼ì œì— DeepSeek-coder ìš°ì„  íƒœê·¸ ì¶”ê°€
        for topic in topics:
            topic.godot_integration = f"[DeepSeek ìš°ì„ ] {topic.godot_integration}"
            
        return topics
        
    def _load_knowledge_base(self) -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "csharp_patterns": {},
            "korean_translations": {},
            "godot_integrations": {},
            "common_errors": {},
            "best_practices": {}
        }
        
    def _save_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, indent=2, ensure_ascii=False)
            
    def _load_learning_progress(self) -> Dict[str, Any]:
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        progress_file = self.progress_dir / "learning_progress.json"
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
                logger.info(f"ê¸°ì¡´ í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {progress.get('total_hours', 0)}ì‹œê°„")
                return progress
        return {
            "total_hours": 0,
            "total_questions": 0,
            "total_successful": 0,
            "topics_progress": {},
            "last_session_id": None,
            "last_save_time": None,
            "sessions_completed": []
        }
        
    def _save_learning_progress(self):
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ì €ì¥"""
        if self.current_session:
            # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
            session_duration = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
            
            self.learning_progress["total_hours"] += session_duration
            self.learning_progress["total_questions"] += self.current_session.questions_asked
            self.learning_progress["total_successful"] += self.current_session.successful_answers
            self.learning_progress["last_session_id"] = self.current_session.session_id
            self.learning_progress["last_save_time"] = datetime.now().isoformat()
            
            # ì£¼ì œë³„ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            if self.current_session.topics_covered:
                for topic in self.current_session.topics_covered:
                    if topic not in self.learning_progress["topics_progress"]:
                        self.learning_progress["topics_progress"][topic] = {
                            "questions_asked": 0,
                            "successful_answers": 0,
                            "last_studied": None
                        }
                    self.learning_progress["topics_progress"][topic]["questions_asked"] += 1
                    self.learning_progress["topics_progress"][topic]["last_studied"] = datetime.now().isoformat()
        
        progress_file = self.progress_dir / "learning_progress.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.learning_progress, f, indent=2, ensure_ascii=False)
        logger.info(f"í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {self.learning_progress['total_hours']:.1f}ì‹œê°„")
            
    def load_model_info(self):
        """ëª¨ë¸ ì •ë³´ë§Œ ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ì€ í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)"""
        models_info_file = self.models_dir / "installed_models.json"
        if not models_info_file.exists():
            logger.error("ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. install_llm_models.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        with open(models_info_file, 'r', encoding='utf-8') as f:
            installed_models = json.load(f)
            
        self.available_models = installed_models
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.available_models.keys())}")
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB)"""
        return psutil.virtual_memory().used / (1024**3)
        
    def get_memory_usage_percent(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜ (%)"""
        return psutil.virtual_memory().percent
        
    def check_memory_safety(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì•ˆì „í•œì§€ í™•ì¸"""
        current_usage = self.get_memory_usage()
        
        # í˜„ì¬ ì‚¬ìš©ëŸ‰ì´ ìµœëŒ€ í—ˆìš©ëŸ‰ì˜ 85%ë¥¼ ë„˜ìœ¼ë©´ ìœ„í—˜
        return current_usage < (self.max_memory_gb * self.memory_threshold)
        
    def unload_current_model(self):
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.currently_loaded_model:
            logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ {self.currently_loaded_model} ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.currently_loaded_model = None
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
    def load_model(self, model_name: str) -> bool:
        """í•„ìš” ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if model_name == self.currently_loaded_model:
            return True  # ì´ë¯¸ ë¡œë“œë¨
            
        if model_name not in self.available_models:
            logger.error(f"ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False
            
        # ë©”ëª¨ë¦¬ í™•ì¸ í›„ í•„ìš”ì‹œ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
        if not self.check_memory_safety() and self.currently_loaded_model:
            self.unload_current_model()
            
        try:
            print(f"ğŸ”„ {model_name} ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            logger.info(f"{model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            info = self.available_models[model_name]
            model_id = info['model_id']
            print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_id}")
            
            # RTX 2080 8GB ìµœì í™” ëª¨ë¸ë§Œ í—ˆìš©
            rtx_2080_optimized = {
                "bitnet-b1.58-2b": {"max_vram": 1, "device": "cpu"},
                "gemma-4b": {"max_vram": 4, "device": "cuda:0"},
                "phi3-mini": {"max_vram": 6, "device": "cuda:0"},
                "deepseek-coder-7b": {"max_vram": 6, "device": "cuda:0"},
                "mistral-7b": {"max_vram": 7, "device": "cuda:0"}
            }
            
            if model_name not in rtx_2080_optimized:
                logger.warning(f"âŒ {model_name}ì€ RTX 2080 8GBì— ìµœì í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {', '.join(rtx_2080_optimized.keys())}")
                logger.info("ğŸ’¡ install_llm_models_rtx2080.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì í™”ëœ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
                return False
            
            model_config = rtx_2080_optimized[model_name]
            logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {model_name} (VRAM: {model_config['max_vram']}GB)")
            
            # AutoTokenizerì™€ AutoModelForCausalLMì„ ì§ì ‘ ì‚¬ìš©
            from transformers import AutoTokenizer, AutoModelForCausalLM
            try:
                from transformers import BitsAndBytesConfig
            except ImportError:
                from transformers.utils.quantization_config import BitsAndBytesConfig
            
            hf_token = os.getenv('HF_TOKEN', None)
            
            # ëª¨ë¸ë³„ ìµœì  ì„¤ì •
            device_map = model_config['device']
            torch_dtype = torch.float32 if device_map == "cpu" else torch.float16
            quantization_config = None
            
            # 4bit ì–‘ìí™” ì„¤ì • (GPU ëª¨ë¸ìš©)
            if device_map != "cpu" and info.get('quantization') == '4bit':
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ìš© í† í¬ë‚˜ì´ì € ê²½ë¡œ ì²˜ë¦¬
            tokenizer_path = model_id
            if model_name in ["deepseek-coder-7b", "llama-3.1-8b"]:
                # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ì€ í† í¬ë‚˜ì´ì € í´ë” ì‚¬ìš©
                tokenizer_path = info.get('tokenizer_path', model_id)
                if not tokenizer_path.startswith('./'):
                    tokenizer_path = f"./{tokenizer_path}"
                print(f"ğŸ“ í† í¬ë‚˜ì´ì € ê²½ë¡œ: {tokenizer_path}")
            
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path, 
                token=hf_token,
                trust_remote_code=True,
                local_files_only=model_id.startswith('./models/')  # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            if device_map == "cpu":
                model_kwargs["device_map"] = "cpu"
            else:
                model_kwargs["device_map"] = {"": 0}  # GPU 0 ì‚¬ìš©
                if quantization_config:
                    model_kwargs["quantization_config"] = quantization_config
            
            if hf_token and not model_id.startswith('./models/'):
                model_kwargs["token"] = hf_token
            
            # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ë¡œë“œ
            if model_id.startswith('./models/'):
                model_kwargs["local_files_only"] = True
                print(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            else:
                print(f"ğŸ”„ Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            
            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ë˜í¼
            def optimized_generate(prompt):
                try:
                    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=1024,
                        padding=True
                    )
                    
                    if device_map != "cpu":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs.get('input_ids'),
                            attention_mask=inputs.get('attention_mask'),
                            max_new_tokens=300,  # ë” ìƒì„¸í•œ ë‹µë³€ì„ ìœ„í•´ ì¦ê°€ (ê¸°ì¡´ 150)
                            temperature=0.6,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ê°ì†Œ (ê¸°ì¡´ 0.7)
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(
                        outputs[0][inputs.get('input_ids').shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return [{"generated_text": response}]
                    
                except Exception as e:
                    logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    return [{"generated_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}]
            
            pipe = optimized_generate
            
            self.model_cache[model_name] = {
                "pipeline": pipe,
                "features": info['features'],
                "info": info
            }
            
            self.currently_loaded_model = model_name
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            memory_usage = self.get_memory_usage()
            self.memory_usage_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": model_name,
                "memory_gb": memory_usage,
                "memory_percent": self.get_memory_usage_percent()
            })
            
            logger.info(f"âœ“ {model_name} ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬: {memory_usage:.1f}GB)")
            return True
            
        except Exception as e:
            logger.error(f"âœ— {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
                
    def _get_quantization_config(self, quantization: str):
        """ì–‘ìí™” ì„¤ì • ë°˜í™˜ (RTX 2080 8GB ìµœì í™”)"""
        if quantization == "4bit":
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True
            )
        elif quantization == "8bit":
            return BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True,  # RTX 2080 8GB í•„ìˆ˜
                llm_int8_has_fp16_weight=False  # ë©”ëª¨ë¦¬ ì ˆì•½
            )
        return None
        
    def generate_question(self, topic: LearningTopic) -> Dict[str, Any]:
        """í•™ìŠµ ì§ˆë¬¸ ìƒì„±"""
        question_types = [
            "explain",      # ê°œë… ì„¤ëª…
            "example",      # ì˜ˆì œ ì½”ë“œ
            "translate",    # í•œê¸€-ì˜ì–´ ë²ˆì—­
            "error",        # ì˜¤ë¥˜ ìˆ˜ì •
            "optimize",     # ìµœì í™”
            "integrate"     # Godot í†µí•©
        ]
        
        # "Godot ì „ë¬¸ê°€" ì£¼ì œì¸ ê²½ìš°, ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ìƒì„±
        if topic.category == "Godot ì „ë¬¸ê°€":
            try:
                with open("collected_godot_docs.json", "r", encoding="utf-8") as f:
                    docs_data = json.load(f)
                
                if docs_data:
                    doc = random.choice(docs_data)
                    question_text = f"{doc['title']}ì— ëŒ€í•´ ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: \n\n{doc['content'][:500]}"
                    return {
                        "id": f"{topic.id}_from_doc_{int(time.time())}",
                        "topic": topic.topic,
                        "type": "doc_based_qna",
                        "language": "korean",
                        "difficulty": topic.difficulty,
                        "question": question_text,
                        "keywords": topic.korean_keywords
                    }
            except FileNotFoundError:
                pass # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ë„˜ì–´ê°

        question_type = random.choice(question_types)
        
        # ì§ˆë¬¸ í…œí”Œë¦¿
        templates = {
            "explain": {
                "korean": f"{topic.topic}ì— ëŒ€í•´ í•œê¸€ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. íŠ¹íˆ {random.choice(topic.korean_keywords)}ì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.",
                "english": f"Explain {topic.topic} in Python with focus on {random.choice(topic.python_concepts)}."
            },
            "example": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•˜ëŠ” Python ì½”ë“œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê³  í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"Write a Python code example demonstrating {topic.topic} with comments."
            },
            "translate": {
                "korean": f"ë‹¤ìŒ Python ê°œë…ì„ í•œê¸€ë¡œ ë²ˆì—­í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”: {random.choice(topic.python_concepts)}",
                "english": f"Translate and explain this Korean term in Python context: {random.choice(topic.korean_keywords)}"
            },
            "error": {
                "korean": f"{topic.topic} ê´€ë ¨ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"What are common errors with {topic.topic} in Python and how to fix them?"
            },
            "optimize": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to optimize performance when using {topic.topic} in Python?"
            },
            "integrate": {
                "korean": f"Godotì—ì„œ {topic.topic}ì„ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ C# ì½”ë“œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to use {topic.topic} in Godot with C#? Provide examples."
            }
        }
        
        # ì–¸ì–´ ì„ íƒ (í•œê¸€ í•™ìŠµ ê°•ì¡°)
        language = "korean" if random.random() < 0.7 else "english"
        question_text = templates[question_type][language]
        
        return {
            "id": f"{topic.id}_{question_type}_{int(time.time())}",
            "topic": topic.topic,
            "type": question_type,
            "language": language,
            "difficulty": topic.difficulty,
            "question": question_text,
            "keywords": topic.korean_keywords if language == "korean" else topic.python_concepts
        }
        
    def select_model_for_question(self, question: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ (5ëŒ€ í•µì‹¬ ì£¼ì œ DeepSeek-coder ìµœì í™”)"""
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ ê³ ë ¤
        rtx_2080_models = {
            "deepseek-coder-7b": {"priority": 10, "specialties": ["code", "csharp", "godot", "korean", "socketio"], "vram": 6},
            "phi3-mini": {"priority": 8, "specialties": ["reasoning", "math", "python"], "vram": 6},
            "llama-3.1-8b": {"priority": 7, "specialties": ["general", "korean", "python"], "vram": 7},
            "gemma-4b": {"priority": 6, "specialties": ["general", "korean"], "vram": 4},
            "mistral-7b": {"priority": 4, "specialties": ["general", "fast"], "vram": 7}
        }
        
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ í•„í„°ë§
        available_optimized = []
        for model_name in self.available_models:
            if (model_name in rtx_2080_models and 
                self.available_models[model_name].get('rtx_2080_optimized', False)):
                available_optimized.append(model_name)
        
        if not available_optimized:
            logger.warning("âŒ RTX 2080 ìµœì í™” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ DeepSeek-coder 6.7Bë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            # Return a default model if available
            if self.available_models:
                return list(self.available_models.keys())[0]
            return "deepseek-coder-7b"  # fallback model name
            
        # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¹´í…Œê³ ë¦¬ í™•ì¸
        core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "ë³€í˜•ëœ Godot", "Socket.IO", "AI ìµœì í™”"]
        topic_category = question.get("category", "")
        is_core_topic = topic_category in core_categories
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ RTX 2080 ìµœì í™”ì´ê³  ì í•©í•˜ë©´ ê³„ì† ì‚¬ìš©
        if (self.currently_loaded_model and 
            self.currently_loaded_model in available_optimized and
            self._is_model_suitable(self.currently_loaded_model, question)):
            return self.currently_loaded_model
            
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„ (5ê°€ì§€ í•µì‹¬ ì£¼ì œ í¬í•¨)
        question_features = set()
        question_text = question.get("question", "").lower()
        topic_text = question.get("topic", "").lower()
        
        # 1ï¸âƒ£ Python í”„ë¡œê·¸ë˜ë° íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['code', 'programming', 'script', 'function', 'class', 'method', 'python', 'py']):
            question_features.add('python')
            question_features.add('code')
        
        # 2ï¸âƒ£ í•œê¸€ í”„ë¡œê·¸ë˜ë° ìš©ì–´ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['korean', 'í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­', 'ìš©ì–´', 'ê°œë…', 'ì–¸ì–´']):
            question_features.add('korean')
        
        # 3ï¸âƒ£ Godot ì—”ì§„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['godot', 'node', 'scene', 'ë…¸ë“œ', 'ì”¬']):
            question_features.add('godot')
        
        # 4ï¸âƒ£ Socket.IO ë„¤íŠ¸ì›Œí‚¹ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['socket', 'socketio', 'realtime', 'ì†Œì¼“', 'ì‹¤ì‹œê°„']):
            question_features.add('socketio')
        
        # 5ï¸âƒ£ AI ìµœì í™” íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['ai', 'optimize', 'prompt', 'ìµœì í™”', 'í”„ë¡¬í”„íŠ¸']):
            question_features.add('ai_optimization')
        
        # ëª¨ë¸ ì ìˆ˜ ê³„ì‚°
        model_scores = []
        for model_name in available_optimized:
            model_info = rtx_2080_models[model_name]
            score = model_info['priority']
            
            # íŠ¹ì„± ë§¤ì¹­ ì ìˆ˜
            for feature in question_features:
                if feature in model_info['specialties']:
                    score += 5
            
            # ëª¨ë¸ë³„ íŠ¹ë³„ ë³´ë„ˆìŠ¤
            if model_name == "llama-3.1-8b" and 'korean' in question_features:
                score += 20  # í•œêµ­ì–´ íŠ¹í™” ë³´ë„ˆìŠ¤
            elif model_name == "gemma-4b" and 'korean' in question_features:
                score += 12  # í•œêµ­ì–´ íŠ¹í™” ë³´ë„ˆìŠ¤
            elif model_name == "deepseek-coder-7b":
                if 'code' in question_features or 'python' in question_features:
                    score += 10  # ì½”ë“œ íŠ¹í™” ë³´ë„ˆìŠ¤
                if is_core_topic:
                    score += 15  # DeepSeek-coderì— í•µì‹¬ ì£¼ì œ ë³´ë„ˆìŠ¤
            
            model_scores.append((model_name, score))
        
        # ì ìˆ˜ê°€ ë†’ì€ ëª¨ë¸ ì„ íƒ
        model_scores.sort(key=lambda x: x[1], reverse=True)
        
        if model_scores:
            selected_model = model_scores[0][0]
            logger.info(f"ì„ íƒëœ ëª¨ë¸: {selected_model} (ì ìˆ˜: {model_scores[0][1]})")
            return selected_model
        else:
            logger.error("ì í•©í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return list(self.available_models.keys())[0] if self.available_models else "deepseek-coder-7b"
    
    def _is_model_suitable(self, model_name: str, question: Dict[str, Any]) -> bool:
        """ëª¨ë¸ì´ ì§ˆë¬¸ì— ì í•©í•œì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ ì í•©ì„± ê²€ì‚¬
        question_text = question.get("question", "").lower()
        
        if model_name == "deepseek-coder-7b":
            # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì— ë§¤ìš° ì í•©
            return any(word in question_text for word in ['code', 'function', 'class', 'godot', 'csharp'])
        elif model_name == "llama-3.1-8b":
            # í•œêµ­ì–´ ì§ˆë¬¸ì— ì í•©
            return any(word in question_text for word in ['í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­'])
        
        return True  # ê¸°ë³¸ì ìœ¼ë¡œ ì í•©í•˜ë‹¤ê³  ê°€ì •
    
    async def ask_and_learn(self, question: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """ì§ˆë¬¸í•˜ê³  ë‹µë³€ í•™ìŠµ"""
        try:
            # ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ìºì‹œëœ ì •ë³´ í™•ì¸
            from modules.shared_knowledge_base import get_shared_knowledge_base
            shared_kb = get_shared_knowledge_base()
            
            # ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ìºì‹œ ê²€ìƒ‰
            question_keywords = question.get('question', '').split()[:3]  # ì²« 3ë‹¨ì–´ë¡œ ê²€ìƒ‰
            search_keyword = ' '.join(question_keywords)
            
            cached_info = await shared_kb.get_cached_search(search_keyword)
            if cached_info:
                logger.info(f"ğŸ“š ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ ë°œê²¬: {search_keyword}")
                # ìºì‹œëœ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë” ë‚˜ì€ ë‹µë³€ ìƒì„± ê°€ëŠ¥
            
            # ëª¨ë¸ ë¡œë“œ
            if not self.load_model(model_name):
                return {
                    "success": False,
                    "error": f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨"
                }
            
            # íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            pipe = self.model_cache[model_name]["pipeline"]
            
            # ë‹µë³€ ìƒì„±
            logger.info(f"ì§ˆë¬¸: {question['question'][:100]}...")
            
            start_time = time.time()
            response = pipe(question['question'])
            answer = response[0]['generated_text'] if response else "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
            
            generation_time = time.time() - start_time
            logger.info(f"ë‹µë³€ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            
            # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (PyTorch ì‹œìŠ¤í…œ í™œìš©)
            quality_score = 0.7  # ê¸°ë³¸ê°’
            if self.pytorch_system and False:  # ì„ì‹œ ë¹„í™œì„±í™” - í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì´ 0.505 ë°˜í™˜í•˜ëŠ” ë¬¸ì œ í•´ê²°
                quality_score = self.pytorch_system.assess_quality(answer)
                
                # ì£¼ì œ ë¶„ë¥˜
                classified_topic = self.pytorch_system.classify_topic(answer)
                logger.info(f"ë¶„ë¥˜ëœ ì£¼ì œ: {classified_topic}, í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}")
            
            # ë‹µë³€ ì €ì¥
            answer_data = {
                "question_id": question['id'],
                "question": question['question'],
                "answer": answer,
                "model": model_name,
                "quality_score": quality_score,
                "generation_time": generation_time,
                "timestamp": datetime.now().isoformat(),
                "topic": question['topic'],
                "language": question['language']
            }
            
            # ë‹µë³€ ë””ë ‰í† ë¦¬ì— ì €ì¥
            today = datetime.now().strftime("%Y%m%d")
            answer_dir = self.answers_dir / today
            answer_dir.mkdir(exist_ok=True)
            
            answer_file = answer_dir / f"{question['id']}.json"
            with open(answer_file, 'w', encoding='utf-8') as f:
                json.dump(answer_data, f, indent=2, ensure_ascii=False)
            
            # ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_knowledge_base(question, answer, quality_score)
            
            # ê³ í’ˆì§ˆ ë‹µë³€ì€ ê³µìœ  ì§€ì‹ ë² ì´ìŠ¤ì— ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¡œ ì €ì¥
            if quality_score > 0.8:
                await shared_kb.save_best_practice(
                    topic=question.get('topic', 'general'),
                    practice={
                        "question": question['question'],
                        "answer": answer,
                        "model": model_name,
                        "quality_score": quality_score,
                        "language": question.get('language', 'ko')
                    }
                )
                logger.info(f"ğŸ“š ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì €ì¥: {question['topic']}")
            
            # PyTorch í•™ìŠµ ë°ì´í„°ë¡œ ì¶”ê°€
            if self.pytorch_system and quality_score > 0.6:
                experience = {
                    'question': question['question'],
                    'answer': answer,
                    'quality_score': quality_score,
                    'topic': question['topic']
                }
                
                # ê°•í™”í•™ìŠµ ì—…ë°ì´íŠ¸
                state = {'quality_score': quality_score, 'topic': question['topic']}
                action = 'generate_answer'
                reward = quality_score
                next_state = {'quality_score': quality_score + 0.1, 'topic': question['topic']}
                
                self.pytorch_system.reinforcement_learning_step(state, action, reward, next_state)
            
            # ì„¸ì…˜ ì—…ë°ì´íŠ¸
            if self.current_session:
                self.current_session.questions_asked += 1
                if quality_score > 0.7:
                    self.current_session.successful_answers += 1
                
                if model_name not in self.current_session.models_used:
                    self.current_session.models_used[model_name] = 0
                self.current_session.models_used[model_name] += 1
                
                self.current_session.knowledge_gained.append({
                    "topic": question['topic'],
                    "quality": quality_score,
                    "timestamp": datetime.now().isoformat()
                })
            
            return {
                "success": True,
                "answer": answer,
                "quality_score": quality_score,
                "model": model_name,
                "generation_time": generation_time
            }
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _update_knowledge_base(self, question: Dict[str, Any], answer: str, quality_score: float):
        """ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        if quality_score < 0.6:
            return  # í’ˆì§ˆì´ ë‚®ì€ ë‹µë³€ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
        
        topic = question['topic']
        category = question.get('category', 'general')
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
        if category not in self.knowledge_base:
            self.knowledge_base[category] = {}
        
        if topic not in self.knowledge_base[category]:
            self.knowledge_base[category][topic] = []
        
        # ì§€ì‹ í•­ëª© ì¶”ê°€
        knowledge_item = {
            "question": question['question'],
            "answer": answer,
            "quality_score": quality_score,
            "timestamp": datetime.now().isoformat(),
            "keywords": question.get('keywords', [])
        }
        
        self.knowledge_base[category][topic].append(knowledge_item)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
        if len(self.knowledge_base[category][topic]) % 10 == 0:
            self._save_knowledge_base()
    
    async def continuous_learning_loop(self, duration_hours: float = 24):
        """ì—°ì† í•™ìŠµ ë£¨í”„"""
        logger.info(f"ğŸš€ {duration_hours}ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œì‘!")
        
        # ì„¸ì…˜ ì‹œì‘
        self.current_session = LearningSession(
            session_id=f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            start_time=datetime.now(),
            topics_covered=[],
            questions_asked=0,
            successful_answers=0,
            models_used={},
            knowledge_gained=[]
        )
        
        end_time = datetime.now() + timedelta(hours=duration_hours)
        
        # PyTorch í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        pytorch_experiences = []
        
        try:
            while datetime.now() < end_time:
                # ë©”ëª¨ë¦¬ ì²´í¬
                memory_usage = self.get_memory_usage_percent()
                if memory_usage > 85:
                    logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_usage:.1f}%")
                    self.unload_current_model()
                    gc.collect()
                    await asyncio.sleep(5)
                
                # í•™ìŠµ ì£¼ì œ ì„ íƒ
                topic = random.choice(self.learning_topics)
                
                # ì§ˆë¬¸ ìƒì„±
                question = self.generate_question(topic)
                question['category'] = topic.category
                
                # ëª¨ë¸ ì„ íƒ
                model_name = self.select_model_for_question(question)
                if not model_name:
                    logger.error("ì í•©í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    await asyncio.sleep(10)
                    continue
                
                # í•™ìŠµ ì‹¤í–‰
                result = await self.ask_and_learn(question, model_name)
                
                if result['success']:
                    logger.info(f"âœ“ í•™ìŠµ ì„±ê³µ! í’ˆì§ˆ: {result['quality_score']:.2f}")
                    
                    # PyTorch í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘
                    if result['quality_score'] > 0.6:
                        pytorch_experiences.append({
                            'question': question['question'],
                            'answer': result['answer'],
                            'quality_score': result['quality_score'],
                            'topic': topic.topic
                        })
                    
                    # ì£¼ì œ ì»¤ë²„ë¦¬ì§€ ì—…ë°ì´íŠ¸
                    if topic.id not in self.current_session.topics_covered:
                        self.current_session.topics_covered.append(topic.id)
                else:
                    logger.error(f"âœ— í•™ìŠµ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
                # PyTorch ë°°ì¹˜ í•™ìŠµ (100ê°œ ê²½í—˜ë§ˆë‹¤)
                if len(pytorch_experiences) >= 100 and self.pytorch_system:
                    logger.info("ğŸ§  PyTorch ë°°ì¹˜ í•™ìŠµ ì‹œì‘...")
                    self.pytorch_system.train_on_experience(pytorch_experiences, epochs=5)
                    pytorch_experiences = []  # ë¦¬ì…‹
                
                # ì§„í–‰ ìƒí™© ì €ì¥ (10ë¶„ë§ˆë‹¤)
                if self.current_session.questions_asked % 10 == 0:
                    self._save_learning_progress()
                
                # ëŒ€ê¸° ì‹œê°„ (ë„ˆë¬´ ë¹ ë¥¸ ë°˜ë³µ ë°©ì§€)
                await asyncio.sleep(random.uniform(5, 15))
                
        except KeyboardInterrupt:
            logger.info("í•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"í•™ìŠµ ë£¨í”„ ì˜¤ë¥˜: {str(e)}")
        finally:
            # ë§ˆì§€ë§‰ PyTorch í•™ìŠµ
            if pytorch_experiences and self.pytorch_system:
                logger.info("ğŸ§  ìµœì¢… PyTorch ë°°ì¹˜ í•™ìŠµ...")
                self.pytorch_system.train_on_experience(pytorch_experiences, epochs=5)
            
            # ì„¸ì…˜ ì¢…ë£Œ
            session_duration = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
            logger.info(f"ğŸ“Š í•™ìŠµ ì„¸ì…˜ ì™„ë£Œ!")
            logger.info(f"- ì´ ì‹œê°„: {session_duration:.1f}ì‹œê°„")
            logger.info(f"- ì§ˆë¬¸ ìˆ˜: {self.current_session.questions_asked}")
            logger.info(f"- ì„±ê³µì ì¸ ë‹µë³€: {self.current_session.successful_answers}")
            logger.info(f"- ì»¤ë²„ëœ ì£¼ì œ: {len(self.current_session.topics_covered)}")
            
            # ìµœì¢… ì €ì¥
            self._save_learning_progress()
            self._save_knowledge_base()
            
            # ì„¸ì…˜ ë³´ê³ ì„œ ìƒì„±
            self._generate_session_report()
    
    def _generate_session_report(self):
        """ì„¸ì…˜ ë³´ê³ ì„œ ìƒì„±"""
        if not self.current_session:
            return
        
        report = {
            "session_id": self.current_session.session_id,
            "start_time": self.current_session.start_time.isoformat(),
            "end_time": datetime.now().isoformat(),
            "duration_hours": (datetime.now() - self.current_session.start_time).total_seconds() / 3600,
            "questions_asked": self.current_session.questions_asked,
            "successful_answers": self.current_session.successful_answers,
            "success_rate": self.current_session.successful_answers / max(1, self.current_session.questions_asked),
            "topics_covered": self.current_session.topics_covered,
            "models_used": self.current_session.models_used,
            "knowledge_gained": len(self.current_session.knowledge_gained),
            "memory_usage_history": self.memory_usage_history[-100:]  # ìµœê·¼ 100ê°œë§Œ
        }
        
        # PyTorch í†µê³„ ì¶”ê°€
        if self.pytorch_system:
            report['pytorch_stats'] = self.pytorch_system.training_stats
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = self.learning_dir / f"session_{self.current_session.session_id}_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ì„¸ì…˜ ë³´ê³ ì„œ ì €ì¥: {report_file}")

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AutoCI ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ")
    parser.add_argument("duration", type=float, nargs='?', default=24, 
                        help="í•™ìŠµ ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)")
    parser.add_argument("memory", type=float, nargs='?', default=32.0,
                        help="ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB, ê¸°ë³¸ê°’: 32)")
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    learning_system = ContinuousLearningSystem(max_memory_gb=args.memory)
    
    # í•™ìŠµ ì‹¤í–‰
    await learning_system.continuous_learning_loop(duration_hours=args.duration)

if __name__ == "__main__":
    asyncio.run(main())