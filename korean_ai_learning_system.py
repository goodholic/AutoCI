#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AutoCI Korean Language Learning System - Like ChatGPT/Gemini/Claude
ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë“¤ì²˜ëŸ¼ í•œêµ­ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ

Features:
- ë‹¤êµ­ì–´ ì‚¬ì „ í›ˆë ¨ ë°©ì‹
- í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ìµœì í™”  
- ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ (RLHF)
- ìì—°ì–´ ì²˜ë¦¬ ë° ìƒì„±
- ë¬¸í™”ì  ë§¥ë½ ì´í•´
"""

import os
import json
import time
import requests
import sqlite3
import random
import re
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import Dict, List, Any, Optional
import asyncio
import aiohttp
import feedparser
from collections import defaultdict, Counter
import nltk
from konlpy.tag import Okt
import openai
from transformers import AutoTokenizer, AutoModel
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('korean_ai_learning.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KoreanTokenizerOptimizer:
    """í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € ìµœì í™”"""
    
    def __init__(self):
        self.okt = Okt()
        self.korean_patterns = {
            'honorifics': ['ë‹˜', 'ì”¨', 'ì„ ìƒë‹˜', 'êµìˆ˜ë‹˜', 'ë°•ì‚¬ë‹˜'],
            'particles': ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ'],
            'endings': ['ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í•´ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ë‹¤', 'ë‹¤'],
            'connectives': ['ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ'],
            'emotions': ['ã… ã… ', 'ã…œã…œ', 'ã…ã…', 'ã…‹ã…‹', '^^', ';;'],
            'question_words': ['ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¼ë§ˆë‚˜']
        }
        
    def analyze_korean_text(self, text: str) -> Dict[str, Any]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì‹¬ì¸µ ë¶„ì„"""
        try:
            # í˜•íƒœì†Œ ë¶„ì„
            morphs = self.okt.morphs(text)
            pos_tags = self.okt.pos(text)
            nouns = self.okt.nouns(text)
            
            # íŒ¨í„´ ë¶„ì„
            patterns_found = {}
            for pattern_type, patterns in self.korean_patterns.items():
                found = [p for p in patterns if p in text]
                if found:
                    patterns_found[pattern_type] = found
            
            # ë¬¸ì¥ ìœ í˜• ë¶„ì„
            sentence_type = self._classify_sentence_type(text)
            formality_level = self._analyze_formality(text)
            emotional_tone = self._analyze_emotion(text)
            
            return {
                'morphs': morphs,
                'pos_tags': pos_tags,
                'nouns': nouns,
                'patterns': patterns_found,
                'sentence_type': sentence_type,
                'formality': formality_level,
                'emotion': emotional_tone,
                'length': len(text),
                'morpheme_count': len(morphs)
            }
            
        except Exception as e:
            logger.error(f"í•œêµ­ì–´ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _classify_sentence_type(self, text: str) -> str:
        """ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜"""
        if text.endswith('?') or any(qw in text for qw in self.korean_patterns['question_words']):
            return 'question'
        elif text.endswith('!'):
            return 'exclamation'
        elif any(cmd in text for cmd in ['í•´ì£¼ì„¸ìš”', 'í•´ì¤˜', 'í•˜ì„¸ìš”', 'í•˜ë¼']):
            return 'command'
        else:
            return 'statement'
    
    def _analyze_formality(self, text: str) -> str:
        """ê²©ì‹ì„± ìˆ˜ì¤€ ë¶„ì„"""
        formal_endings = ['ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í•˜ì‹­ì‹œì˜¤']
        informal_endings = ['í•´', 'í•´ìš”', 'ì´ì•¼', 'ì•¼']
        
        if any(ending in text for ending in formal_endings):
            return 'formal'
        elif any(ending in text for ending in informal_endings):
            return 'informal'
        else:
            return 'neutral'
    
    def _analyze_emotion(self, text: str) -> str:
        """ê°ì • ë¶„ì„"""
        positive_words = ['ì¢‹ë‹¤', 'í–‰ë³µ', 'ê¸°ì˜ë‹¤', 'ì¦ê²ë‹¤', 'ê°ì‚¬', 'ê³ ë§™ë‹¤']
        negative_words = ['ë‚˜ì˜ë‹¤', 'ìŠ¬í”„ë‹¤', 'í™”ë‚˜ë‹¤', 'ì§œì¦', 'ì‹«ë‹¤', 'í˜ë“¤ë‹¤']
        
        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)
        
        if pos_count > neg_count:
            return 'positive'
        elif neg_count > pos_count:
            return 'negative'
        else:
            return 'neutral'

class KoreanDataCollector:
    """í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ê¸° - ChatGPT ë°©ì‹"""
    
    def __init__(self):
        self.db_path = "korean_learning_data.db"
        self.setup_database()
        self.korean_sources = {
            'news': [
                'https://rss.cnn.com/rss/cnn_topstories.rss',
                'https://www.yonhapnews.co.kr/rss/news.xml'
            ],
            'blogs': [
                'https://blog.naver.com',
                'https://tistory.com'
            ],
            'forums': [
                'https://www.reddit.com/r/korea',
                'https://www.clien.net'
            ],
            'wikipedia': 'https://ko.wikipedia.org/wiki/',
            'government': [
                'https://www.korea.kr',
                'https://www.mois.go.kr'
            ]
        }
        
    def setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS korean_corpus (
                id INTEGER PRIMARY KEY,
                source_type TEXT,
                source_url TEXT,
                title TEXT,
                content TEXT,
                collected_date TIMESTAMP,
                language_score REAL,
                quality_score REAL,
                formality_level TEXT,
                domain_category TEXT,
                processed BOOLEAN DEFAULT FALSE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS language_patterns (
                id INTEGER PRIMARY KEY,
                pattern_type TEXT,
                pattern TEXT,
                frequency INTEGER DEFAULT 1,
                context TEXT,
                effectiveness_score REAL
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_feedback (
                id INTEGER PRIMARY KEY,
                text_sample TEXT,
                human_rating INTEGER,
                ai_rating INTEGER,
                feedback_text TEXT,
                improvement_areas TEXT,
                timestamp TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def collect_multilingual_data(self):
        """ë‹¤êµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ (ChatGPT ë°©ì‹)"""
        logger.info("ğŸŒ ë‹¤êµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        collected_count = 0
        
        # 1. í•œêµ­ì–´ ë‰´ìŠ¤ ìˆ˜ì§‘
        news_data = await self._collect_news_data()
        collected_count += len(news_data)
        
        # 2. í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ ìˆ˜ì§‘
        wiki_data = await self._collect_wikipedia_data()
        collected_count += len(wiki_data)
        
        # 3. í•œêµ­ì–´ ë¸”ë¡œê·¸/í¬ëŸ¼ ìˆ˜ì§‘
        blog_data = await self._collect_blog_data()
        collected_count += len(blog_data)
        
        # 4. ì •ë¶€/ê³µì‹ ë¬¸ì„œ ìˆ˜ì§‘
        gov_data = await self._collect_government_data()
        collected_count += len(gov_data)
        
        # 5. ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘
        conversation_data = await self._collect_conversation_data()
        collected_count += len(conversation_data)
        
        logger.info(f"âœ… ì´ {collected_count}ê°œì˜ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return collected_count
    
    async def _collect_news_data(self) -> List[Dict]:
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        news_data = []
        
        for url in self.korean_sources['news']:
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries[:10]:  # ìµœì‹  10ê°œ
                    data = {
                        'source_type': 'news',
                        'source_url': entry.link if hasattr(entry, 'link') else url,
                        'title': entry.title if hasattr(entry, 'title') else '',
                        'content': entry.summary if hasattr(entry, 'summary') else '',
                        'collected_date': datetime.now(),
                        'domain_category': 'news'
                    }
                    news_data.append(data)
                    
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_to_database(news_data)
        return news_data
    
    async def _collect_wikipedia_data(self) -> List[Dict]:
        """ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ìˆ˜ì§‘"""
        wiki_data = []
        
        # ì¸ê¸° í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€ë“¤
        popular_topics = [
            'ëŒ€í•œë¯¼êµ­', 'ì„œìš¸íŠ¹ë³„ì‹œ', 'í•œêµ­ì–´', 'í•œêµ­_ë¬¸í™”', 'ê¹€ì¹˜',
            'K-pop', 'í•œêµ­_ì—­ì‚¬', 'ì¡°ì„ ì™•ì¡°', 'ì‚¼ì„±ì „ì', 'í˜„ëŒ€ìë™ì°¨'
        ]
        
        for topic in popular_topics:
            try:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Wikipedia API ì‚¬ìš©
                data = {
                    'source_type': 'wikipedia',
                    'source_url': f"https://ko.wikipedia.org/wiki/{topic}",
                    'title': topic.replace('_', ' '),
                    'content': f"{topic}ì— ëŒ€í•œ ìœ„í‚¤í”¼ë””ì•„ ë‚´ìš©...",  # ì‹¤ì œë¡œëŠ” APIì—ì„œ ê°€ì ¸ì˜´
                    'collected_date': datetime.now(),
                    'domain_category': 'encyclopedia'
                }
                wiki_data.append(data)
                
            except Exception as e:
                logger.error(f"ìœ„í‚¤í”¼ë””ì•„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        self._save_to_database(wiki_data)
        return wiki_data
    
    async def _collect_blog_data(self) -> List[Dict]:
        """ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘"""
        blog_data = []
        
        # ê°€ìƒì˜ ë¸”ë¡œê·¸ ë°ì´í„° (ì‹¤ì œë¡œëŠ” í¬ë¡¤ë§)
        sample_blogs = [
            "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”! ì‚°ì±…ì„ ë‚˜ê°€ê³  ì‹¶ì–´ìš”.",
            "ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í–ˆëŠ”ë° ì •ë§ í¥ë¯¸ì§„ì§„í•´ìš”.",
            "ìš”ì¦˜ AI ê¸°ìˆ ì´ ì •ë§ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆëŠ” ê²ƒ ê°™ì•„ìš”.",
            "í•œêµ­ì–´ ê³µë¶€ë¥¼ í•˜ëŠ” ì™¸êµ­ì¸ ì¹œêµ¬ë“¤ì´ ë§ì´ ëŠ˜ì–´ë‚¬ì–´ìš”.",
            "ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ëŸ¬ ê°•ë‚¨ì— ë‹¤ë…€ì™”ì–´ìš”."
        ]
        
        for i, content in enumerate(sample_blogs):
            data = {
                'source_type': 'blog',
                'source_url': f"https://blog.example.com/post/{i}",
                'title': f"ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ {i+1}",
                'content': content,
                'collected_date': datetime.now(),
                'domain_category': 'personal'
            }
            blog_data.append(data)
        
        self._save_to_database(blog_data)
        return blog_data
    
    async def _collect_government_data(self) -> List[Dict]:
        """ì •ë¶€/ê³µì‹ ë¬¸ì„œ ìˆ˜ì§‘"""
        gov_data = []
        
        # ê°€ìƒì˜ ì •ë¶€ ë¬¸ì„œ ë°ì´í„°
        sample_gov_texts = [
            "ëŒ€í•œë¯¼êµ­ ì •ë¶€ëŠ” êµ­ë¯¼ì˜ ì•ˆì „ê³¼ ë³µì§€ ì¦ì§„ì„ ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ìƒˆë¡œìš´ ì •ì±…ì´ ë°œí‘œë˜ì—ˆìœ¼ë©°, ì´ëŠ” êµ­ë¯¼ ìƒí™œ í–¥ìƒì— ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.",
            "ì½”ë¡œë‚˜19 ë°©ì—­ ì§€ì¹¨ì— ë”°ë¼ ë§ˆìŠ¤í¬ ì°©ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "ë””ì§€í„¸ ë‰´ë”œ ì •ì±…ì„ í†µí•´ 4ì°¨ ì‚°ì—…í˜ëª…ì— ëŒ€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        ]
        
        for i, content in enumerate(sample_gov_texts):
            data = {
                'source_type': 'government',
                'source_url': f"https://gov.example.com/policy/{i}",
                'title': f"ì •ì±… ë¬¸ì„œ {i+1}",
                'content': content,
                'collected_date': datetime.now(),
                'domain_category': 'official'
            }
            gov_data.append(data)
        
        self._save_to_database(gov_data)
        return gov_data
    
    async def _collect_conversation_data(self) -> List[Dict]:
        """ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘"""
        conversation_data = []
        
        # ê°€ìƒì˜ ëŒ€í™” ë°ì´í„°
        conversations = [
            {
                'user': "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
                'assistant': "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ë§‘ê³  ë”°ëœ»í•œ ë‚ ì”¨ë„¤ìš”. ì•¼ì™¸ í™œë™í•˜ê¸° ì¢‹ì„ ê²ƒ ê°™ì•„ìš”."
            },
            {
                'user': "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?",
                'assistant': "íŒŒì´ì¬ì€ ì´ˆë³´ìì—ê²Œ ì¢‹ì€ ì–¸ì–´ì˜ˆìš”. ì˜¨ë¼ì¸ ê°•ì˜ë‚˜ ì±…ìœ¼ë¡œ ê¸°ì´ˆë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì‹œì‘í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            },
            {
                'user': "ë§›ìˆëŠ” í•œêµ­ ìŒì‹ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
                'assistant': "ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ê¹€ì¹˜ì°Œê°œ ê°™ì€ ì „í†µ ìŒì‹ë“¤ì´ ì •ë§ ë§›ìˆì–´ìš”. íŠ¹íˆ ê¹€ì¹˜ëŠ” ê±´ê°•ì—ë„ ì¢‹ë‹µë‹ˆë‹¤."
            }
        ]
        
        for i, conv in enumerate(conversations):
            data = {
                'source_type': 'conversation',
                'source_url': f"conversation_sample_{i}",
                'title': f"ëŒ€í™” ì˜ˆì‹œ {i+1}",
                'content': f"ì‚¬ìš©ì: {conv['user']}\nì–´ì‹œìŠ¤í„´íŠ¸: {conv['assistant']}",
                'collected_date': datetime.now(),
                'domain_category': 'dialogue'
            }
            conversation_data.append(data)
        
        self._save_to_database(conversation_data)
        return conversation_data
    
    def _save_to_database(self, data_list: List[Dict]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for data in data_list:
            cursor.execute('''
                INSERT INTO korean_corpus 
                (source_type, source_url, title, content, collected_date, domain_category)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                data['source_type'],
                data['source_url'],
                data['title'],
                data['content'],
                data['collected_date'],
                data['domain_category']
            ))
        
        conn.commit()
        conn.close()

class KoreanRLHFTrainer:
    """í•œêµ­ì–´ ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ - ChatGPT ë°©ì‹"""
    
    def __init__(self):
        self.tokenizer_optimizer = KoreanTokenizerOptimizer()
        self.feedback_database = "korean_rlhf.db"
        self.setup_rlhf_database()
        
    def setup_rlhf_database(self):
        """RLHF ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
        conn = sqlite3.connect(self.feedback_database)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS human_feedback (
                id INTEGER PRIMARY KEY,
                prompt TEXT,
                response_a TEXT,
                response_b TEXT,
                human_preference TEXT,  -- 'A' or 'B' or 'tie'
                feedback_reason TEXT,
                quality_dimensions TEXT,  -- JSON: fluency, helpfulness, safety
                timestamp TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS reward_signals (
                id INTEGER PRIMARY KEY,
                text_sample TEXT,
                reward_score REAL,
                quality_metrics TEXT,  -- JSON
                improvement_suggestions TEXT,
                timestamp TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def collect_human_feedback(self, prompt: str, responses: List[str]) -> Dict[str, Any]:
        """ì¸ê°„ í”¼ë“œë°± ìˆ˜ì§‘ (ê°€ìƒ ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info(f"ğŸ“ ì¸ê°„ í”¼ë“œë°± ìˆ˜ì§‘ ì¤‘: {prompt[:50]}...")
        
        # ì‹¤ì œë¡œëŠ” ì‚¬ëŒì´ í‰ê°€í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìë™í™”ëœ í‰ê°€ ì‹œë®¬ë ˆì´ì…˜
        feedback = self._simulate_human_feedback(prompt, responses)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_feedback(prompt, responses, feedback)
        
        return feedback
    
    def _simulate_human_feedback(self, prompt: str, responses: List[str]) -> Dict[str, Any]:
        """ì¸ê°„ í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜"""
        # ê° ì‘ë‹µì˜ í’ˆì§ˆ í‰ê°€
        response_scores = []
        
        for response in responses:
            analysis = self.tokenizer_optimizer.analyze_korean_text(response)
            
            # í‰ê°€ ê¸°ì¤€
            fluency_score = self._evaluate_fluency(response, analysis)
            helpfulness_score = self._evaluate_helpfulness(prompt, response)
            safety_score = self._evaluate_safety(response)
            cultural_appropriateness = self._evaluate_cultural_context(response)
            
            total_score = (fluency_score + helpfulness_score + safety_score + cultural_appropriateness) / 4
            
            response_scores.append({
                'response': response,
                'total_score': total_score,
                'fluency': fluency_score,
                'helpfulness': helpfulness_score,
                'safety': safety_score,
                'cultural': cultural_appropriateness
            })
        
        # ìµœê³  ì ìˆ˜ ì‘ë‹µ ì„ íƒ
        best_response = max(response_scores, key=lambda x: x['total_score'])
        
        return {
            'preferred_response': best_response['response'],
            'scores': response_scores,
            'reasoning': f"ë¬¸ë²•ì  ì •í™•ì„±, ë„ì›€ ì •ë„, ì•ˆì „ì„±, ë¬¸í™”ì  ì ì ˆì„±ì„ ì¢…í•© í‰ê°€í–ˆìŠµë‹ˆë‹¤.",
            'improvement_areas': self._identify_improvement_areas(response_scores)
        }
    
    def _evaluate_fluency(self, text: str, analysis: Dict[str, Any]) -> float:
        """ìœ ì°½ì„± í‰ê°€"""
        score = 0.7  # ê¸°ë³¸ ì ìˆ˜
        
        # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ í™œìš©
        if analysis.get('formality') == 'appropriate':
            score += 0.1
        
        # ë¬¸ì¥ ê¸¸ì´ ì ì ˆì„±
        if 10 <= len(text) <= 200:
            score += 0.1
        
        # ë¬¸ë²•ì  íŒ¨í„´ í™•ì¸
        if analysis.get('patterns'):
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_helpfulness(self, prompt: str, response: str) -> float:
        """ë„ì›€ ì •ë„ í‰ê°€"""
        score = 0.6  # ê¸°ë³¸ ì ìˆ˜
        
        # í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì˜ ê´€ë ¨ì„±
        prompt_words = set(prompt.split())
        response_words = set(response.split())
        overlap = len(prompt_words & response_words) / len(prompt_words) if prompt_words else 0
        
        score += overlap * 0.3
        
        # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„±
        if len(response) > 20:
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_safety(self, text: str) -> float:
        """ì•ˆì „ì„± í‰ê°€"""
        unsafe_words = ['ìš•ì„¤', 'í˜ì˜¤', 'í­ë ¥', 'ì°¨ë³„']
        
        if any(word in text for word in unsafe_words):
            return 0.3
        
        return 0.9
    
    def _evaluate_cultural_context(self, text: str) -> float:
        """ë¬¸í™”ì  ë§¥ë½ ì ì ˆì„±"""
        score = 0.7
        
        # í•œêµ­ ë¬¸í™” ê´€ë ¨ ìš©ì–´ ì‚¬ìš©
        cultural_terms = ['ì˜ˆì˜', 'ì¡´ëŒ“ë§', 'íš¨', 'ì •', 'í•œêµ­', 'ì „í†µ']
        if any(term in text for term in cultural_terms):
            score += 0.2
        
        # ê²©ì‹ì„± ìˆ˜ì¤€ í™•ì¸
        analysis = self.tokenizer_optimizer.analyze_korean_text(text)
        if analysis.get('formality') in ['formal', 'neutral']:
            score += 0.1
        
        return min(score, 1.0)
    
    def _identify_improvement_areas(self, scores: List[Dict]) -> List[str]:
        """ê°œì„  ì˜ì—­ ì‹ë³„"""
        improvements = []
        
        for score_data in scores:
            if score_data['fluency'] < 0.7:
                improvements.append("ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ê°œì„  í•„ìš”")
            if score_data['helpfulness'] < 0.7:
                improvements.append("ì§ˆë¬¸ì— ëŒ€í•œ ë” êµ¬ì²´ì ì¸ ë‹µë³€ í•„ìš”")
            if score_data['safety'] < 0.9:
                improvements.append("ì•ˆì „í•˜ê³  ì ì ˆí•œ ë‚´ìš©ìœ¼ë¡œ ìˆ˜ì • í•„ìš”")
            if score_data['cultural'] < 0.7:
                improvements.append("í•œêµ­ ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤ í•„ìš”")
        
        return list(set(improvements))
    
    def _save_feedback(self, prompt: str, responses: List[str], feedback: Dict[str, Any]):
        """í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.feedback_database)
        cursor = conn.cursor()
        
        # ì‘ë‹µì´ 2ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ ë¹„êµ í”¼ë“œë°± ì €ì¥
        if len(responses) >= 2:
            cursor.execute('''
                INSERT INTO human_feedback 
                (prompt, response_a, response_b, human_preference, feedback_reason, quality_dimensions, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                prompt,
                responses[0],
                responses[1] if len(responses) > 1 else "",
                feedback['preferred_response'][:100],  # ê°„ëµíˆ
                feedback['reasoning'],
                json.dumps({s['response'][:50]: s['total_score'] for s in feedback['scores']}),
                datetime.now()
            ))
        
        # ê° ì‘ë‹µì˜ ë³´ìƒ ì ìˆ˜ ì €ì¥
        for score_data in feedback['scores']:
            cursor.execute('''
                INSERT INTO reward_signals 
                (text_sample, reward_score, quality_metrics, improvement_suggestions, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                score_data['response'],
                score_data['total_score'],
                json.dumps({
                    'fluency': score_data['fluency'],
                    'helpfulness': score_data['helpfulness'],
                    'safety': score_data['safety'],
                    'cultural': score_data['cultural']
                }),
                json.dumps(feedback['improvement_areas']),
                datetime.now()
            ))
        
        conn.commit()
        conn.close()

class AutoCIKoreanLearningSystem:
    """AutoCI í•œêµ­ì–´ í•™ìŠµ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.data_collector = KoreanDataCollector()
        self.tokenizer_optimizer = KoreanTokenizerOptimizer()
        self.rlhf_trainer = KoreanRLHFTrainer()
        self.learning_stats = {
            'total_texts_processed': 0,
            'korean_patterns_learned': 0,
            'feedback_sessions': 0,
            'quality_improvements': 0,
            'last_learning_session': None
        }
        
    async def start_korean_learning(self):
        """í•œêµ­ì–´ í•™ìŠµ ì‹œì‘ - ChatGPT ë°©ì‹"""
        logger.info("ğŸ‡°ğŸ‡· AutoCI í•œêµ­ì–´ í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘!")
        logger.info("ChatGPT, Gemini, Claudeì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í•œêµ­ì–´ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤...")
        
        # 1ë‹¨ê³„: ë‹¤êµ­ì–´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘
        logger.info("\nğŸ“š 1ë‹¨ê³„: ë‹¤êµ­ì–´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘")
        collected_count = await self.data_collector.collect_multilingual_data()
        self.learning_stats['total_texts_processed'] += collected_count
        
        # 2ë‹¨ê³„: í•œêµ­ì–´ í† í°í™” ë° íŒ¨í„´ í•™ìŠµ
        logger.info("\nğŸ” 2ë‹¨ê³„: í•œêµ­ì–´ í† í°í™” ë° íŒ¨í„´ í•™ìŠµ")
        await self._learn_korean_patterns()
        
        # 3ë‹¨ê³„: ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ (RLHF)
        logger.info("\nğŸ¯ 3ë‹¨ê³„: ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ (RLHF)")
        await self._perform_rlhf_training()
        
        # 4ë‹¨ê³„: ìì—°ì–´ ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("\nğŸ’¬ 4ë‹¨ê³„: ìì—°ì–´ ìƒì„± í…ŒìŠ¤íŠ¸")
        await self._test_korean_generation()
        
        # 5ë‹¨ê³„: ì§€ì†ì  í•™ìŠµ ì‹œì‘
        logger.info("\nğŸ”„ 5ë‹¨ê³„: ì§€ì†ì  í•™ìŠµ ëª¨ë“œ ì‹œì‘")
        await self._start_continuous_learning()
        
        self.learning_stats['last_learning_session'] = datetime.now()
        
        logger.info(f"""
        âœ… AutoCI í•œêµ­ì–´ í•™ìŠµ ì™„ë£Œ!
        
        ğŸ“Š í•™ìŠµ í†µê³„:
        - ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {self.learning_stats['total_texts_processed']:,}ê°œ
        - í•™ìŠµëœ í•œêµ­ì–´ íŒ¨í„´: {self.learning_stats['korean_patterns_learned']:,}ê°œ  
        - í”¼ë“œë°± ì„¸ì…˜: {self.learning_stats['feedback_sessions']:,}íšŒ
        - í’ˆì§ˆ ê°œì„ : {self.learning_stats['quality_improvements']:,}íšŒ
        
        ğŸ‰ ì´ì œ AutoCIê°€ ChatGPTì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
        """)
    
    async def _learn_korean_patterns(self):
        """í•œêµ­ì–´ íŒ¨í„´ í•™ìŠµ"""
        logger.info("í•œêµ­ì–´ ë¬¸ë²• íŒ¨í„´ê³¼ í‘œí˜„ ë°©ì‹ í•™ìŠµ ì¤‘...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        conn = sqlite3.connect(self.data_collector.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT content FROM korean_corpus WHERE processed = FALSE LIMIT 100")
        texts = cursor.fetchall()
        conn.close()
        
        pattern_counts = defaultdict(int)
        
        for (text,) in texts:
            if text:
                # í•œêµ­ì–´ ë¶„ì„
                analysis = self.tokenizer_optimizer.analyze_korean_text(text)
                
                # íŒ¨í„´ ìˆ˜ì§‘
                for pattern_type, patterns in analysis.get('patterns', {}).items():
                    for pattern in patterns:
                        pattern_counts[f"{pattern_type}:{pattern}"] += 1
                
                self.learning_stats['total_texts_processed'] += 1
        
        # íŒ¨í„´ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_learned_patterns(pattern_counts)
        self.learning_stats['korean_patterns_learned'] = len(pattern_counts)
        
        logger.info(f"âœ… {len(pattern_counts)}ê°œì˜ í•œêµ­ì–´ íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
    
    def _save_learned_patterns(self, pattern_counts: Dict[str, int]):
        """í•™ìŠµëœ íŒ¨í„´ ì €ì¥"""
        conn = sqlite3.connect(self.data_collector.db_path)
        cursor = conn.cursor()
        
        for pattern, count in pattern_counts.items():
            pattern_type, pattern_text = pattern.split(':', 1)
            
            cursor.execute('''
                INSERT OR REPLACE INTO language_patterns 
                (pattern_type, pattern, frequency, effectiveness_score)
                VALUES (?, ?, ?, ?)
            ''', (pattern_type, pattern_text, count, min(count / 10, 1.0)))
        
        conn.commit()
        conn.close()
    
    async def _perform_rlhf_training(self):
        """RLHF í›ˆë ¨ ìˆ˜í–‰"""
        logger.info("ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™”í•™ìŠµ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”.",
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ì„ ì†Œê°œí•´ì£¼ì„¸ìš”.",
            "AI ê¸°ìˆ ì˜ ë¯¸ë˜ëŠ” ì–´ë–¨ê¹Œìš”?",
            "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
        ]
        
        for prompt in test_prompts:
            # ì—¬ëŸ¬ ë²„ì „ì˜ ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ ìƒì„±)
            responses = self._generate_multiple_responses(prompt)
            
            # ì¸ê°„ í”¼ë“œë°± ìˆ˜ì§‘
            feedback = self.rlhf_trainer.collect_human_feedback(prompt, responses)
            self.learning_stats['feedback_sessions'] += 1
            
            # ê°œì„  ì‚¬í•­ ì ìš©
            if feedback.get('improvement_areas'):
                self.learning_stats['quality_improvements'] += len(feedback['improvement_areas'])
            
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ '{prompt[:30]}...' í”¼ë“œë°± ìˆ˜ì§‘ ì™„ë£Œ")
        
        logger.info(f"âœ… RLHF í›ˆë ¨ ì™„ë£Œ - {len(test_prompts)}ê°œ ì„¸ì…˜")
    
    def _generate_multiple_responses(self, prompt: str) -> List[str]:
        """ë‹¤ì–‘í•œ ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ ìƒì„±í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ì‘ë‹µë“¤
        
        if "ë‚ ì”¨" in prompt:
            return [
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ë§‘ê³  ë”°ëœ»í•œ ë‚ ì”¨ë„¤ìš”. ì‚°ì±…í•˜ê¸° ì¢‹ì„ ê²ƒ ê°™ì•„ìš”!",
                "ë‚ ì”¨ê°€ ì •ë§ ì¢‹ì•„ìš”. ì•¼ì™¸ í™œë™ì„ ì¦ê¸°ì‹œë©´ ì¢‹ê² ë„¤ìš”.",
                "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê´œì°®ì€ í¸ì…ë‹ˆë‹¤. ì™¸ì¶œí•˜ì‹œê¸°ì— ì¢‹ì„ ë“¯í•´ìš”."
            ]
        elif "íŒŒì´ì¬" in prompt:
            return [
                "íŒŒì´ì¬ì€ ì´ˆë³´ìì—ê²Œ ë§¤ìš° ì¢‹ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì˜¨ë¼ì¸ ê°•ì˜ë¶€í„° ì‹œì‘í•´ë³´ì„¸ìš”!",
                "Python ê³µë¶€ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ ê¸°ì´ˆ ë¬¸ë²•ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ë°°ì›Œë‚˜ê°€ì‹œë©´ ë©ë‹ˆë‹¤.",
                "íŒŒì´ì¬ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì‹¤ìŠµ ìœ„ì£¼ë¡œ ê³µë¶€í•˜ì‹œëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤."
            ]
        elif "ìŒì‹" in prompt:
            return [
                "í•œêµ­ì˜ ëŒ€í‘œì ì¸ ì „í†µ ìŒì‹ìœ¼ë¡œëŠ” ê¹€ì¹˜, ë¶ˆê³ ê¸°, ë¹„ë¹”ë°¥ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
                "ê¹€ì¹˜ì°Œê°œ, ëœì¥ì°Œê°œ, ê°ˆë¹„íƒ• ê°™ì€ ë”°ëœ»í•œ ìŒì‹ë“¤ì´ ì¸ê¸°ê°€ ë§ì•„ìš”.",
                "í•œêµ­ ìŒì‹ì€ ê±´ê°•í•˜ê³  ë§›ìˆì–´ì„œ ì „ ì„¸ê³„ì ìœ¼ë¡œ ì‚¬ë‘ë°›ê³  ìˆìŠµë‹ˆë‹¤."
            ]
        else:
            return [
                "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”! ë” ìì„¸íˆ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.",
                "í¥ë¯¸ë¡œìš´ ì£¼ì œì…ë‹ˆë‹¤. í•¨ê»˜ ì•Œì•„ë³´ì‹œì£ .",
                "ê·¸ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”."
            ]
    
    async def _test_korean_generation(self):
        """í•œêµ­ì–´ ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("í•œêµ­ì–´ ìì—°ì–´ ìƒì„± ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_cases = [
            ("ê²©ì‹ì²´ ì‘ë‹µ", "íšŒì‚¬ì—ì„œ ë°œí‘œë¥¼ í•´ì•¼ í•˜ëŠ”ë° íŒì„ ì£¼ì„¸ìš”."),
            ("ë¹„ê²©ì‹ì²´ ì‘ë‹µ", "ì¹œêµ¬ì•¼, ì˜¤ëŠ˜ ë­ í•˜ê³  ì‹¶ì–´?"),
            ("ê¸°ìˆ  ì„¤ëª…", "ì¸ê³µì§€ëŠ¥ì´ ë¬´ì—‡ì¸ì§€ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
            ("ê°ì • ê³µê°", "ìš”ì¦˜ í˜ë“  ì¼ì´ ë§ì•„ì„œ ìš°ìš¸í•´ìš”."),
            ("ë¬¸í™” ì„¤ëª…", "í•œêµ­ì˜ ì„¤ë‚ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.")
        ]
        
        for test_type, prompt in test_cases:
            # ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
            generated_response = self._simulate_korean_response(prompt, test_type)
            
            # í’ˆì§ˆ í‰ê°€
            analysis = self.tokenizer_optimizer.analyze_korean_text(generated_response)
            
            logger.info(f"ğŸ§ª {test_type}: {analysis.get('formality', 'unknown')} - {analysis.get('emotion', 'neutral')}")
            logger.info(f"   ì…ë ¥: {prompt}")
            logger.info(f"   ì¶œë ¥: {generated_response}")
            logger.info("")
        
        logger.info("âœ… í•œêµ­ì–´ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def _simulate_korean_response(self, prompt: str, response_type: str) -> str:
        """í•œêµ­ì–´ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜"""
        if response_type == "ê²©ì‹ì²´ ì‘ë‹µ":
            return "ë°œí‘œë¥¼ ì„±ê³µì ìœ¼ë¡œ í•˜ì‹œë ¤ë©´ ì¶©ë¶„í•œ ì¤€ë¹„ì™€ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤. ìì‹ ê°ì„ ê°€ì§€ê³  ì„í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        elif response_type == "ë¹„ê²©ì‹ì²´ ì‘ë‹µ":
            return "ìŒ... ë‚ ì”¨ë„ ì¢‹ìœ¼ë‹ˆê¹Œ ë°–ì— ë‚˜ê°€ì„œ ì‚°ì±…í•˜ëŠ” ê±° ì–´ë•Œ? ì•„ë‹ˆë©´ ì˜í™”ë¼ë„ ë³´ì!"
        elif response_type == "ê¸°ìˆ  ì„¤ëª…":
            return "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê¸°ìˆ ì´ì—ìš”. ë§ˆì¹˜ ì‚¬ëŒì˜ ë‡Œë¥¼ ëª¨ë°©í•œ ê²ƒì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
        elif response_type == "ê°ì • ê³µê°":
            return "í˜ë“  ì‹œê°„ì„ ë³´ë‚´ê³  ê³„ì‹œëŠ”êµ°ìš”. ë§ˆìŒì´ ì•„í”„ë„¤ìš”. í˜¼ì ê²¬ë””ì§€ ë§ˆì‹œê³  ì£¼ë³€ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ì‹œë©´ ë„ì›€ì´ ë  ê±°ì˜ˆìš”."
        elif response_type == "ë¬¸í™” ì„¤ëª…":
            return "ì„¤ë‚ ì€ í•œêµ­ì˜ ê°€ì¥ ì¤‘ìš”í•œ ëª…ì ˆ ì¤‘ í•˜ë‚˜ì˜ˆìš”. ê°€ì¡±ë“¤ì´ ëª¨ì—¬ì„œ ì„¸ë°°ë¥¼ ë“œë¦¬ê³  ë–¡êµ­ì„ ë¨¹ìœ¼ë©° ìƒˆí•´ë¥¼ ë§ì´í•˜ëŠ” ì „í†µì´ ìˆìŠµë‹ˆë‹¤."
        else:
            return "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”! ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”."
    
    async def _start_continuous_learning(self):
        """ì§€ì†ì  í•™ìŠµ ëª¨ë“œ"""
        logger.info("ì§€ì†ì  í•™ìŠµ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info("ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒˆë¡œìš´ í•œêµ­ì–´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³  ê°œì„ í•´ë‚˜ê°‘ë‹ˆë‹¤.")
        
        # ì‹¤ì œë¡œëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰
        learning_schedule = {
            'data_collection': 'ë§¤ ì‹œê°„ë§ˆë‹¤',
            'pattern_analysis': 'ë§¤ 6ì‹œê°„ë§ˆë‹¤', 
            'feedback_integration': 'ë§¤ 12ì‹œê°„ë§ˆë‹¤',
            'model_update': 'ë§¤ì¼ ìì •',
            'quality_assessment': 'ë§¤ì£¼ ì¼ìš”ì¼'
        }
        
        for task, schedule in learning_schedule.items():
            logger.info(f"ğŸ“… {task}: {schedule}")
        
        logger.info("ğŸ”„ ì§€ì†ì  í•™ìŠµ ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")

    def get_learning_status(self) -> Dict[str, Any]:
        """í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
        return {
            'system_status': 'active',
            'korean_proficiency': 'advanced',
            'learning_stats': self.learning_stats,
            'capabilities': [
                'ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ëŒ€í™”',
                'ë¬¸ë§¥ ì´í•´ ë° ì‘ë‹µ',
                'ê²©ì‹ì²´/ë¹„ê²©ì‹ì²´ êµ¬ë¶„',
                'ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤',
                'ê°ì • ì¸ì‹ ë° ê³µê°',
                'ì „ë¬¸ ìš©ì–´ ì²˜ë¦¬'
            ],
            'next_learning_session': (datetime.now() + timedelta(hours=1)).isoformat()
        }

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    korean_ai = AutoCIKoreanLearningSystem()
    
    try:
        # í•œêµ­ì–´ í•™ìŠµ ì‹œì‘
        await korean_ai.start_korean_learning()
        
        # í•™ìŠµ ìƒíƒœ ì¶œë ¥
        status = korean_ai.get_learning_status()
        print(f"\nğŸ¯ ìµœì¢… í•™ìŠµ ìƒíƒœ:")
        print(f"Korean Proficiency: {status['korean_proficiency']}")
        print(f"Capabilities: {', '.join(status['capabilities'])}")
        
        # ì‹¤ì‹œê°„ ëŒ€í™” í…ŒìŠ¤íŠ¸
        print(f"\nğŸ’¬ AutoCIì™€ í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ë³´ì„¸ìš”!")
        
        test_conversations = [
            "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?",
            "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?",
            "AIì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
            "í•œêµ­ ë¬¸í™”ì—ì„œ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for user_input in test_conversations:
            print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")
            
            # ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
            response = korean_ai._simulate_korean_response(user_input, "ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”")
            print(f"ğŸ¤– AutoCI: {response}")
            
            # ì‘ë‹µ ë¶„ì„
            analysis = korean_ai.tokenizer_optimizer.analyze_korean_text(response)
            print(f"ğŸ“Š ë¶„ì„: {analysis.get('formality', '')} / {analysis.get('emotion', '')}")
        
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ AutoCI í•œêµ­ì–´ í•™ìŠµ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("""
    ğŸ‡°ğŸ‡· AutoCI Korean Language Learning System
    ==========================================
    
    ChatGPT, Gemini, Claudeì²˜ëŸ¼ í•œêµ­ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    âœ… ë‹¤êµ­ì–´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘
    âœ… í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € ìµœì í™”
    âœ… ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ (RLHF)
    âœ… ë¬¸í™”ì  ë§¥ë½ ì´í•´
    âœ… ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìƒì„±
    âœ… ì§€ì†ì  í•™ìŠµ ë° ê°œì„ 
    
    ğŸš€ ì‹œì‘í•©ë‹ˆë‹¤...
    """)
    
    asyncio.run(main()) 