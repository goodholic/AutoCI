from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import os

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
model = None
tokenizer = None

class GenerateRequest(BaseModel):
    prompt: str
    max_length: Optional[int] = 150
    temperature: Optional[float] = 0.7

class GenerateResponse(BaseModel):
    generated_text: str
    error: Optional[str] = None

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    global model, tokenizer
    
    if not TRANSFORMERS_AVAILABLE:
        print("transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install transformers torch")
        return False
    
    try:
        model_path = "CodeLlama-7b-Instruct-hf"
        
        if not os.path.exists(model_path):
            print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print("ë¨¼ì € ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:")
            print("python -c \"from huggingface_hub import snapshot_download; snapshot_download(repo_id='codellama/CodeLlama-7b-Instruct-hf', local_dir='./CodeLlama-7b-Instruct-hf')\"")
            return False
        
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ ì„¤ì •)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        )
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()

@app.get("/")
async def root():
    """API ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Code Llama AI Server",
        "endpoints": {
            "/generate": "POST - ì½”ë“œ ìƒì„±",
            "/status": "GET - ì„œë²„ ìƒíƒœ í™•ì¸",
            "/health": "GET - í—¬ìŠ¤ ì²´í¬"
        }
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate_code(request: GenerateRequest):
    """ì½”ë“œ ìƒì„± API"""
    global model, tokenizer
    
    if not TRANSFORMERS_AVAILABLE:
        raise HTTPException(status_code=500, detail="transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # C# ì½”ë“œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        formatted_prompt = f"### Instruction:\n{request.prompt}\n\n### Response:\n"
        
        # í† í°í™”
        inputs = tokenizer.encode(formatted_prompt, return_tensors="pt")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = inputs.cuda()
        
        # ì½”ë“œ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + request.max_length,
                temperature=request.temperature,
                do_sample=True,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì œê±°
        if generated_text.startswith(formatted_prompt):
            generated_text = generated_text[len(formatted_prompt):].strip()
        
        return GenerateResponse(generated_text=generated_text)
        
    except Exception as e:
        return GenerateResponse(
            generated_text="",
            error=f"ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@app.get("/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "transformers_available": TRANSFORMERS_AVAILABLE,
        "cuda_available": torch.cuda.is_available() if TRANSFORMERS_AVAILABLE else False,
        "device": "cuda" if TRANSFORMERS_AVAILABLE and torch.cuda.is_available() else "cpu"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    if model is not None and tokenizer is not None:
        return {"status": "healthy"}
    else:
        raise HTTPException(status_code=503, detail="Service unavailable")

# ê°œë°œ ì¤‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.post("/test")
async def test_generation():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„±"""
    test_prompt = "Create a simple Unity player controller script in C#"
    request = GenerateRequest(prompt=test_prompt)
    return await generate_code(request)

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ Code Llama AI Server ì‹œì‘...")
    print("ğŸ“ API ë¬¸ì„œ: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)