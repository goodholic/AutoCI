#!/usr/bin/env python3
"""
AutoCI í•™ìŠµ ì§„í–‰ë¥  ì¶”ì  ì‹œìŠ¤í…œ
ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©, ì„±ëŠ¥ ë³€í™”, ëª¨ë¸ ê°œì„  ì¶”ì 
"""

import os
import sys
import time
import json
import sqlite3
import threading
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
from dataclasses import dataclass, asdict
import pandas as pd
import numpy as np

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LearningMetric:
    """í•™ìŠµ ë©”íŠ¸ë¦­"""
    timestamp: str
    epoch: int
    loss: float
    accuracy: float
    learning_rate: float
    batch_size: int
    data_points: int
    training_time: float
    model_version: str
    performance_score: float

@dataclass
class ProgressSummary:
    """ì§„í–‰ë¥  ìš”ì•½"""
    total_epochs: int
    total_training_time: float
    total_data_points: int
    best_accuracy: float
    best_loss: float
    current_performance: float
    improvement_rate: float
    estimated_completion: str
    learning_efficiency: float

class LearningProgressDatabase:
    """í•™ìŠµ ì§„í–‰ë¥  ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self, db_path: str = "learning_progress.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # í•™ìŠµ ë©”íŠ¸ë¦­ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    epoch INTEGER,
                    loss REAL,
                    accuracy REAL,
                    learning_rate REAL,
                    batch_size INTEGER,
                    data_points INTEGER,
                    training_time REAL,
                    model_version TEXT,
                    performance_score REAL
                )
            ''')
            
            # ì§„í–‰ë¥  ìŠ¤ëƒ…ìƒ· í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS progress_snapshots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_epochs INTEGER,
                    total_training_time REAL,
                    total_data_points INTEGER,
                    best_accuracy REAL,
                    best_loss REAL,
                    current_performance REAL,
                    improvement_rate REAL,
                    learning_efficiency REAL
                )
            ''')
            
            # ì´ì •í‘œ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS milestones (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    milestone_type TEXT,
                    description TEXT,
                    metric_value REAL,
                    target_value REAL,
                    achieved BOOLEAN DEFAULT FALSE
                )
            ''')
            
            conn.commit()
    
    def record_metric(self, metric: LearningMetric):
        """í•™ìŠµ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO learning_metrics 
                (timestamp, epoch, loss, accuracy, learning_rate, batch_size, 
                 data_points, training_time, model_version, performance_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                metric.timestamp, metric.epoch, metric.loss, metric.accuracy,
                metric.learning_rate, metric.batch_size, metric.data_points,
                metric.training_time, metric.model_version, metric.performance_score
            ))
            conn.commit()
    
    def record_progress_snapshot(self, summary: ProgressSummary):
        """ì§„í–‰ë¥  ìŠ¤ëƒ…ìƒ· ê¸°ë¡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO progress_snapshots 
                (timestamp, total_epochs, total_training_time, total_data_points,
                 best_accuracy, best_loss, current_performance, improvement_rate, learning_efficiency)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(), summary.total_epochs, summary.total_training_time,
                summary.total_data_points, summary.best_accuracy, summary.best_loss,
                summary.current_performance, summary.improvement_rate, summary.learning_efficiency
            ))
            conn.commit()
    
    def get_recent_metrics(self, hours: int = 24) -> List[LearningMetric]:
        """ìµœê·¼ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT timestamp, epoch, loss, accuracy, learning_rate, batch_size,
                       data_points, training_time, model_version, performance_score
                FROM learning_metrics 
                WHERE timestamp >= ?
                ORDER BY timestamp DESC
            ''', (cutoff_time,))
            
            rows = cursor.fetchall()
            return [LearningMetric(*row) for row in rows]
    
    def get_all_metrics(self) -> List[LearningMetric]:
        """ëª¨ë“  ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT timestamp, epoch, loss, accuracy, learning_rate, batch_size,
                       data_points, training_time, model_version, performance_score
                FROM learning_metrics 
                ORDER BY timestamp ASC
            ''')
            
            rows = cursor.fetchall()
            return [LearningMetric(*row) for row in rows]
    
    def add_milestone(self, milestone_type: str, description: str, 
                     metric_value: float, target_value: float):
        """ì´ì •í‘œ ì¶”ê°€"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO milestones 
                (timestamp, milestone_type, description, metric_value, target_value, achieved)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(), milestone_type, description,
                metric_value, target_value, metric_value >= target_value
            ))
            conn.commit()

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self, db: LearningProgressDatabase):
        self.db = db
        
    def analyze_learning_trend(self, hours: int = 24) -> Dict:
        """í•™ìŠµ íŠ¸ë Œë“œ ë¶„ì„"""
        metrics = self.db.get_recent_metrics(hours)
        
        if len(metrics) < 2:
            return {"trend": "insufficient_data"}
        
        # ì‹œê°„ìˆœ ì •ë ¬
        metrics.sort(key=lambda x: x.timestamp)
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        accuracies = [m.accuracy for m in metrics]
        losses = [m.loss for m in metrics]
        
        accuracy_trend = self._calculate_trend(accuracies)
        loss_trend = self._calculate_trend(losses)
        
        # ê°œì„ ë¥  ê³„ì‚°
        if len(metrics) >= 2:
            recent_accuracy = np.mean(accuracies[-5:]) if len(accuracies) >= 5 else accuracies[-1]
            older_accuracy = np.mean(accuracies[:5]) if len(accuracies) >= 10 else accuracies[0]
            improvement_rate = (recent_accuracy - older_accuracy) / max(older_accuracy, 0.001)
        else:
            improvement_rate = 0.0
        
        return {
            "trend": "improving" if accuracy_trend > 0 and loss_trend < 0 else "degrading" if accuracy_trend < 0 or loss_trend > 0 else "stable",
            "accuracy_trend": accuracy_trend,
            "loss_trend": loss_trend,
            "improvement_rate": improvement_rate,
            "best_accuracy": max(accuracies),
            "best_loss": min(losses),
            "avg_accuracy": np.mean(accuracies),
            "avg_loss": np.mean(losses),
            "learning_speed": len(metrics) / hours  # metrics per hour
        }
    
    def _calculate_trend(self, values: List[float]) -> float:
        """ê°’ë“¤ì˜ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 2:
            return 0.0
        
        x = np.arange(len(values))
        z = np.polyfit(x, values, 1)
        return z[0]  # ê¸°ìš¸ê¸°
    
    def calculate_learning_efficiency(self, metrics: List[LearningMetric]) -> float:
        """í•™ìŠµ íš¨ìœ¨ì„± ê³„ì‚°"""
        if not metrics:
            return 0.0
        
        # ì‹œê°„ë‹¹ ì„±ëŠ¥ í–¥ìƒë¥ 
        total_time = sum(m.training_time for m in metrics)
        if total_time <= 0:
            return 0.0
        
        performance_gain = max(m.performance_score for m in metrics) - min(m.performance_score for m in metrics)
        efficiency = performance_gain / (total_time / 3600)  # per hour
        
        return min(efficiency, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def predict_performance(self, target_accuracy: float) -> Dict:
        """ì„±ëŠ¥ ì˜ˆì¸¡"""
        metrics = self.db.get_all_metrics()
        
        if len(metrics) < 10:
            return {"prediction": "insufficient_data"}
        
        # ìµœê·¼ íŠ¸ë Œë“œ ê¸°ë°˜ ì˜ˆì¸¡
        recent_metrics = metrics[-20:]  # ìµœê·¼ 20ê°œ
        accuracies = [m.accuracy for m in recent_metrics]
        timestamps = [datetime.fromisoformat(m.timestamp) for m in recent_metrics]
        
        # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        if len(accuracies) >= 5:
            x = np.arange(len(accuracies))
            z = np.polyfit(x, accuracies, 1)
            slope = z[0]
            
            if slope > 0:
                current_accuracy = accuracies[-1]
                remaining_improvement = target_accuracy - current_accuracy
                estimated_steps = remaining_improvement / slope
                
                # ì‹œê°„ ê°„ê²© ê³„ì‚°
                time_intervals = [(timestamps[i+1] - timestamps[i]).total_seconds() 
                                for i in range(len(timestamps)-1)]
                avg_interval = np.mean(time_intervals) if time_intervals else 3600
                
                estimated_time = estimated_steps * avg_interval
                estimated_completion = datetime.now() + timedelta(seconds=estimated_time)
                
                return {
                    "prediction": "achievable",
                    "estimated_completion": estimated_completion.isoformat(),
                    "estimated_days": estimated_time / 86400,
                    "current_accuracy": current_accuracy,
                    "target_accuracy": target_accuracy,
                    "improvement_rate": slope
                }
        
        return {"prediction": "uncertain"}

class VisualizationGenerator:
    """ì‹œê°í™” ìƒì„±ê¸°"""
    
    def __init__(self, db: LearningProgressDatabase):
        self.db = db
        self.output_dir = "progress_charts"
        os.makedirs(self.output_dir, exist_ok=True)
    
    def generate_learning_curve(self, save_path: str = None) -> str:
        """í•™ìŠµ ê³¡ì„  ìƒì„±"""
        metrics = self.db.get_all_metrics()
        
        if not metrics:
            return None
        
        # ë°ì´í„° ì¤€ë¹„
        timestamps = [datetime.fromisoformat(m.timestamp) for m in metrics]
        accuracies = [m.accuracy for m in metrics]
        losses = [m.loss for m in metrics]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # ì •í™•ë„ ê·¸ë˜í”„
        ax1.plot(timestamps, accuracies, 'b-', linewidth=2, label='Accuracy')
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_title('AutoCI Learning Progress', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # ì†ì‹¤ ê·¸ë˜í”„
        ax2.plot(timestamps, losses, 'r-', linewidth=2, label='Loss')
        ax2.set_ylabel('Loss', fontsize=12)
        ax2.set_xlabel('Time', fontsize=12)
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # ì‹œê°„ ì¶• í¬ë§·
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ì €ì¥
        if not save_path:
            save_path = os.path.join(self.output_dir, f"learning_curve_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def generate_performance_dashboard(self, save_path: str = None) -> str:
        """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        metrics = self.db.get_all_metrics()
        
        if not metrics:
            return None
        
        # ë°ì´í„° ì¤€ë¹„
        timestamps = [datetime.fromisoformat(m.timestamp) for m in metrics]
        accuracies = [m.accuracy for m in metrics]
        losses = [m.loss for m in metrics]
        learning_rates = [m.learning_rate for m in metrics]
        batch_sizes = [m.batch_size for m in metrics]
        
        # 4ê°œ ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. ì •í™•ë„/ì†ì‹¤
        ax1_twin = ax1.twinx()
        ax1.plot(timestamps, accuracies, 'b-', linewidth=2, label='Accuracy')
        ax1_twin.plot(timestamps, losses, 'r-', linewidth=2, label='Loss')
        ax1.set_ylabel('Accuracy', color='b', fontsize=10)
        ax1_twin.set_ylabel('Loss', color='r', fontsize=10)
        ax1.set_title('Accuracy & Loss Over Time', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # 2. í•™ìŠµë¥ 
        ax2.plot(timestamps, learning_rates, 'g-', linewidth=2)
        ax2.set_ylabel('Learning Rate', fontsize=10)
        ax2.set_title('Learning Rate Changes', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # 3. ë°°ì¹˜ í¬ê¸°
        ax3.plot(timestamps, batch_sizes, 'm-', linewidth=2)
        ax3.set_ylabel('Batch Size', fontsize=10)
        ax3.set_title('Batch Size Changes', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 4. ì„±ëŠ¥ ì ìˆ˜
        performance_scores = [m.performance_score for m in metrics]
        ax4.plot(timestamps, performance_scores, 'orange', linewidth=2)
        ax4.set_ylabel('Performance Score', fontsize=10)
        ax4.set_title('Overall Performance Score', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # ì‹œê°„ ì¶• í¬ë§·
        for ax in [ax1, ax2, ax3, ax4]:
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ì €ì¥
        if not save_path:
            save_path = os.path.join(self.output_dir, f"dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def generate_efficiency_chart(self, save_path: str = None) -> str:
        """íš¨ìœ¨ì„± ì°¨íŠ¸ ìƒì„±"""
        metrics = self.db.get_all_metrics()
        
        if len(metrics) < 10:
            return None
        
        # ì‹œê°„ë³„ íš¨ìœ¨ì„± ê³„ì‚°
        window_size = 10
        timestamps = []
        efficiencies = []
        
        for i in range(window_size, len(metrics)):
            window_metrics = metrics[i-window_size:i]
            analyzer = PerformanceAnalyzer(self.db)
            efficiency = analyzer.calculate_learning_efficiency(window_metrics)
            
            timestamps.append(datetime.fromisoformat(metrics[i].timestamp))
            efficiencies.append(efficiency)
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(12, 6))
        plt.plot(timestamps, efficiencies, 'purple', linewidth=2, marker='o', markersize=4)
        plt.ylabel('Learning Efficiency', fontsize=12)
        plt.xlabel('Time', fontsize=12)
        plt.title('Learning Efficiency Over Time', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # í‰ê· ì„  ì¶”ê°€
        avg_efficiency = np.mean(efficiencies)
        plt.axhline(y=avg_efficiency, color='red', linestyle='--', alpha=0.7, 
                   label=f'Average: {avg_efficiency:.3f}')
        plt.legend()
        
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ì €ì¥
        if not save_path:
            save_path = os.path.join(self.output_dir, f"efficiency_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path

class LearningProgressTracker:
    """í•™ìŠµ ì§„í–‰ë¥  ì¶”ì ê¸°"""
    
    def __init__(self, db_path: str = "learning_progress.db"):
        self.db = LearningProgressDatabase(db_path)
        self.analyzer = PerformanceAnalyzer(self.db)
        self.visualizer = VisualizationGenerator(self.db)
        
        # ì´ì •í‘œ ì„¤ì •
        self.milestones = [
            {"type": "accuracy", "description": "70% ì •í™•ë„ ë‹¬ì„±", "target": 0.7},
            {"type": "accuracy", "description": "80% ì •í™•ë„ ë‹¬ì„±", "target": 0.8},
            {"type": "accuracy", "description": "90% ì •í™•ë„ ë‹¬ì„±", "target": 0.9},
            {"type": "loss", "description": "0.3 ì´í•˜ ì†ì‹¤ ë‹¬ì„±", "target": 0.3},
            {"type": "loss", "description": "0.1 ì´í•˜ ì†ì‹¤ ë‹¬ì„±", "target": 0.1}
        ]
        
        logger.info("ğŸ“ˆ í•™ìŠµ ì§„í–‰ë¥  ì¶”ì ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def record_learning_step(self, epoch: int, loss: float, accuracy: float,
                           learning_rate: float, batch_size: int, data_points: int,
                           training_time: float, model_version: str = "default"):
        """í•™ìŠµ ë‹¨ê³„ ê¸°ë¡"""
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ì •í™•ë„ ê¸°ë°˜, ì†ì‹¤ íŒ¨ë„í‹°)
        performance_score = accuracy - (loss * 0.1)
        
        metric = LearningMetric(
            timestamp=datetime.now().isoformat(),
            epoch=epoch,
            loss=loss,
            accuracy=accuracy,
            learning_rate=learning_rate,
            batch_size=batch_size,
            data_points=data_points,
            training_time=training_time,
            model_version=model_version,
            performance_score=performance_score
        )
        
        self.db.record_metric(metric)
        
        # ì´ì •í‘œ í™•ì¸
        self._check_milestones(metric)
        
        logger.info(f"ğŸ“Š í•™ìŠµ ë‹¨ê³„ ê¸°ë¡: ì—í¬í¬={epoch}, ì†ì‹¤={loss:.4f}, ì •í™•ë„={accuracy:.4f}")
    
    def _check_milestones(self, metric: LearningMetric):
        """ì´ì •í‘œ í™•ì¸"""
        for milestone in self.milestones:
            if milestone["type"] == "accuracy":
                if metric.accuracy >= milestone["target"]:
                    self.db.add_milestone(
                        milestone["type"],
                        milestone["description"],
                        metric.accuracy,
                        milestone["target"]
                    )
                    logger.info(f"ğŸ¯ ì´ì •í‘œ ë‹¬ì„±: {milestone['description']}")
            
            elif milestone["type"] == "loss":
                if metric.loss <= milestone["target"]:
                    self.db.add_milestone(
                        milestone["type"],
                        milestone["description"],
                        metric.loss,
                        milestone["target"]
                    )
                    logger.info(f"ğŸ¯ ì´ì •í‘œ ë‹¬ì„±: {milestone['description']}")
    
    def get_current_progress(self) -> ProgressSummary:
        """í˜„ì¬ ì§„í–‰ë¥  ì¡°íšŒ"""
        all_metrics = self.db.get_all_metrics()
        
        if not all_metrics:
            return ProgressSummary(
                total_epochs=0,
                total_training_time=0.0,
                total_data_points=0,
                best_accuracy=0.0,
                best_loss=999.0,
                current_performance=0.0,
                improvement_rate=0.0,
                estimated_completion="unknown",
                learning_efficiency=0.0
            )
        
        # í†µê³„ ê³„ì‚°
        total_epochs = len(all_metrics)
        total_training_time = sum(m.training_time for m in all_metrics)
        total_data_points = sum(m.data_points for m in all_metrics)
        best_accuracy = max(m.accuracy for m in all_metrics)
        best_loss = min(m.loss for m in all_metrics)
        current_performance = all_metrics[-1].performance_score
        
        # ê°œì„ ë¥  ê³„ì‚°
        trend_analysis = self.analyzer.analyze_learning_trend(24)
        improvement_rate = trend_analysis.get("improvement_rate", 0.0)
        
        # í•™ìŠµ íš¨ìœ¨ì„±
        learning_efficiency = self.analyzer.calculate_learning_efficiency(all_metrics[-20:])
        
        # ì™„ë£Œ ì˜ˆìƒ ì‹œê°„
        prediction = self.analyzer.predict_performance(0.9)  # 90% ì •í™•ë„ ëª©í‘œ
        estimated_completion = prediction.get("estimated_completion", "unknown")
        
        summary = ProgressSummary(
            total_epochs=total_epochs,
            total_training_time=total_training_time / 3600,  # ì‹œê°„ ë‹¨ìœ„
            total_data_points=total_data_points,
            best_accuracy=best_accuracy,
            best_loss=best_loss,
            current_performance=current_performance,
            improvement_rate=improvement_rate,
            estimated_completion=estimated_completion,
            learning_efficiency=learning_efficiency
        )
        
        # ìŠ¤ëƒ…ìƒ· ì €ì¥
        self.db.record_progress_snapshot(summary)
        
        return summary
    
    def generate_progress_report(self) -> Dict:
        """ì§„í–‰ë¥  ë³´ê³ ì„œ ìƒì„±"""
        progress = self.get_current_progress()
        trend_analysis = self.analyzer.analyze_learning_trend(24)
        
        # ì‹œê°í™” ìƒì„±
        learning_curve_path = self.visualizer.generate_learning_curve()
        dashboard_path = self.visualizer.generate_performance_dashboard()
        efficiency_path = self.visualizer.generate_efficiency_chart()
        
        report = {
            "progress_summary": asdict(progress),
            "trend_analysis": trend_analysis,
            "visualizations": {
                "learning_curve": learning_curve_path,
                "dashboard": dashboard_path,
                "efficiency_chart": efficiency_path
            },
            "recommendations": self._generate_recommendations(progress, trend_analysis),
            "generated_at": datetime.now().isoformat()
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = f"progress_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ ì§„í–‰ë¥  ë³´ê³ ì„œ ìƒì„±: {report_file}")
        
        return report
    
    def _generate_recommendations(self, progress: ProgressSummary, trend: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if progress.current_performance < 0.6:
            recommendations.append("ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if progress.improvement_rate < 0.01:
            recommendations.append("ê°œì„ ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë°ì´í„° í’ˆì§ˆ ì ê²€ ë˜ëŠ” ëª¨ë¸ êµ¬ì¡° ë³€ê²½ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if progress.learning_efficiency < 0.3:
            recommendations.append("í•™ìŠµ íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë‚˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if trend.get("trend") == "degrading":
            recommendations.append("ì„±ëŠ¥ì´ í•˜ë½í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³¼ì í•© ê°€ëŠ¥ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        if trend.get("learning_speed", 0) < 1:
            recommendations.append("í•™ìŠµ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤. ë” ë¹ˆë²ˆí•œ í•™ìŠµ ìŠ¤ì¼€ì¤„ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ í•™ìŠµì´ ì˜ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê³„ì† ìœ ì§€í•˜ì„¸ìš”!")
        
        return recommendations
    
    def start_real_time_tracking(self, interval: int = 300):
        """ì‹¤ì‹œê°„ ì¶”ì  ì‹œì‘ (5ë¶„ë§ˆë‹¤)"""
        logger.info(f"ğŸ”„ ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì¶”ì  ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")
        
        def tracking_loop():
            while True:
                try:
                    progress = self.get_current_progress()
                    logger.info(f"ğŸ“ˆ í˜„ì¬ ì§„í–‰ë¥ : "
                              f"ì—í¬í¬={progress.total_epochs}, "
                              f"ìµœê³ ì •í™•ë„={progress.best_accuracy:.3f}, "
                              f"íš¨ìœ¨ì„±={progress.learning_efficiency:.3f}")
                    
                    time.sleep(interval)
                    
                except Exception as e:
                    logger.error(f"ì‹¤ì‹œê°„ ì¶”ì  ì˜¤ë¥˜: {e}")
                    time.sleep(60)
        
        tracking_thread = threading.Thread(target=tracking_loop, daemon=True)
        tracking_thread.start()

def test_progress_tracker():
    """ì§„í–‰ë¥  ì¶”ì ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í•™ìŠµ ì§„í–‰ë¥  ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì¶”ì ê¸° ì´ˆê¸°í™”
    tracker = LearningProgressTracker("test_progress.db")
    
    # ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ë°ì´í„°
    print("ğŸ“š ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
    
    import random
    
    for epoch in range(1, 21):  # 20 ì—í¬í¬
        # ì‹œë®¬ë ˆì´ì…˜: ì ì§„ì  ê°œì„ 
        base_accuracy = 0.5 + (epoch * 0.02) + random.uniform(-0.05, 0.05)
        base_loss = 1.0 - (epoch * 0.03) + random.uniform(-0.1, 0.1)
        
        base_accuracy = max(0.0, min(1.0, base_accuracy))
        base_loss = max(0.1, base_loss)
        
        lr = 0.001 * (0.95 ** (epoch // 5))  # í•™ìŠµë¥  ê°ì†Œ
        batch_size = 16 + (epoch // 10) * 8  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        
        tracker.record_learning_step(
            epoch=epoch,
            loss=base_loss,
            accuracy=base_accuracy,
            learning_rate=lr,
            batch_size=batch_size,
            data_points=random.randint(50, 200),
            training_time=random.uniform(30, 120),
            model_version="test_v1.0"
        )
        
        print(f"ì—í¬í¬ {epoch:2d}: ì •í™•ë„={base_accuracy:.3f}, ì†ì‹¤={base_loss:.3f}")
        
        time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
    
    # ì§„í–‰ë¥  ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“‹ ì§„í–‰ë¥  ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report = tracker.generate_progress_report()
    
    print(f"\nğŸ“Š ì§„í–‰ë¥  ìš”ì•½:")
    summary = report["progress_summary"]
    print(f"  ì´ ì—í¬í¬: {summary['total_epochs']}")
    print(f"  ì´ í•™ìŠµ ì‹œê°„: {summary['total_training_time']:.2f}ì‹œê°„")
    print(f"  ìµœê³  ì •í™•ë„: {summary['best_accuracy']:.3f}")
    print(f"  ìµœì € ì†ì‹¤: {summary['best_loss']:.3f}")
    print(f"  í•™ìŠµ íš¨ìœ¨ì„±: {summary['learning_efficiency']:.3f}")
    
    print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(report["recommendations"], 1):
        print(f"  {i}. {rec}")
    
    print(f"\nğŸ“ˆ ìƒì„±ëœ ì‹œê°í™”:")
    for name, path in report["visualizations"].items():
        if path:
            print(f"  {name}: {path}")
    
    print("\nğŸ‰ ì§„í–‰ë¥  ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_progress_tracker()