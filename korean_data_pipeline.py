#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Korean Data Pipeline for AutoCI
ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ë°©ì‹ì˜ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ChatGPT/Gemini/Claude ë°©ì‹:
1. ëŒ€ê·œëª¨ ì›¹ í¬ë¡¤ë§
2. ë‹¤ì–‘í•œ ë„ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘  
3. í’ˆì§ˆ í•„í„°ë§
4. í† í¬ë‚˜ì´ì§• ìµœì í™”
5. ë°°ì¹˜ ì²˜ë¦¬
"""

import os
import json
import asyncio
import aiohttp
import aiofiles
import sqlite3
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
import hashlib
import time
from urllib.parse import urljoin, urlparse
import feedparser
from konlpy.tag import Okt, Mecab, Hannanum
import kss  # Korean Sentence Splitter
from transformers import AutoTokenizer
import threading
from concurrent.futures import ThreadPoolExecutor
import queue

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('korean_data_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KoreanWebCrawler:
    """í•œêµ­ì–´ ì›¹ í¬ë¡¤ë§ - ChatGPT ë°©ì‹"""
    
    def __init__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì†ŒìŠ¤ë“¤
        self.korean_sources = {
            'news': {
                'naver_news': 'https://news.naver.com',
                'daum_news': 'https://news.daum.net',
                'ytn': 'https://www.ytn.co.kr',
                'kbs': 'https://news.kbs.co.kr',
                'mbc': 'https://imnews.imbc.com',
                'sbs': 'https://news.sbs.co.kr'
            },
            'blogs': {
                'naver_blog': 'https://blog.naver.com',
                'tistory': 'https://tistory.com',
                'brunch': 'https://brunch.co.kr'
            },
            'forums': {
                'clien': 'https://www.clien.net',
                'dcinside': 'https://www.dcinside.com',
                'bobaedream': 'https://www.bobaedream.co.kr',
                'mlbpark': 'https://mlbpark.donga.com'
            },
            'government': {
                'korea_kr': 'https://www.korea.kr',
                'mois': 'https://www.mois.go.kr',
                'moef': 'https://www.moef.go.kr',
                'me': 'https://www.moe.go.kr'
            },
            'education': {
                'kocw': 'http://www.kocw.net',
                'kmooc': 'http://www.kmooc.kr',
                'coursera_ko': 'https://www.coursera.org/browse?language=ko'
            },
            'literature': {
                'yes24': 'http://www.yes24.com',
                'kyobobook': 'http://www.kyobobook.co.kr',
                'aladin': 'https://www.aladin.co.kr'
            }
        }
        
        self.visited_urls = set()
        self.crawl_stats = {
            'total_pages': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'korean_content_found': 0,
            'duplicate_content': 0
        }
    
    async def crawl_korean_web(self, max_pages: int = 1000) -> List[Dict[str, Any]]:
        """í•œêµ­ì–´ ì›¹ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ğŸ•·ï¸ í•œêµ­ì–´ ì›¹ í¬ë¡¤ë§ ì‹œì‘ - ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        all_content = []
        semaphore = asyncio.Semaphore(10)  # ë™ì‹œ ìš”ì²­ ì œí•œ
        
        tasks = []
        page_count = 0
        
        for category, sources in self.korean_sources.items():
            for source_name, base_url in sources.items():
                if page_count >= max_pages:
                    break
                
                # ê° ì†ŒìŠ¤ë³„ë¡œ í¬ë¡¤ë§ íƒœìŠ¤í¬ ìƒì„±
                task = self._crawl_source(semaphore, category, source_name, base_url)
                tasks.append(task)
                page_count += 1
        
        # ëª¨ë“  í¬ë¡¤ë§ íƒœìŠ¤í¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for result in results:
            if isinstance(result, list):
                all_content.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {result}")
        
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(all_content)}ê°œ ì»¨í…ì¸  ìˆ˜ì§‘")
        logger.info(f"ğŸ“Š í†µê³„: {self.crawl_stats}")
        
        return all_content
    
    async def _crawl_source(self, semaphore: asyncio.Semaphore, category: str, source_name: str, base_url: str) -> List[Dict[str, Any]]:
        """ê°œë³„ ì†ŒìŠ¤ í¬ë¡¤ë§"""
        async with semaphore:
            try:
                logger.info(f"ğŸ” í¬ë¡¤ë§ ì¤‘: {category}/{source_name}")
                
                content_list = []
                
                # RSS í”¼ë“œ í™•ì¸
                rss_content = await self._try_rss_crawl(base_url)
                if rss_content:
                    content_list.extend(rss_content)
                
                # ì¼ë°˜ ì›¹ í˜ì´ì§€ í¬ë¡¤ë§
                web_content = await self._crawl_web_pages(base_url, category)
                if web_content:
                    content_list.extend(web_content)
                
                self.crawl_stats['successful_crawls'] += 1
                return content_list
                
            except Exception as e:
                logger.error(f"ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨ {source_name}: {e}")
                self.crawl_stats['failed_crawls'] += 1
                return []
    
    async def _try_rss_crawl(self, base_url: str) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œë„"""
        rss_paths = ['/rss', '/rss.xml', '/feed', '/feed.xml', '/atom.xml']
        content_list = []
        
        for rss_path in rss_paths:
            try:
                rss_url = urljoin(base_url, rss_path)
                
                async with self.session.get(rss_url, headers=self.headers) as response:
                    if response.status == 200:
                        rss_content = await response.text()
                        feed = feedparser.parse(rss_content)
                        
                        for entry in feed.entries[:10]:  # ìµœì‹  10ê°œ
                            if hasattr(entry, 'title') and hasattr(entry, 'summary'):
                                content = {
                                    'source_type': 'rss',
                                    'source_url': getattr(entry, 'link', rss_url),
                                    'title': entry.title,
                                    'content': entry.summary,
                                    'published_date': getattr(entry, 'published', ''),
                                    'collected_date': datetime.now().isoformat(),
                                    'language': 'korean',
                                    'domain': urlparse(base_url).netloc
                                }
                                
                                if self._is_korean_content(content['title'] + content['content']):
                                    content_list.append(content)
                                    self.crawl_stats['korean_content_found'] += 1
                        break
                        
            except Exception as e:
                continue  # ë‹¤ìŒ RSS ê²½ë¡œ ì‹œë„
        
        return content_list
    
    async def _crawl_web_pages(self, base_url: str, category: str) -> List[Dict[str, Any]]:
        """ì¼ë°˜ ì›¹ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            async with self.session.get(base_url, headers=self.headers) as response:
                if response.status != 200:
                    return []
                
                html_content = await response.text()
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ''
                
                # ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ íƒœê·¸ì—ì„œ)
                content_tags = soup.find_all(['p', 'div', 'article', 'section'], 
                                           class_=re.compile(r'content|article|post|text', re.I))
                
                content_text = ''
                for tag in content_tags:
                    text = tag.get_text().strip()
                    if len(text) > 50:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                        content_text += text + '\n'
                
                if not content_text:
                    # ëŒ€ì•ˆ: ëª¨ë“  p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    paragraphs = soup.find_all('p')
                    content_text = '\n'.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 20])
                
                # í•œêµ­ì–´ ì»¨í…ì¸  í™•ì¸
                full_text = title_text + content_text
                if self._is_korean_content(full_text) and len(content_text) > 100:
                    self.crawl_stats['korean_content_found'] += 1
                    self.crawl_stats['total_pages'] += 1
                    
                    return [{
                        'source_type': 'web',
                        'source_url': base_url,
                        'title': title_text,
                        'content': content_text,
                        'collected_date': datetime.now().isoformat(),
                        'language': 'korean',
                        'domain': urlparse(base_url).netloc,
                        'category': category,
                        'content_hash': hashlib.md5(content_text.encode()).hexdigest()
                    }]
                
                return []
                
        except Exception as e:
            logger.error(f"ì›¹ í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜ {base_url}: {e}")
            return []
    
    def _is_korean_content(self, text: str) -> bool:
        """í•œêµ­ì–´ ì»¨í…ì¸  ì—¬ë¶€ í™•ì¸"""
        if not text or len(text) < 10:
            return False
        
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.findall(r'[ê°€-í£a-zA-Z0-9]', text))
        
        if total_chars == 0:
            return False
        
        korean_ratio = korean_chars / total_chars
        return korean_ratio >= 0.3  # 30% ì´ìƒ í•œê¸€
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        await self.session.close()

class KoreanTextProcessor:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ëŒ€í˜• ëª¨ë¸ ë°©ì‹"""
    
    def __init__(self):
        self.okt = Okt()
        try:
            self.mecab = Mecab()
        except:
            self.mecab = None
            logger.warning("Mecab ì‚¬ìš© ë¶ˆê°€, Oktë§Œ ì‚¬ìš©")
        
        self.hannanum = Hannanum()
        
        # ì „ì²˜ë¦¬ ê·œì¹™
        self.cleaning_rules = {
            'remove_html': True,
            'remove_urls': True,
            'remove_emails': True,
            'remove_phone_numbers': True,
            'normalize_whitespace': True,
            'remove_special_chars': False,  # í•œêµ­ì–´ íŠ¹ì„±ìƒ ìœ ì§€
            'min_length': 10,
            'max_length': 10000
        }
        
        # í’ˆì§ˆ í•„í„° ê·œì¹™
        self.quality_filters = {
            'min_korean_ratio': 0.3,
            'max_repetition_ratio': 0.3,
            'min_sentence_count': 2,
            'forbidden_patterns': [
                r'(.)\1{10,}',  # ê°™ì€ ë¬¸ì 10ë²ˆ ì´ìƒ ë°˜ë³µ
                r'[^\w\sê°€-í£.,!?;:()]{5,}',  # íŠ¹ìˆ˜ë¬¸ì 5ê°œ ì´ìƒ ì—°ì†
            ]
        }
    
    def process_text_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬"""
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ - {len(texts)}ê°œ")
        
        processed_texts = []
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(self.process_single_text, text) for text in texts]
            
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=30)
                    if result:
                        processed_texts.append(result)
                        
                    if (i + 1) % 100 == 0:
                        logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(texts)}")
                        
                except Exception as e:
                    logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - {len(processed_texts)}ê°œ ì„±ê³µ")
        return processed_texts
    
    def process_single_text(self, text: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        try:
            # 1. ê¸°ë³¸ ì •ë¦¬
            cleaned_text = self._clean_text(text)
            
            # 2. í’ˆì§ˆ ê²€ì‚¬
            if not self._quality_check(cleaned_text):
                return None
            
            # 3. ë¬¸ì¥ ë¶„ë¦¬
            sentences = kss.split_sentences(cleaned_text)
            
            # 4. í˜•íƒœì†Œ ë¶„ì„
            morphological_analysis = self._analyze_morphology(cleaned_text)
            
            # 5. ì–¸ì–´í•™ì  íŠ¹ì„± ì¶”ì¶œ
            linguistic_features = self._extract_linguistic_features(cleaned_text, sentences)
            
            # 6. í† í°í™”
            tokens = self._tokenize_korean(cleaned_text)
            
            return {
                'original_text': text,
                'cleaned_text': cleaned_text,
                'sentences': sentences,
                'sentence_count': len(sentences),
                'morphology': morphological_analysis,
                'features': linguistic_features,
                'tokens': tokens,
                'token_count': len(tokens),
                'processing_date': datetime.now().isoformat(),
                'quality_score': self._calculate_quality_score(cleaned_text, sentences, morphological_analysis)
            }
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        cleaned = text
        
        if self.cleaning_rules['remove_html']:
            cleaned = re.sub(r'<[^>]+>', '', cleaned)
        
        if self.cleaning_rules['remove_urls']:
            cleaned = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', cleaned)
        
        if self.cleaning_rules['remove_emails']:
            cleaned = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', cleaned)
        
        if self.cleaning_rules['remove_phone_numbers']:
            cleaned = re.sub(r'(\d{2,3}[-.]?\d{3,4}[-.]?\d{4})', '', cleaned)
        
        if self.cleaning_rules['normalize_whitespace']:
            cleaned = re.sub(r'\s+', ' ', cleaned)
            cleaned = cleaned.strip()
        
        return cleaned
    
    def _quality_check(self, text: str) -> bool:
        """í’ˆì§ˆ ê²€ì‚¬"""
        if not text:
            return False
        
        # ê¸¸ì´ ê²€ì‚¬
        if len(text) < self.quality_filters['min_length']:
            return False
        if len(text) > self.quality_filters['max_length']:
            return False
        
        # í•œêµ­ì–´ ë¹„ìœ¨ ê²€ì‚¬
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.findall(r'[ê°€-í£a-zA-Z0-9]', text))
        
        if total_chars == 0:
            return False
        
        korean_ratio = korean_chars / total_chars
        if korean_ratio < self.quality_filters['min_korean_ratio']:
            return False
        
        # ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
        for pattern in self.quality_filters['forbidden_patterns']:
            if re.search(pattern, text):
                return False
        
        # ë¬¸ì¥ ìˆ˜ ê²€ì‚¬
        sentences = kss.split_sentences(text)
        if len(sentences) < self.quality_filters['min_sentence_count']:
            return False
        
        return True
    
    def _analyze_morphology(self, text: str) -> Dict[str, Any]:
        """í˜•íƒœì†Œ ë¶„ì„"""
        try:
            # OKT ë¶„ì„
            okt_morphs = self.okt.morphs(text)
            okt_pos = self.okt.pos(text)
            okt_nouns = self.okt.nouns(text)
            
            analysis = {
                'morphemes': okt_morphs[:100],  # ì²˜ìŒ 100ê°œë§Œ ì €ì¥
                'pos_tags': okt_pos[:100],
                'nouns': okt_nouns[:50],
                'total_morphemes': len(okt_morphs),
                'unique_morphemes': len(set(okt_morphs)),
                'vocabulary_richness': len(set(okt_morphs)) / len(okt_morphs) if okt_morphs else 0
            }
            
            # Mecab ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì¶”ê°€ ë¶„ì„
            if self.mecab:
                try:
                    mecab_morphs = self.mecab.morphs(text)
                    analysis['mecab_morphemes'] = mecab_morphs[:100]
                    analysis['mecab_total'] = len(mecab_morphs)
                except:
                    pass
            
            return analysis
            
        except Exception as e:
            logger.error(f"í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def _extract_linguistic_features(self, text: str, sentences: List[str]) -> Dict[str, Any]:
        """ì–¸ì–´í•™ì  íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # ê¸°ë³¸ í†µê³„
        features['char_count'] = len(text)
        features['word_count'] = len(text.split())
        features['sentence_count'] = len(sentences)
        features['avg_sentence_length'] = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
        
        # ë¬¸ì¥ ìœ í˜• ë¶„ì„
        question_sentences = [s for s in sentences if s.strip().endswith('?')]
        exclamation_sentences = [s for s in sentences if s.strip().endswith('!')]
        
        features['question_ratio'] = len(question_sentences) / len(sentences) if sentences else 0
        features['exclamation_ratio'] = len(exclamation_sentences) / len(sentences) if sentences else 0
        
        # ê²©ì‹ì„± ë¶„ì„
        formal_endings = ['ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í•˜ì‹­ì‹œì˜¤', 'í•˜ê² ìŠµë‹ˆë‹¤']
        informal_endings = ['í•´', 'í•´ìš”', 'ì´ì•¼', 'ì•¼', 'ë„¤']
        
        formal_count = sum(text.count(ending) for ending in formal_endings)
        informal_count = sum(text.count(ending) for ending in informal_endings)
        
        if formal_count + informal_count > 0:
            features['formality_score'] = formal_count / (formal_count + informal_count)
        else:
            features['formality_score'] = 0.5
        
        # ì–´íœ˜ ë‹¤ì–‘ì„±
        words = text.split()
        unique_words = set(words)
        features['lexical_diversity'] = len(unique_words) / len(words) if words else 0
        
        # ë³µì¡ì„± ì ìˆ˜
        features['complexity_score'] = self._calculate_complexity(text, sentences)
        
        return features
    
    def _calculate_complexity(self, text: str, sentences: List[str]) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ì„± ì ìˆ˜ ê³„ì‚°"""
        if not sentences:
            return 0.0
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        # ì–´ë ¤ìš´ ë‹¨ì–´ ë¹„ìœ¨ (3ìŒì ˆ ì´ìƒ)
        words = text.split()
        complex_words = [w for w in words if len(w) >= 3]
        complex_word_ratio = len(complex_words) / len(words) if words else 0
        
        # ë³µì¡ì„± ì ìˆ˜ (0-1)
        complexity = min((avg_sentence_length / 20) * 0.5 + complex_word_ratio * 0.5, 1.0)
        
        return complexity
    
    def _tokenize_korean(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í† í°í™”"""
        try:
            # ì—¬ëŸ¬ í† í¬ë‚˜ì´ì € ì¡°í•© ì‚¬ìš©
            tokens = []
            
            # 1. í˜•íƒœì†Œ ë‹¨ìœ„
            morphemes = self.okt.morphs(text)
            tokens.extend(morphemes)
            
            # 2. ì–´ì ˆ ë‹¨ìœ„  
            words = text.split()
            tokens.extend(words)
            
            # 3. ë¬¸ì ë‹¨ìœ„ (í•œê¸€ë§Œ)
            korean_chars = re.findall(r'[ê°€-í£]', text)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            unique_tokens = list(set(tokens))
            
            return unique_tokens[:500]  # ìµœëŒ€ 500ê°œ í† í°
            
        except Exception as e:
            logger.error(f"í† í°í™” ì˜¤ë¥˜: {e}")
            return text.split()[:100]
    
    def _calculate_quality_score(self, text: str, sentences: List[str], morphology: Dict[str, Any]) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê¸¸ì´ ì ìˆ˜ (0.3)
        ideal_length = 500
        length_score = min(len(text) / ideal_length, 1.0) * 0.3
        score += length_score
        
        # ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜ (0.2)
        if sentences and len(sentences) >= 2:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            structure_score = min(avg_sentence_length / 50, 1.0) * 0.2
            score += structure_score
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ (0.3)
        if 'vocabulary_richness' in morphology:
            vocab_score = morphology['vocabulary_richness'] * 0.3
            score += vocab_score
        
        # ë¬¸ë²•ì  ì™„ì„±ë„ ì ìˆ˜ (0.2)
        grammar_patterns = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ']
        pattern_count = sum(text.count(pattern) for pattern in grammar_patterns)
        grammar_score = min(pattern_count / 10, 1.0) * 0.2
        score += grammar_score
        
        return min(score, 1.0)

class KoreanDataPipeline:
    """í•œêµ­ì–´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í†µí•© ê´€ë¦¬"""
    
    def __init__(self):
        self.crawler = KoreanWebCrawler()
        self.processor = KoreanTextProcessor()
        self.db_path = "korean_training_data.db"
        self.setup_database()
        
        self.pipeline_stats = {
            'total_crawled': 0,
            'total_processed': 0,
            'high_quality_texts': 0,
            'training_ready_texts': 0,
            'last_run': None
        }
    
    def setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS raw_korean_data (
                id INTEGER PRIMARY KEY,
                source_type TEXT,
                source_url TEXT,
                title TEXT,
                content TEXT,
                collected_date TIMESTAMP,
                domain TEXT,
                category TEXT,
                language TEXT,
                content_hash TEXT UNIQUE,
                processed BOOLEAN DEFAULT FALSE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processed_korean_data (
                id INTEGER PRIMARY KEY,
                raw_data_id INTEGER,
                cleaned_text TEXT,
                sentences TEXT,  -- JSON
                morphology TEXT,  -- JSON
                features TEXT,  -- JSON
                tokens TEXT,  -- JSON
                quality_score REAL,
                token_count INTEGER,
                sentence_count INTEGER,
                processing_date TIMESTAMP,
                training_ready BOOLEAN DEFAULT FALSE,
                FOREIGN KEY (raw_data_id) REFERENCES raw_korean_data (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS training_dataset (
                id INTEGER PRIMARY KEY,
                text TEXT,
                metadata TEXT,  -- JSON
                quality_score REAL,
                tokens INTEGER,
                created_date TIMESTAMP,
                dataset_version TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def run_full_pipeline(self, max_pages: int = 500):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ í•œêµ­ì–´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
        logger.info("ChatGPT/Gemini/Claude ë°©ì‹ìœ¼ë¡œ ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬")
        
        try:
            # 1ë‹¨ê³„: ì›¹ í¬ë¡¤ë§
            logger.info("\nğŸ“¥ 1ë‹¨ê³„: í•œêµ­ì–´ ì›¹ ë°ì´í„° ìˆ˜ì§‘")
            crawled_data = await self.crawler.crawl_korean_web(max_pages)
            self._save_raw_data(crawled_data)
            self.pipeline_stats['total_crawled'] = len(crawled_data)
            
            # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            logger.info("\nğŸ”§ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í’ˆì§ˆ í•„í„°ë§")
            processed_data = await self._process_crawled_data()
            self.pipeline_stats['total_processed'] = len(processed_data)
            
            # 3ë‹¨ê³„: í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
            logger.info("\nğŸ“š 3ë‹¨ê³„: í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±")
            training_dataset = self._create_training_dataset(processed_data)
            self.pipeline_stats['training_ready_texts'] = len(training_dataset)
            
            # 4ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸
            logger.info("\nğŸ“Š 4ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±")
            quality_report = self._generate_quality_report()
            
            self.pipeline_stats['last_run'] = datetime.now()
            
            # ê²°ê³¼ ì¶œë ¥
            self._print_pipeline_results(quality_report)
            
            return {
                'crawled_data': crawled_data,
                'processed_data': processed_data,
                'training_dataset': training_dataset,
                'quality_report': quality_report,
                'stats': self.pipeline_stats
            }
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            raise
        finally:
            await self.crawler.close()
    
    def _save_raw_data(self, data_list: List[Dict[str, Any]]):
        """ì›ë³¸ ë°ì´í„° ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        duplicate_count = 0
        
        for data in data_list:
            try:
                cursor.execute('''
                    INSERT INTO raw_korean_data 
                    (source_type, source_url, title, content, collected_date, domain, category, language, content_hash)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data.get('source_type', ''),
                    data.get('source_url', ''),
                    data.get('title', ''),
                    data.get('content', ''),
                    data.get('collected_date', datetime.now().isoformat()),
                    data.get('domain', ''),
                    data.get('category', ''),
                    data.get('language', 'korean'),
                    data.get('content_hash', '')
                ))
                saved_count += 1
                
            except sqlite3.IntegrityError:
                duplicate_count += 1
                continue
        
        conn.commit()
        conn.close()
        
        logger.info(f"ğŸ’¾ ì›ë³¸ ë°ì´í„° ì €ì¥: {saved_count}ê°œ ì €ì¥, {duplicate_count}ê°œ ì¤‘ë³µ")
    
    async def _process_crawled_data(self) -> List[Dict[str, Any]]:
        """í¬ë¡¤ë§ëœ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë¯¸ì²˜ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT id, content FROM raw_korean_data WHERE processed = FALSE")
        raw_data = cursor.fetchall()
        conn.close()
        
        if not raw_data:
            logger.info("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        logger.info(f"ğŸ“ {len(raw_data)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì‘")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        texts = [content for _, content in raw_data]
        processed_results = self.processor.process_text_batch(texts)
        
        # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
        self._save_processed_data(raw_data, processed_results)
        
        return processed_results
    
    def _save_processed_data(self, raw_data: List[tuple], processed_results: List[Dict[str, Any]]):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for i, result in enumerate(processed_results):
            if i < len(raw_data):
                raw_id = raw_data[i][0]
                
                # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
                cursor.execute('''
                    INSERT INTO processed_korean_data 
                    (raw_data_id, cleaned_text, sentences, morphology, features, tokens, 
                     quality_score, token_count, sentence_count, processing_date, training_ready)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    raw_id,
                    result.get('cleaned_text', ''),
                    json.dumps(result.get('sentences', []), ensure_ascii=False),
                    json.dumps(result.get('morphology', {}), ensure_ascii=False),
                    json.dumps(result.get('features', {}), ensure_ascii=False),
                    json.dumps(result.get('tokens', []), ensure_ascii=False),
                    result.get('quality_score', 0.0),
                    result.get('token_count', 0),
                    result.get('sentence_count', 0),
                    result.get('processing_date', datetime.now().isoformat()),
                    result.get('quality_score', 0) >= 0.7  # 0.7 ì´ìƒì´ë©´ í›ˆë ¨ ê°€ëŠ¥
                ))
                
                # ì›ë³¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                cursor.execute(
                    "UPDATE raw_korean_data SET processed = TRUE WHERE id = ?",
                    (raw_id,)
                )
        
        conn.commit()
        conn.close()
        
        logger.info(f"ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(processed_results)}ê°œ")
    
    def _create_training_dataset(self, processed_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ ì„ ë³„ (quality_score >= 0.7)
        cursor.execute('''
            SELECT cleaned_text, features, quality_score, token_count, sentence_count
            FROM processed_korean_data 
            WHERE training_ready = TRUE AND quality_score >= 0.7
            ORDER BY quality_score DESC
        ''')
        
        high_quality_data = cursor.fetchall()
        conn.close()
        
        training_dataset = []
        dataset_version = f"korean_v{datetime.now().strftime('%Y%m%d_%H%M')}"
        
        for text, features_json, quality_score, token_count, sentence_count in high_quality_data:
            features = json.loads(features_json) if features_json else {}
            
            training_sample = {
                'text': text,
                'metadata': {
                    'quality_score': quality_score,
                    'token_count': token_count,
                    'sentence_count': sentence_count,
                    'features': features,
                    'dataset_version': dataset_version,
                    'created_date': datetime.now().isoformat()
                }
            }
            
            training_dataset.append(training_sample)
        
        # í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥
        self._save_training_dataset(training_dataset, dataset_version)
        
        logger.info(f"ğŸ“š í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(training_dataset)}ê°œ ìƒ˜í”Œ")
        return training_dataset
    
    def _save_training_dataset(self, dataset: List[Dict[str, Any]], version: str):
        """í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for sample in dataset:
            cursor.execute('''
                INSERT INTO training_dataset 
                (text, metadata, quality_score, tokens, created_date, dataset_version)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                sample['text'],
                json.dumps(sample['metadata'], ensure_ascii=False),
                sample['metadata']['quality_score'],
                sample['metadata']['token_count'],
                sample['metadata']['created_date'],
                version
            ))
        
        conn.commit()
        conn.close()
    
    def _generate_quality_report(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì „ì²´ í†µê³„
        cursor.execute("SELECT COUNT(*) FROM raw_korean_data")
        total_raw = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM processed_korean_data")
        total_processed = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM processed_korean_data WHERE training_ready = TRUE")
        training_ready = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(quality_score) FROM processed_korean_data")
        avg_quality = cursor.fetchone()[0] or 0
        
        # í’ˆì§ˆ ë¶„í¬
        cursor.execute('''
            SELECT 
                CASE 
                    WHEN quality_score >= 0.9 THEN 'excellent'
                    WHEN quality_score >= 0.7 THEN 'good'
                    WHEN quality_score >= 0.5 THEN 'fair'
                    ELSE 'poor'
                END as quality_level,
                COUNT(*) as count
            FROM processed_korean_data
            GROUP BY quality_level
        ''')
        quality_distribution = dict(cursor.fetchall())
        
        # ë„ë©”ì¸ë³„ í†µê³„
        cursor.execute('''
            SELECT r.domain, COUNT(*) as count, AVG(p.quality_score) as avg_quality
            FROM raw_korean_data r
            JOIN processed_korean_data p ON r.id = p.raw_data_id
            GROUP BY r.domain
            ORDER BY count DESC
        ''')
        domain_stats = cursor.fetchall()
        
        conn.close()
        
        report = {
            'total_statistics': {
                'raw_data_count': total_raw,
                'processed_count': total_processed,
                'training_ready_count': training_ready,
                'average_quality_score': round(avg_quality, 3),
                'processing_rate': round(total_processed / total_raw * 100, 1) if total_raw > 0 else 0
            },
            'quality_distribution': quality_distribution,
            'domain_statistics': [
                {'domain': domain, 'count': count, 'avg_quality': round(avg_quality, 3)}
                for domain, count, avg_quality in domain_stats
            ],
            'generated_date': datetime.now().isoformat()
        }
        
        self.pipeline_stats['high_quality_texts'] = quality_distribution.get('excellent', 0) + quality_distribution.get('good', 0)
        
        return report
    
    def _print_pipeline_results(self, quality_report: Dict[str, Any]):
        """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì¶œë ¥"""
        print(f"""
        ğŸ‰ í•œêµ­ì–´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!
        ================================
        
        ğŸ“Š ì „ì²´ í†µê³„:
        - ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°: {quality_report['total_statistics']['raw_data_count']:,}ê°œ
        - ì²˜ë¦¬ëœ ë°ì´í„°: {quality_report['total_statistics']['processed_count']:,}ê°œ  
        - í›ˆë ¨ ê°€ëŠ¥í•œ ë°ì´í„°: {quality_report['total_statistics']['training_ready_count']:,}ê°œ
        - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {quality_report['total_statistics']['average_quality_score']}
        - ì²˜ë¦¬ ì„±ê³µë¥ : {quality_report['total_statistics']['processing_rate']}%
        
        ğŸ† í’ˆì§ˆ ë¶„í¬:
        - ìš°ìˆ˜ (0.9+): {quality_report['quality_distribution'].get('excellent', 0):,}ê°œ
        - ì–‘í˜¸ (0.7+): {quality_report['quality_distribution'].get('good', 0):,}ê°œ  
        - ë³´í†µ (0.5+): {quality_report['quality_distribution'].get('fair', 0):,}ê°œ
        - ë‚®ìŒ (<0.5): {quality_report['quality_distribution'].get('poor', 0):,}ê°œ
        
        ğŸŒ ë„ë©”ì¸ë³„ ìƒìœ„ 5ê°œ:
        """)
        
        for i, domain_stat in enumerate(quality_report['domain_statistics'][:5]):
            print(f"        {i+1}. {domain_stat['domain']}: {domain_stat['count']:,}ê°œ (í’ˆì§ˆ: {domain_stat['avg_quality']})")
        
        print(f"""
        âœ… ì´ì œ AutoCIê°€ ChatGPTì²˜ëŸ¼ ê³ í’ˆì§ˆ í•œêµ­ì–´ ë°ì´í„°ë¡œ í•™ìŠµí•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
        """)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = KoreanDataPipeline()
    
    try:
        print("""
        ğŸ‡°ğŸ‡· AutoCI Korean Data Pipeline
        ===============================
        
        ChatGPT/Gemini/Claude ë°©ì‹ì˜ ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
        
        ì²˜ë¦¬ ë‹¨ê³„:
        1ï¸âƒ£ í•œêµ­ì–´ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ë‰´ìŠ¤, ë¸”ë¡œê·¸, í¬ëŸ¼, ì •ë¶€ì‚¬ì´íŠ¸ ë“±)
        2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° í’ˆì§ˆ í•„í„°ë§
        3ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„ ë° ì–¸ì–´í•™ì  íŠ¹ì„± ì¶”ì¶œ
        4ï¸âƒ£ í† í°í™” ë° í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
        5ï¸âƒ£ í’ˆì§ˆ í‰ê°€ ë° ë¦¬í¬íŠ¸ ìƒì„±
        
        ğŸš€ ì‹œì‘í•©ë‹ˆë‹¤...
        """)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        results = await pipeline.run_full_pipeline(max_pages=100)  # í…ŒìŠ¤íŠ¸ìš© 100í˜ì´ì§€
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if results['training_dataset']:
            print(f"\nğŸ“ í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ:")
            sample = results['training_dataset'][0]
            print(f"í…ìŠ¤íŠ¸: {sample['text'][:200]}...")
            print(f"í’ˆì§ˆ ì ìˆ˜: {sample['metadata']['quality_score']}")
            print(f"í† í° ìˆ˜: {sample['metadata']['token_count']}")
        
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 