#!/bin/bash

# AutoCI v3.0 - ê³ ê¸‰ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
# ëŒ€í˜• ëª¨ë¸ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤ (ì´ 150GB+)

echo "ğŸš€ AutoCI v3.0 - ê³ ê¸‰ AI ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"
echo "======================================="
echo ""
echo "âš ï¸  ì£¼ì˜ì‚¬í•­:"
echo "  â€¢ ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: ~150GB"
echo "  â€¢ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-3ì‹œê°„ (ì¸í„°ë„· ì†ë„ì— ë”°ë¼)"
echo "  â€¢ 32GB RAM ê¶Œì¥"
echo "  â€¢ 200GB ì—¬ìœ  ê³µê°„ í•„ìš”"
echo ""

# ê°€ìƒí™˜ê²½ í™•ì¸
if [ -z "$VIRTUAL_ENV" ] && [ -d "autoci_env" ]; then
    echo "ğŸ”„ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
    source autoci_env/bin/activate
fi

# Python í™˜ê²½ í™•ì¸
if ! command -v python &> /dev/null; then
    echo "âŒ Pythonì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    exit 1
fi

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
python -m pip install huggingface-hub transformers torch --quiet

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p models
cd models

# ì‚¬ìš©ìì—ê²Œ ì–´ë–¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí• ì§€ ì„ íƒí•˜ê²Œ í•¨
echo ""
echo "ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:"
echo "1. Llama 3.1 70B (4-bit ì–‘ìí™”) - ~35GB"
echo "2. Qwen2.5 72B (4-bit ì–‘ìí™”) - ~36GB"  
echo "3. DeepSeek V2.5 (4-bit ì–‘ìí™”) - ~50GB"
echo "4. ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥) - ~150GB"
echo "5. ê¸°ë³¸ ëª¨ë¸ë§Œ (Code Llama 7B) - ~13GB"
echo ""
read -p "ì„ íƒí•˜ì„¸ìš” (1-5): " choice

case $choice in
    1)
        echo "ğŸ¦™ Llama 3.1 70B ë‹¤ìš´ë¡œë“œ ì¤‘..."
        download_llama=true
        ;;
    2)
        echo "ğŸ¤– Qwen2.5 72B ë‹¤ìš´ë¡œë“œ ì¤‘..."
        download_qwen=true
        ;;
    3)
        echo "ğŸ§  DeepSeek V2.5 ë‹¤ìš´ë¡œë“œ ì¤‘..."
        download_deepseek=true
        ;;
    4)
        echo "ğŸ“¦ ëª¨ë“  ê³ ê¸‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
        download_llama=true
        download_qwen=true
        download_deepseek=true
        ;;
    5)
        echo "ğŸ“¦ ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
        download_basic=true
        ;;
    *)
        echo "âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤."
        exit 1
        ;;
esac

echo ""
echo "ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
echo ""

# Python ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > download_models.py << 'EOF'
#!/usr/bin/env python3
"""
AutoCI ê³ ê¸‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def download_model(repo_id, local_dir, description):
    """ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤"""
    print(f"ğŸ“¥ {description} ë‹¤ìš´ë¡œë“œ ì¤‘...")
    print(f"   ì €ì¥ì†Œ: {repo_id}")
    print(f"   ì €ì¥ ìœ„ì¹˜: {local_dir}")
    
    try:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=repo_id,
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            resume_download=True,
            cache_dir="./cache"
        )
        print(f"âœ… {description} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return True
    except Exception as e:
        print(f"âŒ {description} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """ë©”ì¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜"""
    import sys
    
    models_to_download = []
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì–´ë–¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí• ì§€ ê²°ì •
    if len(sys.argv) > 1:
        if "llama" in sys.argv:
            models_to_download.append({
                "repo_id": "meta-llama/Llama-3.1-70B-Instruct",
                "local_dir": "./Llama-3.1-70B-Instruct",
                "description": "Llama 3.1 70B Instruct"
            })
        
        if "qwen" in sys.argv:
            models_to_download.append({
                "repo_id": "Qwen/Qwen2.5-72B-Instruct",
                "local_dir": "./Qwen2.5-72B-Instruct", 
                "description": "Qwen2.5 72B Instruct"
            })
        
        if "deepseek" in sys.argv:
            models_to_download.append({
                "repo_id": "deepseek-ai/DeepSeek-V2.5",
                "local_dir": "./DeepSeek-V2.5",
                "description": "DeepSeek V2.5"
            })
            
        if "basic" in sys.argv:
            models_to_download.append({
                "repo_id": "codellama/CodeLlama-7b-Instruct-hf",
                "local_dir": "../CodeLlama-7b-Instruct-hf",
                "description": "Code Llama 7B Instruct"
            })
    
    if not models_to_download:
        print("âŒ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ì´ {len(models_to_download)}ê°œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    print("")
    
    success_count = 0
    for i, model in enumerate(models_to_download, 1):
        print(f"[{i}/{len(models_to_download)}] {model['description']}")
        if download_model(model["repo_id"], model["local_dir"], model["description"]):
            success_count += 1
        print("")
    
    print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(models_to_download)} ì„±ê³µ")
    
    # ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±
    create_model_info_file(models_to_download, success_count)

def create_model_info_file(downloaded_models, success_count):
    """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±"""
    info_content = f"""# AutoCI ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì •ë³´

## ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{len(downloaded_models)}

### ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:
"""
    
    for model in downloaded_models:
        if os.path.exists(model["local_dir"]):
            info_content += f"- âœ… {model['description']} - {model['local_dir']}\n"
        else:
            info_content += f"- âŒ {model['description']} - ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n"
    
    info_content += f"""
### ì‚¬ìš©ë²•:
```python
# ëª¨ë¸ ë¡œë“œ ì˜ˆì‹œ
from transformers import AutoModelForCausalLM, AutoTokenizer

# Llama 3.1 70B (ì˜ˆì‹œ)
tokenizer = AutoTokenizer.from_pretrained("./Llama-3.1-70B-Instruct")
model = AutoModelForCausalLM.from_pretrained(
    "./Llama-3.1-70B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True  # ë©”ëª¨ë¦¬ ì ˆì•½
)
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:
- 4-bit ì–‘ìí™” ì‚¬ìš© ì‹œ ëª¨ë¸ë‹¹ ~25-30GB RAM
- 32GB RAMì—ì„œ 1ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰ ê¶Œì¥
- GPU ë©”ëª¨ë¦¬: 8GB+ ê¶Œì¥

### ì„±ëŠ¥ ìµœì í™”:
- `load_in_4bit=True` ì‚¬ìš©
- `device_map="auto"` ì‚¬ìš©
- í•„ìš”ì‹œ CPU ì˜¤í”„ë¡œë”© í™œìš©
"""
    
    with open("../downloaded_models_info.md", "w", encoding="utf-8") as f:
        f.write(info_content)
    
    print("ğŸ“„ ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±: downloaded_models_info.md")

if __name__ == "__main__":
    main()
EOF

# ì„ íƒì— ë”°ë¼ Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
args=""
if [ "$download_llama" = true ]; then
    args="$args llama"
fi
if [ "$download_qwen" = true ]; then
    args="$args qwen"
fi
if [ "$download_deepseek" = true ]; then
    args="$args deepseek"
fi
if [ "$download_basic" = true ]; then
    args="$args basic"
fi

# Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python download_models.py $args

# ì •ë¦¬
rm -f download_models.py

echo ""
echo "ğŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸ“‹ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™•ì¸:"
if [ -d "Llama-3.1-70B-Instruct" ]; then
    echo "  âœ… Llama 3.1 70B"
fi
if [ -d "Qwen2.5-72B-Instruct" ]; then
    echo "  âœ… Qwen2.5 72B"
fi
if [ -d "DeepSeek-V2.5" ]; then
    echo "  âœ… DeepSeek V2.5"
fi
if [ -d "../CodeLlama-7b-Instruct-hf" ]; then
    echo "  âœ… Code Llama 7B"
fi

echo ""
echo "ğŸ“– ìì„¸í•œ ì •ë³´: downloaded_models_info.md íŒŒì¼ í™•ì¸"
echo "ğŸš€ AutoCI ì‹œì‘: python start_autoci_agent.py --advanced-models"
echo "" 