#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ - ì €ì‚¬ì–‘ ìµœì í™” ë²„ì „
C#ê³¼ í•œê¸€ì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
Llama-3.1-8B, CodeLlama-13B, Qwen2.5-Coder-32B ëª¨ë¸ì„ í™œìš©

RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ì— ìµœì í™”ë¨
autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import random
import asyncio
import logging
import gc
import psutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('continuous_learning.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Hugging Face í† í°
HF_TOKEN = "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"

@dataclass
class LearningTopic:
    """í•™ìŠµ ì£¼ì œ"""
    id: str
    category: str
    topic: str
    difficulty: int  # 1-5
    korean_keywords: List[str]
    csharp_concepts: List[str]
    godot_integration: Optional[str] = None
    
@dataclass
class LearningSession:
    """í•™ìŠµ ì„¸ì…˜"""
    session_id: str
    start_time: datetime
    topics_covered: List[str]
    questions_asked: int
    successful_answers: int
    models_used: Dict[str, int]
    knowledge_gained: List[Dict[str, Any]]
    
class ContinuousLearningSystem:
    def __init__(self, models_dir: str = "./models", learning_dir: str = "./continuous_learning", max_memory_gb: float = 32.0):
        self.models_dir = Path(models_dir)
        self.learning_dir = Path(learning_dir)
        self.learning_dir.mkdir(exist_ok=True)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • (ì €ì‚¬ì–‘ ìµœì í™”)
        self.max_memory_gb = max_memory_gb
        self.memory_threshold = 0.70  # 70% ì‚¬ìš© ì‹œ ëª¨ë¸ ì–¸ë¡œë“œ (ë” ë³´ìˆ˜ì )
        self.currently_loaded_model = None
        self.model_cache = {}  # ì–¸ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì €ì¥
        
        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        self.questions_dir = self.learning_dir / "questions"
        self.answers_dir = self.learning_dir / "answers"
        self.knowledge_base_dir = self.learning_dir / "knowledge_base"
        
        for dir in [self.questions_dir, self.answers_dir, self.knowledge_base_dir]:
            dir.mkdir(exist_ok=True)
            
        # ëª¨ë¸ ì •ë³´ (ì‹¤ì œ ë¡œë”©ì€ í•„ìš”í•  ë•Œë§Œ)
        self.available_models = {}
        self.load_model_info()
        
        # í•™ìŠµ ì£¼ì œ ì •ì˜
        self.learning_topics = self._initialize_learning_topics()
        
        # í˜„ì¬ ì„¸ì…˜
        self.current_session = None
        
        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_base = self._load_knowledge_base()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        self.memory_usage_history = []
        
    def _initialize_learning_topics(self) -> List[LearningTopic]:
        """5ê°€ì§€ í•µì‹¬ í•™ìŠµ ì£¼ì œ ì´ˆê¸°í™” (DeepSeek-coder ìµœì í™”)"""
        topics = [
            # 1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì „ë¬¸ í•™ìŠµ (DeepSeek-coder íŠ¹í™”)
            LearningTopic("core_csharp_basics", "C# í”„ë¡œê·¸ë˜ë°", "C# ê¸°ì´ˆ ë¬¸ë²•", 2,
                         ["ë³€ìˆ˜", "íƒ€ì…", "ì—°ì‚°ì", "ì¡°ê±´ë¬¸", "ë°˜ë³µë¬¸", "ë°°ì—´"],
                         ["int", "string", "bool", "var", "if", "for", "foreach", "array"],
                         "Godot Node ê¸°ë³¸ í”„ë¡œí¼í‹°"),
            LearningTopic("core_csharp_oop", "C# í”„ë¡œê·¸ë˜ë°", "ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°", 3,
                         ["í´ë˜ìŠ¤", "ê°ì²´", "ìƒì†", "ë‹¤í˜•ì„±", "ìº¡ìŠí™”", "ì¸í„°í˜ì´ìŠ¤"],
                         ["class", "object", "inheritance", "polymorphism", "interface", "abstract"],
                         "Godot ë…¸ë“œ ìƒì† êµ¬ì¡°"),
            LearningTopic("core_csharp_advanced", "C# í”„ë¡œê·¸ë˜ë°", "ê³ ê¸‰ C# ê¸°ëŠ¥", 4,
                         ["ì œë„¤ë¦­", "ë¹„ë™ê¸°", "LINQ", "ë¸ë¦¬ê²Œì´íŠ¸", "ëŒë‹¤", "ì†ì„±"],
                         ["generics", "async", "await", "Task", "LINQ", "delegate", "lambda"],
                         "Godot ê³ ê¸‰ ìŠ¤í¬ë¦½íŒ…"),
            
            # 2ï¸âƒ£ í•œê¸€ í”„ë¡œê·¸ë˜ë° ìš©ì–´ í•™ìŠµ (DeepSeek-coder ë²ˆì—­ íŠ¹í™”)
            LearningTopic("core_korean_translation", "í•œê¸€ ìš©ì–´", "í”„ë¡œê·¸ë˜ë° ìš©ì–´ ë²ˆì—­", 2,
                         ["ë³€ìˆ˜", "í•¨ìˆ˜", "í´ë˜ìŠ¤", "ê°ì²´", "ìƒì†", "ì¸í„°í˜ì´ìŠ¤", "ì•Œê³ ë¦¬ì¦˜"],
                         ["variable", "function", "class", "object", "inheritance", "interface", "algorithm"],
                         "Godot ìš©ì–´ í•œê¸€í™”"),
            LearningTopic("core_korean_concepts", "í•œê¸€ ìš©ì–´", "í•œêµ­ì–´ ì½”ë”© ê°œë…", 3,
                         ["ìë£Œêµ¬ì¡°", "ë””ìì¸íŒ¨í„´", "ì•„í‚¤í…ì²˜", "í”„ë ˆì„ì›Œí¬", "ë¼ì´ë¸ŒëŸ¬ë¦¬"],
                         ["data structure", "design pattern", "architecture", "framework", "library"],
                         "Godot ì•„í‚¤í…ì²˜ ì´í•´"),
            
            # 3ï¸âƒ£ Godot ì—”ì§„ ê°œë°œ ë°©í–¥ì„± ë¶„ì„ (DeepSeek-coder Godot íŠ¹í™”)
            LearningTopic("core_godot_architecture", "Godot ì—”ì§„", "Godot 4.x ì•„í‚¤í…ì²˜", 4,
                         ["ë…¸ë“œì‹œìŠ¤í…œ", "ì”¬íŠ¸ë¦¬", "ë¦¬ì†ŒìŠ¤", "ì‹œê·¸ë„", "ë Œë”ë§"],
                         ["Node", "SceneTree", "Resource", "Signal", "RenderingServer"],
                         "í˜„ëŒ€ì  ê²Œì„ ì—”ì§„ ì„¤ê³„"),
            LearningTopic("core_godot_future", "Godot ì—”ì§„", "Godot ë¯¸ë˜ ë°©í–¥ì„±", 5,
                         ["ì›¹ì–´ì…ˆë¸”ë¦¬", "ëª¨ë°”ì¼ìµœì í™”", "VRì§€ì›", "AIí†µí•©", "í´ë¼ìš°ë“œ"],
                         ["WebAssembly", "mobile", "VR", "AI", "cloud", "C# bindings"],
                         "ì°¨ì„¸ëŒ€ ê²Œì„ ê°œë°œ"),
            
            # 4ï¸âƒ£ Godot ë‚´ì¥ ë„¤íŠ¸ì›Œí‚¹ (AI ì œì–´) (DeepSeek-coder ë„¤íŠ¸ì›Œí‚¹ íŠ¹í™”)
            LearningTopic("core_godot_networking", "Godot ë„¤íŠ¸ì›Œí‚¹", "MultiplayerAPI ì‹œìŠ¤í…œ", 4,
                         ["ë©€í‹°í”Œë ˆì´ì–´", "ì„œë²„", "í´ë¼ì´ì–¸íŠ¸", "ë™ê¸°í™”", "RPC", "í”¼ì–´"],
                         ["MultiplayerAPI", "server", "client", "sync", "RPC", "peer"],
                         "ì‹¤ì‹œê°„ ë©€í‹°í”Œë ˆì´ì–´"),
            LearningTopic("core_godot_ai_network", "Godot ë„¤íŠ¸ì›Œí‚¹", "AI ë„¤íŠ¸ì›Œí¬ ì œì–´", 5,
                         ["AIì œì–´", "ìë™ë™ê¸°í™”", "ì§€ëŠ¥í˜•ë§¤ì¹­", "ì˜ˆì¸¡ë³´ìƒ", "ìµœì í™”"],
                         ["AI control", "auto sync", "intelligent matching", "prediction", "optimization"],
                         "AI ê¸°ë°˜ ë„¤íŠ¸ì›Œí‚¹"),
            
            # 5ï¸âƒ£ Nakama ì„œë²„ ê°œë°œ (AI ìµœì í™”) (DeepSeek-coder ì„œë²„ íŠ¹í™”)
            LearningTopic("core_nakama_basics", "Nakama ì„œë²„", "Nakama ê¸°ë³¸ êµ¬ì¡°", 3,
                         ["ê²Œì„ì„œë²„", "ì¸ì¦", "ì„¸ì…˜", "ë§¤ì¹˜ë©”ì´í‚¹", "ë¦¬ë”ë³´ë“œ"],
                         ["game server", "authentication", "session", "matchmaking", "leaderboard"],
                         "ë°±ì—”ë“œ ì„œë¹„ìŠ¤ í†µí•©"),
            LearningTopic("core_nakama_ai", "Nakama ì„œë²„", "AI í†µí•© Nakama", 5,
                         ["AIë§¤ì¹­", "ì§€ëŠ¥í˜•ìŠ¤í† ë¦¬ì§€", "ìë™ìŠ¤ì¼€ì¼ë§", "ì˜ˆì¸¡ë¶„ì„"],
                         ["AI matching", "intelligent storage", "auto scaling", "predictive analytics"],
                         "ì°¨ì„¸ëŒ€ ê²Œì„ ë°±ì—”ë“œ"),
        ]
        
        # ëª¨ë“  ì£¼ì œì— DeepSeek-coder ìš°ì„  íƒœê·¸ ì¶”ê°€
        for topic in topics:
            topic.godot_integration = f"[DeepSeek ìš°ì„ ] {topic.godot_integration}"
            
        return topics
        
    def _load_knowledge_base(self) -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "csharp_patterns": {},
            "korean_translations": {},
            "godot_integrations": {},
            "common_errors": {},
            "best_practices": {}
        }
        
    def _save_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, indent=2, ensure_ascii=False)
            
    def load_model_info(self):
        """ëª¨ë¸ ì •ë³´ë§Œ ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ì€ í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)"""
        models_info_file = self.models_dir / "installed_models.json"
        if not models_info_file.exists():
            logger.error("ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. install_llm_models.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        with open(models_info_file, 'r', encoding='utf-8') as f:
            installed_models = json.load(f)
            
        self.available_models = installed_models
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.available_models.keys())}")
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB)"""
        return psutil.virtual_memory().used / (1024**3)
        
    def get_memory_usage_percent(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜ (%)"""
        return psutil.virtual_memory().percent
        
    def check_memory_safety(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì•ˆì „í•œì§€ í™•ì¸"""
        current_usage = self.get_memory_usage()
        usage_percent = self.get_memory_usage_percent()
        
        # í˜„ì¬ ì‚¬ìš©ëŸ‰ì´ ìµœëŒ€ í—ˆìš©ëŸ‰ì˜ 85%ë¥¼ ë„˜ìœ¼ë©´ ìœ„í—˜
        return current_usage < (self.max_memory_gb * self.memory_threshold)
        
    def unload_current_model(self):
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.currently_loaded_model:
            logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ {self.currently_loaded_model} ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.currently_loaded_model = None
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
    def load_model(self, model_name: str) -> bool:
        """í•„ìš” ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if model_name == self.currently_loaded_model:
            return True  # ì´ë¯¸ ë¡œë“œë¨
            
        if model_name not in self.available_models:
            logger.error(f"ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False
            
        # ë©”ëª¨ë¦¬ í™•ì¸ í›„ í•„ìš”ì‹œ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
        if not self.check_memory_safety() and self.currently_loaded_model:
            self.unload_current_model()
            
        try:
            logger.info(f"{model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            info = self.available_models[model_name]
            model_id = info['model_id']
            
            # RTX 2080 8GB ìµœì í™” ëª¨ë¸ë§Œ í—ˆìš©
            rtx_2080_optimized = {
                "bitnet-b1.58-2b": {"max_vram": 1, "device": "cpu"},
                "gemma-4b": {"max_vram": 4, "device": "cuda:0"},
                "phi3-mini": {"max_vram": 6, "device": "cuda:0"},
                "deepseek-coder-7b": {"max_vram": 6, "device": "cuda:0"},
                "mistral-7b": {"max_vram": 7, "device": "cuda:0"}
            }
            
            if model_name not in rtx_2080_optimized:
                logger.warning(f"âŒ {model_name}ì€ RTX 2080 8GBì— ìµœì í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {', '.join(rtx_2080_optimized.keys())}")
                logger.info("ğŸ’¡ install_llm_models_rtx2080.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì í™”ëœ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
                return False
            
            model_config = rtx_2080_optimized[model_name]
            logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {model_name} (VRAM: {model_config['max_vram']}GB)")
            
            # AutoTokenizerì™€ AutoModelForCausalLMì„ ì§ì ‘ ì‚¬ìš©
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            hf_token = os.getenv('HF_TOKEN', None)
            
            # ëª¨ë¸ë³„ ìµœì  ì„¤ì •
            device_map = model_config['device']
            torch_dtype = torch.float32 if device_map == "cpu" else torch.float16
            quantization_config = None
            
            # 4bit ì–‘ìí™” ì„¤ì • (GPU ëª¨ë¸ìš©)
            if device_map != "cpu" and info.get('quantization') == '4bit':
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                token=hf_token,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            if device_map == "cpu":
                model_kwargs["device_map"] = "cpu"
            else:
                model_kwargs["device_map"] = {"": 0}  # GPU 0 ì‚¬ìš©
                if quantization_config:
                    model_kwargs["quantization_config"] = quantization_config
            
            if hf_token:
                model_kwargs["token"] = hf_token
            
            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ë˜í¼
            def optimized_generate(prompt):
                try:
                    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=1024,
                        padding=True
                    )
                    
                    if device_map != "cpu":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs.input_ids,
                            attention_mask=inputs.attention_mask,
                            max_new_tokens=150,  # RTX 2080 ìµœì í™”
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(
                        outputs[0][inputs.input_ids.shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return [{"generated_text": response}]
                    
                except Exception as e:
                    logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    return [{"generated_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}]
            
            pipe = optimized_generate
            
            self.model_cache[model_name] = {
                "pipeline": pipe,
                "features": info['features'],
                "info": info
            }
            
            self.currently_loaded_model = model_name
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            memory_usage = self.get_memory_usage()
            self.memory_usage_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": model_name,
                "memory_gb": memory_usage,
                "memory_percent": self.get_memory_usage_percent()
            })
            
            logger.info(f"âœ“ {model_name} ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬: {memory_usage:.1f}GB)")
            return True
            
        except Exception as e:
            logger.error(f"âœ— {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
                
    def _get_quantization_config(self, quantization: str):
        """ì–‘ìí™” ì„¤ì • ë°˜í™˜ (RTX 2080 8GB ìµœì í™”)"""
        if quantization == "4bit":
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True
            )
        elif quantization == "8bit":
            return BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True,  # RTX 2080 8GB í•„ìˆ˜
                llm_int8_has_fp16_weight=False  # ë©”ëª¨ë¦¬ ì ˆì•½
            )
        return None
        
    def generate_question(self, topic: LearningTopic) -> Dict[str, Any]:
        """í•™ìŠµ ì§ˆë¬¸ ìƒì„±"""
        question_types = [
            "explain",      # ê°œë… ì„¤ëª…
            "example",      # ì˜ˆì œ ì½”ë“œ
            "translate",    # í•œê¸€-ì˜ì–´ ë²ˆì—­
            "error",        # ì˜¤ë¥˜ ìˆ˜ì •
            "optimize",     # ìµœì í™”
            "integrate"     # Godot í†µí•©
        ]
        
        question_type = random.choice(question_types)
        
        # ì§ˆë¬¸ í…œí”Œë¦¿
        templates = {
            "explain": {
                "korean": f"{topic.topic}ì— ëŒ€í•´ í•œê¸€ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. íŠ¹íˆ {random.choice(topic.korean_keywords)}ì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.",
                "english": f"Explain {topic.topic} in C# with focus on {random.choice(topic.csharp_concepts)}."
            },
            "example": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•˜ëŠ” C# ì½”ë“œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê³  í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"Write a C# code example demonstrating {topic.topic} with comments."
            },
            "translate": {
                "korean": f"ë‹¤ìŒ C# ê°œë…ì„ í•œê¸€ë¡œ ë²ˆì—­í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”: {random.choice(topic.csharp_concepts)}",
                "english": f"Translate and explain this Korean term in C# context: {random.choice(topic.korean_keywords)}"
            },
            "error": {
                "korean": f"{topic.topic} ê´€ë ¨ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"What are common errors with {topic.topic} in C# and how to fix them?"
            },
            "optimize": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to optimize performance when using {topic.topic} in C#?"
            },
            "integrate": {
                "korean": f"Godotì—ì„œ {topic.topic}ì„ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ C# ì½”ë“œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to use {topic.topic} in Godot with C#? Provide examples."
            }
        }
        
        # ì–¸ì–´ ì„ íƒ (í•œê¸€ í•™ìŠµ ê°•ì¡°)
        language = "korean" if random.random() < 0.7 else "english"
        question_text = templates[question_type][language]
        
        return {
            "id": f"{topic.id}_{question_type}_{int(time.time())}",
            "topic": topic.topic,
            "type": question_type,
            "language": language,
            "difficulty": topic.difficulty,
            "question": question_text,
            "keywords": topic.korean_keywords if language == "korean" else topic.csharp_concepts
        }
        
    def select_model_for_question(self, question: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ (5ëŒ€ í•µì‹¬ ì£¼ì œ DeepSeek-coder ìµœì í™”)"""
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ ê³ ë ¤
        rtx_2080_models = {
            "deepseek-coder-7b": {"priority": 10, "specialties": ["code", "csharp", "godot", "korean", "nakama"], "vram": 6},
            "phi3-mini": {"priority": 8, "specialties": ["reasoning", "math", "csharp"], "vram": 6},
            "llama-3.1-8b": {"priority": 7, "specialties": ["general", "korean", "csharp"], "vram": 7},
            "gemma-4b": {"priority": 6, "specialties": ["general", "korean"], "vram": 4},
            "mistral-7b": {"priority": 4, "specialties": ["general", "fast"], "vram": 7}
        }
        
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ í•„í„°ë§
        available_optimized = []
        for model_name in self.available_models:
            if (model_name in rtx_2080_models and 
                self.available_models[model_name].get('rtx_2080_optimized', False)):
                available_optimized.append(model_name)
        
        if not available_optimized:
            logger.warning("âŒ RTX 2080 ìµœì í™” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ DeepSeek-coder 6.7Bë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return None
            
        # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¹´í…Œê³ ë¦¬ í™•ì¸
        core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "Godot ì—”ì§„", "Godot ë„¤íŠ¸ì›Œí‚¹", "Nakama ì„œë²„"]
        topic_category = question.get("category", "")
        is_core_topic = topic_category in core_categories
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ RTX 2080 ìµœì í™”ì´ê³  ì í•©í•˜ë©´ ê³„ì† ì‚¬ìš©
        if (self.currently_loaded_model and 
            self.currently_loaded_model in available_optimized and
            self._is_model_suitable(self.currently_loaded_model, question)):
            return self.currently_loaded_model
            
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„ (5ê°€ì§€ í•µì‹¬ ì£¼ì œ í¬í•¨)
        question_features = set()
        question_text = question.get("question", "").lower()
        topic_text = question.get("topic", "").lower()
        
        # 1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['code', 'programming', 'script', 'function', 'class', 'method', 'c#', 'csharp']):
            question_features.add('csharp')
            question_features.add('code')
        
        # 2ï¸âƒ£ í•œê¸€ ìš©ì–´ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['korean', 'í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­', 'ìš©ì–´', 'ê°œë…']):
            question_features.add('korean')
        
        # 3ï¸âƒ£ Godot ì—”ì§„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['godot', 'engine', 'ì—”ì§„', 'ë…¸ë“œ', 'ì”¬', 'ì•„í‚¤í…ì²˜']):
            question_features.add('godot')
        
        # 4ï¸âƒ£ Godot ë„¤íŠ¸ì›Œí‚¹ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['multiplayer', 'network', 'ë„¤íŠ¸ì›Œí‚¹', 'rpc', 'ë™ê¸°í™”', 'AIì œì–´']):
            question_features.add('networking')
        
        # 5ï¸âƒ£ Nakama ì„œë²„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['nakama', 'server', 'ì„œë²„', 'ë§¤ì¹˜ë©”ì´í‚¹', 'backend']):
            question_features.add('nakama')
        
        # ê¸°íƒ€ íŠ¹ì„±
        if any(word in question_text for word in ['reasoning', 'math', 'ìˆ˜í•™', 'ì¶”ë¡ ', 'ë…¼ë¦¬']):
            question_features.add('reasoning')
        
        # ëª¨ë¸ë³„ ì ìˆ˜ ê³„ì‚°
        model_scores = {}
        for model_name in available_optimized:
            if model_name not in rtx_2080_models:
                continue
                
            model_config = rtx_2080_models[model_name]
            score = model_config['priority']
            
            # ğŸ”¥ í•µì‹¬ ì£¼ì œì—ì„œ DeepSeek-coder ê°•ë ¥ ìš°ì„ ìˆœìœ„
            if model_name == "deepseek-coder-7b":
                # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¼ë©´ ë¬´ì¡°ê±´ DeepSeek-coder ì„ íƒ
                if is_core_topic:
                    score += 50  # í•µì‹¬ ì£¼ì œ íŠ¹ëŒ€ ë³´ë„ˆìŠ¤
                    logger.info(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ '{topic_category}' ê°ì§€! DeepSeek-coder ìµœìš°ì„  ì„ íƒ")
                
                # ì„¸ë¶€ íŠ¹ì„±ë³„ ì¶”ê°€ ë³´ë„ˆìŠ¤
                if 'code' in question_features or 'csharp' in question_features:
                    score += 25  # C# ì½”ë”© íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'korean' in question_features:
                    score += 20  # í•œê¸€ ë²ˆì—­ íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'godot' in question_features:
                    score += 20  # Godot íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'networking' in question_features:
                    score += 15  # ë„¤íŠ¸ì›Œí‚¹ íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'nakama' in question_features:
                    score += 15  # ì„œë²„ íŠ¹í™” ë³´ë„ˆìŠ¤
                
                # ì¼ë°˜ ì§ˆë¬¸ì—ë„ ê¸°ë³¸ ë³´ë„ˆìŠ¤
                score += 10
            
            # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ íŠ¹ê¸° ë¶„ì•¼
            elif model_name == "phi3-mini" and 'reasoning' in question_features:
                score += 15
            elif model_name == "gemma-4b" and 'korean' in question_features and not is_core_topic:
                score += 12  # í•µì‹¬ ì£¼ì œê°€ ì•„ë‹ ë•Œë§Œ
            elif model_name == "llama-3.1-8b" and 'korean' in question_features and not is_core_topic:
                score += 10  # í•µì‹¬ ì£¼ì œê°€ ì•„ë‹ ë•Œë§Œ
            
            # íŠ¹ê¸° ë¶„ì•¼ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            for specialty in model_config['specialties']:
                if specialty in question_features:
                    if model_name == "deepseek-coder-7b":
                        score += 12  # DeepSeek-coderì— ë” ë†’ì€ ë³´ë„ˆìŠ¤
                    else:
                        score += 6
            
            # VRAM íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤ (ë‚®ì€ VRAM ì‚¬ìš©ëŸ‰ ì„ í˜¸)
            score += (8 - model_config['vram'])
            
            model_scores[model_name] = score
        
        # ìµœê³  ì ìˆ˜ ëª¨ë¸ ì„ íƒ
        if model_scores:
            best_model = max(model_scores.items(), key=lambda x: x[1])[0]
            selected_score = model_scores[best_model]
            
            # ë¡œê·¸ ì¶œë ¥ (ì‹¤ì‹œê°„ UIìš©)
            if best_model == "deepseek-coder-7b" and is_core_topic:
                logger.info(f"ğŸ¯ í•µì‹¬ ì£¼ì œ ìµœì í™”: {best_model} ì„ íƒ (ì ìˆ˜: {selected_score}) - {topic_category}")
                print(f"Selected model: {best_model} (ğŸ”¥ DeepSeek-coder í•µì‹¬ ì£¼ì œ)")
            else:
                logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {best_model} ì„ íƒ (ì ìˆ˜: {selected_score})")
                print(f"Selected model: {best_model}")
            
            return best_model
        
        # ê¸°ë³¸ ìš°ì„ ìˆœìœ„: DeepSeek > Phi3 > Llama > Gemma > Mistral
        for fallback in ["deepseek-coder-7b", "phi3-mini", "llama-3.1-8b", "gemma-4b", "mistral-7b"]:
            if fallback in available_optimized:
                logger.info(f"ğŸ¯ RTX 2080 ê¸°ë³¸ ëª¨ë¸: {fallback}")
                return fallback
        
        return None
        
    def _is_model_suitable(self, model_name: str, question: Dict[str, Any]) -> bool:
        """ëª¨ë¸ì´ ì§ˆë¬¸ì— ì í•©í•œì§€ í™•ì¸"""
        if model_name not in self.available_models:
            return False
            
        model_features = self.available_models[model_name]['features']
        
        # í•œê¸€ ì§ˆë¬¸ì¸ ê²½ìš°
        if "korean" in question["language"]:
            return "korean" in model_features
            
        # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°
        if question["type"] in ["example", "error", "optimize"]:
            return "code" in model_features or "csharp" in model_features
            
        return True  # ê¸°ë³¸ì ìœ¼ë¡œ ì í•©í•˜ë‹¤ê³  ê°€ì •
        
    async def ask_model(self, model_name: str, question: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°"""
        # ëª¨ë¸ ë¡œë“œ í™•ì¸ ë° í•„ìš”ì‹œ ë¡œë“œ
        if not self.load_model(model_name):
            return {"error": f"Model {model_name} failed to load"}
            
        if model_name not in self.model_cache:
            return {"error": f"Model {model_name} not in cache"}
            
        try:
            model_pipeline = self.model_cache[model_name]["pipeline"]
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_before = self.get_memory_usage()
            if not self.check_memory_safety():
                logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_before:.1f}GB")
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°„ë‹¨í™”)
            system_prompt = f"""C# programming and Godot expert. Answer about {question['topic']} in Korean with examples."""
            full_prompt = f"{system_prompt}\n\nQ: {question['question']}\nA:"
            
            # ëª¨ë¸ í˜¸ì¶œ (ìƒˆë¡œìš´ ê°„ë‹¨í•œ ë°©ì‹)
            start_time = time.time()
            response = model_pipeline(full_prompt)
            
            answer_text = response[0]['generated_text'].strip()
            response_time = time.time() - start_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
            memory_after = self.get_memory_usage()
            memory_delta = memory_after - memory_before
            
            return {
                "model": model_name,
                "question_id": question["id"],
                "answer": answer_text,
                "response_time": response_time,
                "timestamp": datetime.now().isoformat(),
                "memory_usage": {
                    "before_gb": memory_before,
                    "after_gb": memory_after,
                    "delta_gb": memory_delta
                }
            }
            
        except Exception as e:
            logger.error(f"Error with model {model_name}: {str(e)}")
            return {"error": str(e), "model": model_name}
            
    def analyze_answer(self, question: Dict[str, Any], answer: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹µë³€ ë¶„ì„ ë° ì§€ì‹ ì¶”ì¶œ"""
        if "error" in answer:
            return {"success": False, "error": answer["error"]}
            
        analysis = {
            "success": True,
            "quality_score": 0,
            "extracted_knowledge": {},
            "new_patterns": [],
            "improvements": []
        }
        
        answer_text = answer["answer"]
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        quality_factors = {
            "length": len(answer_text) > 100,
            "has_code": "```" in answer_text or "class" in answer_text or "public" in answer_text,
            "has_korean": any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in answer_text),
            "has_explanation": any(word in answer_text.lower() for word in ["because", "therefore", "ì´ìœ ", "ë•Œë¬¸", "ë”°ë¼ì„œ"]),
            "has_example": any(word in answer_text.lower() for word in ["example", "ì˜ˆì œ", "ì˜ˆì‹œ", "ë‹¤ìŒ"])
        }
        
        analysis["quality_score"] = sum(1 for factor in quality_factors.values() if factor) / len(quality_factors)
        
        # ì§€ì‹ ì¶”ì¶œ
        if question["type"] == "translate" and quality_factors["has_korean"]:
            # í•œê¸€ ë²ˆì—­ ì €ì¥
            for keyword in question["keywords"]:
                if keyword in answer_text:
                    self.knowledge_base["korean_translations"][keyword] = answer_text[:200]
                    
        elif question["type"] == "example" and quality_factors["has_code"]:
            # ì½”ë“œ íŒ¨í„´ ì €ì¥
            code_pattern = {
                "topic": question["topic"],
                "code": answer_text,
                "language": question["language"]
            }
            self.knowledge_base["csharp_patterns"][question["topic"]] = code_pattern
            
        elif question["type"] == "error":
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ ì €ì¥
            self.knowledge_base["common_errors"][question["topic"]] = answer_text[:300]
            
        # ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬
        if analysis["quality_score"] > 0.7:
            analysis["new_patterns"].append({
                "topic": question["topic"],
                "pattern": "High quality answer",
                "model": answer["model"]
            })
            
        return analysis
        
    def save_qa_pair(self, question: Dict[str, Any], answer: Dict[str, Any], analysis: Dict[str, Any]):
        """ì§ˆë¬¸-ë‹µë³€ ìŒ ì €ì¥"""
        qa_data = {
            "question": question,
            "answer": answer,
            "analysis": analysis,
            "session_id": self.current_session.session_id if self.current_session else None
        }
        
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬
        today = datetime.now().strftime("%Y%m%d")
        daily_dir = self.answers_dir / today
        daily_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        filename = f"{question['id']}.json"
        with open(daily_dir / filename, 'w', encoding='utf-8') as f:
            json.dump(qa_data, f, indent=2, ensure_ascii=False)
            
    async def learning_cycle(self, duration_hours: int = 24):
        """í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        logger.info(f"Starting {duration_hours} hour learning cycle...")
        logger.info(f"Max memory limit: {self.max_memory_gb:.1f}GB")
        
        # ì„¸ì…˜ ì‹œì‘
        self.current_session = LearningSession(
            session_id=datetime.now().strftime("%Y%m%d_%H%M%S"),
            start_time=datetime.now(),
            topics_covered=[],
            questions_asked=0,
            successful_answers=0,
            models_used={model: 0 for model in self.available_models.keys()},
            knowledge_gained=[]
        )
        
        end_time = datetime.now() + timedelta(hours=duration_hours)
        cycle_count = 0
        model_rotation_count = 0
        
        while datetime.now() < end_time:
            cycle_count += 1
            logger.info(f"\n--- Learning Cycle {cycle_count} ---")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            current_memory = self.get_memory_usage()
            memory_percent = self.get_memory_usage_percent()
            logger.info(f"Memory: {current_memory:.1f}GB ({memory_percent:.1f}%)")
            
            # ë©”ëª¨ë¦¬ ì„ê³„ì¹˜ ì´ˆê³¼ì‹œ ëª¨ë¸ êµì²´ (ì €ì‚¬ì–‘ ìµœì í™”: 10ì‚¬ì´í´ë§ˆë‹¤)
            if not self.check_memory_safety() or (cycle_count % 10 == 0):
                self.unload_current_model()
                model_rotation_count += 1
                logger.info(f"Model rotation #{model_rotation_count}")
            
            # ëœë¤ ì£¼ì œ ì„ íƒ (5ê°€ì§€ í•µì‹¬ ì£¼ì œ ê°•í™”)
            topic = random.choice(self.learning_topics)
            
            # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ê°ì§€
            core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "Godot ì—”ì§„", "Godot ë„¤íŠ¸ì›Œí‚¹", "Nakama ì„œë²„"]
            is_core_topic = topic.category in core_categories
            
            # ì§ˆë¬¸ ìƒì„±
            question = self.generate_question(topic)
            
            # ì‹¤ì‹œê°„ UIìš© ì¶œë ¥
            print(f"Topic: {topic.topic} | Category: {topic.category} | Type: {question['type']}")
            if is_core_topic:
                print(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ ê°ì§€: {topic.category} - DeepSeek-coder ìµœìš°ì„  ì‚¬ìš©!")
                logger.info(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ: {topic.category} - {topic.topic}")
            else:
                logger.info(f"ğŸ“š ì¼ë°˜ ì£¼ì œ: {topic.category} - {topic.topic}")
            
            logger.info(f"Topic: {topic.topic} | Type: {question['type']} | Language: {question['language']}")
            logger.info(f"Question: {question['question'][:100]}...")
            
            # ëª¨ë¸ ì„ íƒ (ë©”ëª¨ë¦¬ ê³ ë ¤)
            model_name = self.select_model_for_question(question)
            if not model_name:
                logger.error("No models available")
                break
                
            logger.info(f"Selected model: {model_name}")
            
            # ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
            answer = await self.ask_model(model_name, question)
            
            # ë‹µë³€ ë¶„ì„
            analysis = self.analyze_answer(question, answer)
            
            # ì„¸ì…˜ ì—…ë°ì´íŠ¸
            self.current_session.questions_asked += 1
            if analysis.get("success", False):
                self.current_session.successful_answers += 1
                self.current_session.models_used[model_name] += 1
                if topic.topic not in self.current_session.topics_covered:
                    self.current_session.topics_covered.append(topic.topic)
                    
            # ê²°ê³¼ ì €ì¥
            self.save_qa_pair(question, answer, analysis)
            
            # ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            if cycle_count % 10 == 0:
                self._save_knowledge_base()
                
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ì €ì‚¬ì–‘ ìµœì í™”)
            if cycle_count % 5 == 0:
                self.save_memory_usage_log()
                gc.collect()  # ë” ë¹ˆë²ˆí•œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì‹¤ì‹œê°„ UIìš©)
            success_rate = (self.current_session.successful_answers / 
                          self.current_session.questions_asked * 100)
            print(f"Progress: {self.current_session.questions_asked} questions, {success_rate:.1f}% success, {model_rotation_count} rotations")
            logger.info(f"Progress: {self.current_session.questions_asked} questions, "
                       f"{success_rate:.1f}% success rate, {model_rotation_count} rotations")
            
            # ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ëŒ€ê¸° ì‹œê°„ ì¡°ì •
            if not self.check_memory_safety():
                wait_time = random.uniform(15, 30)  # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ë” ì˜¤ë˜ ëŒ€ê¸°
                logger.info(f"Memory high, waiting {wait_time:.1f}s...")
            else:
                wait_time = random.uniform(5, 15)  # ì •ìƒì‹œ ì§§ì€ ëŒ€ê¸°
                
            await asyncio.sleep(wait_time)
            
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        self.unload_current_model()
        
        # ì„¸ì…˜ ì¢…ë£Œ
        self.save_session_summary()
        
    def save_session_summary(self):
        """ì„¸ì…˜ ìš”ì•½ ì €ì¥"""
        if not self.current_session:
            return
            
        summary = {
            "session": asdict(self.current_session),
            "duration": str(datetime.now() - self.current_session.start_time),
            "knowledge_base_size": {
                "patterns": len(self.knowledge_base["csharp_patterns"]),
                "translations": len(self.knowledge_base["korean_translations"]),
                "errors": len(self.knowledge_base["common_errors"])
            }
        }
        
        summary_file = self.learning_dir / f"session_{self.current_session.session_id}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
            
        logger.info(f"\nSession Summary:")
        logger.info(f"Duration: {summary['duration']}")
        logger.info(f"Questions: {self.current_session.questions_asked}")
        logger.info(f"Success Rate: {self.current_session.successful_answers / max(1, self.current_session.questions_asked) * 100:.1f}%")
        logger.info(f"Topics Covered: {len(self.current_session.topics_covered)}")
        logger.info(f"Models Used: {self.current_session.models_used}")
        
    def generate_learning_report(self):
        """í•™ìŠµ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "generated_at": datetime.now().isoformat(),
            "total_sessions": 0,
            "total_questions": 0,
            "total_successful": 0,
            "topics_mastered": [],
            "korean_vocabulary": len(self.knowledge_base["korean_translations"]),
            "code_patterns": len(self.knowledge_base["csharp_patterns"]),
            "model_performance": {}
        }
        
        # ëª¨ë“  ì„¸ì…˜ ë¶„ì„
        for session_file in self.learning_dir.glob("session_*.json"):
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
                report["total_sessions"] += 1
                report["total_questions"] += session_data["session"]["questions_asked"]
                report["total_successful"] += session_data["session"]["successful_answers"]
                
        # ë³´ê³ ì„œ ì €ì¥
        report_file = self.learning_dir / f"learning_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        logger.info(f"\nLearning Report Generated: {report_file}")
        return report
        
    def save_memory_usage_log(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê·¸ ì €ì¥"""
        if not self.memory_usage_history:
            return
            
        log_file = self.learning_dir / "memory_usage.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.memory_usage_history, f, indent=2, ensure_ascii=False)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (ê¸°ë³¸ 32GB, ëª…ë ¹í–‰ì—ì„œ ë³€ê²½ ê°€ëŠ¥)
    max_memory = 32.0
    if len(sys.argv) > 2:
        try:
            max_memory = float(sys.argv[2])
            logger.info(f"ë©”ëª¨ë¦¬ ì œí•œì„ {max_memory}GBë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
        except ValueError:
            logger.warning("ì˜ëª»ëœ ë©”ëª¨ë¦¬ ì œí•œê°’. ê¸°ë³¸ê°’ 32GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    system = ContinuousLearningSystem(max_memory_gb=max_memory)
    
    if not system.available_models:
        logger.error("No models available. Please run install_llm_models.py first.")
        return
        
    # í•™ìŠµ ì‹œê°„ ì„¤ì • (ê¸°ë³¸ 24ì‹œê°„)
    duration = 24
    if len(sys.argv) > 1:
        try:
            duration = float(sys.argv[1])
        except ValueError:
            logger.error("Invalid duration. Using default 24 hours.")
            
    logger.info(f"Starting continuous learning for {duration} hours...")
    
    # DeepSeek-coder ìš°ì„  í™•ì¸ ë° ì•ˆë‚´
    deepseek_available = (
        "deepseek-coder-7b" in system.available_models and 
        system.available_models["deepseek-coder-7b"].get('status') == 'installed'
    )
    
    # ì‹¤ì‹œê°„ UIìš© DeepSeek-coder ìƒíƒœ ì¶œë ¥
    if deepseek_available:
        print("ğŸ”¥ DeepSeek-coder-v2 6.7B ëª¨ë¸ì„ 5ê°€ì§€ í•µì‹¬ ì£¼ì œì— ìµœìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤:")
        print("   1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° â†’ DeepSeek-coder íŠ¹í™”")
        print("   2ï¸âƒ£ í•œê¸€ ìš©ì–´ â†’ DeepSeek-coder ë²ˆì—­")
        print("   3ï¸âƒ£ Godot ì—”ì§„ â†’ DeepSeek-coder ì—”ì§„")
        print("   4ï¸âƒ£ Godot ë„¤íŠ¸ì›Œí‚¹ â†’ DeepSeek-coder ë„¤íŠ¸ì›Œí‚¹")
        print("   5ï¸âƒ£ Nakama ì„œë²„ â†’ DeepSeek-coder ì„œë²„")
        logger.info("ğŸ”¥ DeepSeek-coder-v2 6.7B ëª¨ë¸ì„ 5ê°€ì§€ í•µì‹¬ ì£¼ì œì— ìµœìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤")
    else:
        print("âš ï¸  DeepSeek-coderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ python download_deepseek_coder.pyë¡œ ì„¤ì¹˜í•˜ë©´ ë” ë‚˜ì€ í•™ìŠµ ê°€ëŠ¥")
        logger.warning("âš ï¸  DeepSeek-coderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ ì„¤ì¹˜í•˜ë©´ ë” ë‚˜ì€ í•™ìŠµ ê°€ëŠ¥")
    
    logger.info(f"Available models: {list(system.available_models.keys())}")
    logger.info(f"Memory limit: {max_memory}GB")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    initial_memory = system.get_memory_usage()
    print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}GB / {max_memory}GB")
    print(f"ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(system.available_models.keys())}")
    print(f"â° í•™ìŠµ ì‹œê°„: {duration}ì‹œê°„")
    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    logger.info(f"Initial memory usage: {initial_memory:.1f}GB")
    
    try:
        # í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰
        await system.learning_cycle(duration)
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        system.generate_learning_report()
        
    except KeyboardInterrupt:
        logger.info("\nLearning interrupted by user.")
        system.save_session_summary()
        system.generate_learning_report()
        
    except Exception as e:
        logger.error(f"Error during learning: {str(e)}")
        system.save_session_summary()

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ¯ AutoCI ì €ì‚¬ì–‘ ìµœì í™” AI í•™ìŠµ ì‹œìŠ¤í…œ")
    print("="*60)
    print("ğŸ’» RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ ìµœì í™”")
    print("ğŸš€ autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‹¤í–‰ë¨")
    print("ğŸ”¥ DeepSeek-coder-v2 6.7B ìµœìš°ì„  ì‚¬ìš©")
    print("ğŸ“š 5ê°€ì§€ í•µì‹¬ ì£¼ì œ: C#, í•œê¸€, Godot ì—”ì§„, Godot ë„¤íŠ¸ì›Œí‚¹, Nakama ì„œë²„")
    print("="*60 + "\n")
    
    # ì§ì ‘ ì‹¤í–‰
    asyncio.run(main())