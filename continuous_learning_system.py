#!/usr/bin/env python3
"""
AutoCI 24ì‹œê°„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ - ì €ì‚¬ì–‘ ìµœì í™” ë²„ì „
C#ê³¼ í•œê¸€ì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
Llama-3.1-8B, CodeLlama-13B, Qwen2.5-Coder-32B ëª¨ë¸ì„ í™œìš©

RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ì— ìµœì í™”ë¨
autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import random
import asyncio
import logging
import gc
import psutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)

# ì •ë³´ ìˆ˜ì§‘ê¸° ì„í¬íŠ¸
try:
    from modules.intelligent_information_gatherer import get_information_gatherer
    INFORMATION_GATHERER_AVAILABLE = True
    print("ğŸŒ ì§€ëŠ¥í˜• ì •ë³´ ìˆ˜ì§‘ê¸° í™œì„±í™”!")
except ImportError:
    INFORMATION_GATHERER_AVAILABLE = False
    print("âš ï¸ ì •ë³´ ìˆ˜ì§‘ê¸°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from modules.ai_model_controller import AIModelController
    MODEL_CONTROLLER_AVAILABLE = True
    print("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ í™œì„±í™”!")
except ImportError:
    MODEL_CONTROLLER_AVAILABLE = False
    print("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('continuous_learning.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Hugging Face í† í°
HF_TOKEN = "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"

@dataclass
class LearningTopic:
    """í•™ìŠµ ì£¼ì œ"""
    id: str
    category: str
    topic: str
    difficulty: int  # 1-5
    korean_keywords: List[str]
    csharp_concepts: List[str]
    godot_integration: Optional[str] = None
    
@dataclass
class LearningSession:
    """í•™ìŠµ ì„¸ì…˜"""
    session_id: str
    start_time: datetime
    topics_covered: List[str]
    questions_asked: int
    successful_answers: int
    models_used: Dict[str, int]
    knowledge_gained: List[Dict[str, Any]]
    
class ContinuousLearningSystem:
    def __init__(self, models_dir: str = "./models", learning_dir: str = "./continuous_learning", max_memory_gb: float = 32.0):
        self.models_dir = Path(models_dir)
        self.learning_dir = Path(learning_dir)
        self.learning_dir.mkdir(exist_ok=True)
        
        # ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if MODEL_CONTROLLER_AVAILABLE:
            self.model_controller = AIModelController()
            print("ğŸ¯ AI ëª¨ë¸ ì¡°ì¢…ê¶Œ í™•ë³´ ì™„ë£Œ!")
            logger.info("ğŸ® AI ëª¨ë¸ ì™„ì „ ì œì–´ ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            self.model_controller = None
            logger.warning("âš ï¸ AI ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì • (ì €ì‚¬ì–‘ ìµœì í™”)
        self.max_memory_gb = max_memory_gb
        self.memory_threshold = 0.70  # 70% ì‚¬ìš© ì‹œ ëª¨ë¸ ì–¸ë¡œë“œ (ë” ë³´ìˆ˜ì )
        self.currently_loaded_model = None
        self.model_cache = {}  # ì–¸ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì €ì¥
        
        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        self.questions_dir = self.learning_dir / "questions"
        self.answers_dir = self.learning_dir / "answers"
        self.knowledge_base_dir = self.learning_dir / "knowledge_base"
        self.progress_dir = self.learning_dir / "progress"
        
        for dir in [self.questions_dir, self.answers_dir, self.knowledge_base_dir, self.progress_dir]:
            dir.mkdir(exist_ok=True)
            
        # ëª¨ë¸ ì •ë³´ (ì‹¤ì œ ë¡œë”©ì€ í•„ìš”í•  ë•Œë§Œ)
        self.available_models = {}
        self.load_model_info()
        
        # í•™ìŠµ ì£¼ì œ ì •ì˜
        self.learning_topics = self._initialize_learning_topics()
        
        # í˜„ì¬ ì„¸ì…˜
        self.current_session = None
        
        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_base = self._load_knowledge_base()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        self.memory_usage_history = []
        
        # í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ
        self.learning_progress = self._load_learning_progress()
        
        # ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì
        try:
            from modules.progressive_learning_manager import ProgressiveLearningManager
            self.progressive_manager = ProgressiveLearningManager(self.learning_dir)
            logger.info("ğŸ“ˆ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ì í™œì„±í™”")
        except:
            self.progressive_manager = None
            logger.warning("âš ï¸ ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
    def _initialize_learning_topics(self) -> List[LearningTopic]:
        """5ê°€ì§€ í•µì‹¬ í•™ìŠµ ì£¼ì œ ì´ˆê¸°í™” (DeepSeek-coder ìµœì í™”)"""
        topics = [
            # ... (ê¸°ì¡´ ì£¼ì œë“¤)
            # 6ï¸âƒ£ Godot ì „ë¬¸ê°€ í•™ìŠµ (ë¬¸ì„œ ê¸°ë°˜)
            LearningTopic("godot_expert_nodes", "Godot ì „ë¬¸ê°€", "ë…¸ë“œì™€ ì”¬ ì‹¬ì¸µ ë¶„ì„", 5,
                         ["ë…¸ë“œ", "ì”¬", "íŠ¸ë¦¬", "ìƒì†", "ì¸ìŠ¤í„´ìŠ¤"],
                         ["Node", "Scene", "SceneTree", "inheritance", "instance"],
                         "Godot í•µì‹¬ ì•„í‚¤í…ì²˜"),
            LearningTopic("godot_expert_scripting", "Godot ì „ë¬¸ê°€", "ê³ ê¸‰ ìŠ¤í¬ë¦½íŒ… ê¸°ìˆ ", 5,
                         ["GDScript", "C#", "ì‹œê·¸ë„", "ì½”ë£¨í‹´", "íˆ´ ìŠ¤í¬ë¦½íŠ¸"],
                         ["GDScript", "C#", "Signal", "Coroutine", "Tool Script"],
                         "íš¨ìœ¨ì ì¸ ê²Œì„ ë¡œì§ êµ¬í˜„"),
        ]
        
        # ëª¨ë“  ì£¼ì œì— DeepSeek-coder ìš°ì„  íƒœê·¸ ì¶”ê°€
        for topic in topics:
            topic.godot_integration = f"[DeepSeek ìš°ì„ ] {topic.godot_integration}"
            
        return topics
        
    def _load_knowledge_base(self) -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        if kb_file.exists():
            with open(kb_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "csharp_patterns": {},
            "korean_translations": {},
            "godot_integrations": {},
            "common_errors": {},
            "best_practices": {}
        }
        
    def _save_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        kb_file = self.knowledge_base_dir / "knowledge_base.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, indent=2, ensure_ascii=False)
            
    def _load_learning_progress(self) -> Dict[str, Any]:
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        progress_file = self.progress_dir / "learning_progress.json"
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
                logger.info(f"ê¸°ì¡´ í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {progress.get('total_hours', 0)}ì‹œê°„")
                return progress
        return {
            "total_hours": 0,
            "total_questions": 0,
            "total_successful": 0,
            "topics_progress": {},
            "last_session_id": None,
            "last_save_time": None,
            "sessions_completed": []
        }
        
    def _save_learning_progress(self):
        """í•™ìŠµ ì§„í–‰ ìƒíƒœ ì €ì¥"""
        if self.current_session:
            # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì—…ë°ì´íŠ¸
            session_duration = (datetime.now() - self.current_session.start_time).total_seconds() / 3600
            
            self.learning_progress["total_hours"] += session_duration
            self.learning_progress["total_questions"] += self.current_session.questions_asked
            self.learning_progress["total_successful"] += self.current_session.successful_answers
            self.learning_progress["last_session_id"] = self.current_session.session_id
            self.learning_progress["last_save_time"] = datetime.now().isoformat()
            
            # ì£¼ì œë³„ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            for topic in self.current_session.topics_covered:
                if topic not in self.learning_progress["topics_progress"]:
                    self.learning_progress["topics_progress"][topic] = {
                        "questions_asked": 0,
                        "successful_answers": 0,
                        "last_studied": None
                    }
                self.learning_progress["topics_progress"][topic]["questions_asked"] += 1
                self.learning_progress["topics_progress"][topic]["last_studied"] = datetime.now().isoformat()
        
        progress_file = self.progress_dir / "learning_progress.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.learning_progress, f, indent=2, ensure_ascii=False)
        logger.info(f"í•™ìŠµ ì§„í–‰ ìƒíƒœë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ì´ í•™ìŠµ ì‹œê°„: {self.learning_progress['total_hours']:.1f}ì‹œê°„")
            
    def load_model_info(self):
        """ëª¨ë¸ ì •ë³´ë§Œ ë¡œë“œ (ì‹¤ì œ ëª¨ë¸ì€ í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)"""
        models_info_file = self.models_dir / "installed_models.json"
        if not models_info_file.exists():
            logger.error("ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. install_llm_models.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        with open(models_info_file, 'r', encoding='utf-8') as f:
            installed_models = json.load(f)
            
        self.available_models = installed_models
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.available_models.keys())}")
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB)"""
        return psutil.virtual_memory().used / (1024**3)
        
    def get_memory_usage_percent(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜ (%)"""
        return psutil.virtual_memory().percent
        
    def check_memory_safety(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì•ˆì „í•œì§€ í™•ì¸"""
        current_usage = self.get_memory_usage()
        usage_percent = self.get_memory_usage_percent()
        
        # í˜„ì¬ ì‚¬ìš©ëŸ‰ì´ ìµœëŒ€ í—ˆìš©ëŸ‰ì˜ 85%ë¥¼ ë„˜ìœ¼ë©´ ìœ„í—˜
        return current_usage < (self.max_memory_gb * self.memory_threshold)
        
    def unload_current_model(self):
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.currently_loaded_model:
            logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ {self.currently_loaded_model} ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            self.currently_loaded_model = None
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            
    def load_model(self, model_name: str) -> bool:
        """í•„ìš” ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if model_name == self.currently_loaded_model:
            return True  # ì´ë¯¸ ë¡œë“œë¨
            
        if model_name not in self.available_models:
            logger.error(f"ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False
            
        # ë©”ëª¨ë¦¬ í™•ì¸ í›„ í•„ìš”ì‹œ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
        if not self.check_memory_safety() and self.currently_loaded_model:
            self.unload_current_model()
            
        try:
            print(f"ğŸ”„ {model_name} ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            logger.info(f"{model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            info = self.available_models[model_name]
            model_id = info['model_id']
            print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_id}")
            
            # RTX 2080 8GB ìµœì í™” ëª¨ë¸ë§Œ í—ˆìš©
            rtx_2080_optimized = {
                "bitnet-b1.58-2b": {"max_vram": 1, "device": "cpu"},
                "gemma-4b": {"max_vram": 4, "device": "cuda:0"},
                "phi3-mini": {"max_vram": 6, "device": "cuda:0"},
                "deepseek-coder-7b": {"max_vram": 6, "device": "cuda:0"},
                "mistral-7b": {"max_vram": 7, "device": "cuda:0"}
            }
            
            if model_name not in rtx_2080_optimized:
                logger.warning(f"âŒ {model_name}ì€ RTX 2080 8GBì— ìµœì í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {', '.join(rtx_2080_optimized.keys())}")
                logger.info("ğŸ’¡ install_llm_models_rtx2080.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì í™”ëœ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
                return False
            
            model_config = rtx_2080_optimized[model_name]
            logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {model_name} (VRAM: {model_config['max_vram']}GB)")
            
            # AutoTokenizerì™€ AutoModelForCausalLMì„ ì§ì ‘ ì‚¬ìš©
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            hf_token = os.getenv('HF_TOKEN', None)
            
            # ëª¨ë¸ë³„ ìµœì  ì„¤ì •
            device_map = model_config['device']
            torch_dtype = torch.float32 if device_map == "cpu" else torch.float16
            quantization_config = None
            
            # 4bit ì–‘ìí™” ì„¤ì • (GPU ëª¨ë¸ìš©)
            if device_map != "cpu" and info.get('quantization') == '4bit':
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ìš© í† í¬ë‚˜ì´ì € ê²½ë¡œ ì²˜ë¦¬
            tokenizer_path = model_id
            if model_name in ["deepseek-coder-7b", "llama-3.1-8b"]:
                # ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ì€ í† í¬ë‚˜ì´ì € í´ë” ì‚¬ìš©
                tokenizer_path = info.get('tokenizer_path', model_id)
                if not tokenizer_path.startswith('./'):
                    tokenizer_path = f"./{tokenizer_path}"
                print(f"ğŸ“ í† í¬ë‚˜ì´ì € ê²½ë¡œ: {tokenizer_path}")
            
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path, 
                token=hf_token,
                trust_remote_code=True,
                local_files_only=model_id.startswith('./models/')  # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_kwargs = {
                "torch_dtype": torch_dtype,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            if device_map == "cpu":
                model_kwargs["device_map"] = "cpu"
            else:
                model_kwargs["device_map"] = {"": 0}  # GPU 0 ì‚¬ìš©
                if quantization_config:
                    model_kwargs["quantization_config"] = quantization_config
            
            if hf_token and not model_id.startswith('./models/'):
                model_kwargs["token"] = hf_token
            
            # ë¡œì»¬ ëª¨ë¸ì€ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ë¡œë“œ
            if model_id.startswith('./models/'):
                model_kwargs["local_files_only"] = True
                print(f"ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            else:
                print(f"ğŸ”„ Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_id}")
            
            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ë˜í¼
            def optimized_generate(prompt):
                try:
                    inputs = tokenizer(
                        prompt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=1024,
                        padding=True
                    )
                    
                    if device_map != "cpu":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs.input_ids,
                            attention_mask=inputs.attention_mask,
                            max_new_tokens=150,  # RTX 2080 ìµœì í™”
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    response = tokenizer.decode(
                        outputs[0][inputs.input_ids.shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    return [{"generated_text": response}]
                    
                except Exception as e:
                    logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    return [{"generated_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}]
            
            pipe = optimized_generate
            
            self.model_cache[model_name] = {
                "pipeline": pipe,
                "features": info['features'],
                "info": info
            }
            
            self.currently_loaded_model = model_name
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            memory_usage = self.get_memory_usage()
            self.memory_usage_history.append({
                "timestamp": datetime.now().isoformat(),
                "model": model_name,
                "memory_gb": memory_usage,
                "memory_percent": self.get_memory_usage_percent()
            })
            
            logger.info(f"âœ“ {model_name} ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬: {memory_usage:.1f}GB)")
            return True
            
        except Exception as e:
            logger.error(f"âœ— {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
                
    def _get_quantization_config(self, quantization: str):
        """ì–‘ìí™” ì„¤ì • ë°˜í™˜ (RTX 2080 8GB ìµœì í™”)"""
        if quantization == "4bit":
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True
            )
        elif quantization == "8bit":
            return BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.bfloat16,
                llm_int8_enable_fp32_cpu_offload=True,  # RTX 2080 8GB í•„ìˆ˜
                llm_int8_has_fp16_weight=False  # ë©”ëª¨ë¦¬ ì ˆì•½
            )
        return None
        
    def generate_question(self, topic: LearningTopic) -> Dict[str, Any]:
        """í•™ìŠµ ì§ˆë¬¸ ìƒì„±"""
        question_types = [
            "explain",      # ê°œë… ì„¤ëª…
            "example",      # ì˜ˆì œ ì½”ë“œ
            "translate",    # í•œê¸€-ì˜ì–´ ë²ˆì—­
            "error",        # ì˜¤ë¥˜ ìˆ˜ì •
            "optimize",     # ìµœì í™”
            "integrate"     # Godot í†µí•©
        ]
        
        # "Godot ì „ë¬¸ê°€" ì£¼ì œì¸ ê²½ìš°, ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ìƒì„±
        if topic.category == "Godot ì „ë¬¸ê°€":
            try:
                with open("collected_godot_docs.json", "r", encoding="utf-8") as f:
                    docs_data = json.load(f)
                
                if docs_data:
                    doc = random.choice(docs_data)
                    question_text = f"{doc['title']}ì— ëŒ€í•´ ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: \n\n{doc['content'][:500]}"
                    return {
                        "id": f"{topic.id}_from_doc_{int(time.time())}",
                        "topic": topic.topic,
                        "type": "doc_based_qna",
                        "language": "korean",
                        "difficulty": topic.difficulty,
                        "question": question_text,
                        "keywords": topic.korean_keywords
                    }
            except FileNotFoundError:
                pass # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ë„˜ì–´ê°

        question_type = random.choice(question_types)
        
        # ì§ˆë¬¸ í…œí”Œë¦¿
        templates = {
            "explain": {
                "korean": f"{topic.topic}ì— ëŒ€í•´ í•œê¸€ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. íŠ¹íˆ {random.choice(topic.korean_keywords)}ì— ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.",
                "english": f"Explain {topic.topic} in C# with focus on {random.choice(topic.csharp_concepts)}."
            },
            "example": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•˜ëŠ” C# ì½”ë“œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê³  í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"Write a C# code example demonstrating {topic.topic} with comments."
            },
            "translate": {
                "korean": f"ë‹¤ìŒ C# ê°œë…ì„ í•œê¸€ë¡œ ë²ˆì—­í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”: {random.choice(topic.csharp_concepts)}",
                "english": f"Translate and explain this Korean term in C# context: {random.choice(topic.korean_keywords)}"
            },
            "error": {
                "korean": f"{topic.topic} ê´€ë ¨ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"What are common errors with {topic.topic} in C# and how to fix them?"
            },
            "optimize": {
                "korean": f"{topic.topic}ì„ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì„ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to optimize performance when using {topic.topic} in C#?"
            },
            "integrate": {
                "korean": f"Godotì—ì„œ {topic.topic}ì„ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ C# ì½”ë“œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "english": f"How to use {topic.topic} in Godot with C#? Provide examples."
            }
        }
        
        # ì–¸ì–´ ì„ íƒ (í•œê¸€ í•™ìŠµ ê°•ì¡°)
        language = "korean" if random.random() < 0.7 else "english"
        question_text = templates[question_type][language]
        
        return {
            "id": f"{topic.id}_{question_type}_{int(time.time())}",
            "topic": topic.topic,
            "type": question_type,
            "language": language,
            "difficulty": topic.difficulty,
            "question": question_text,
            "keywords": topic.korean_keywords if language == "korean" else topic.csharp_concepts
        }
        
    def select_model_for_question(self, question: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ (5ëŒ€ í•µì‹¬ ì£¼ì œ DeepSeek-coder ìµœì í™”)"""
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ ê³ ë ¤
        rtx_2080_models = {
            "deepseek-coder-7b": {"priority": 10, "specialties": ["code", "csharp", "godot", "korean", "nakama"], "vram": 6},
            "phi3-mini": {"priority": 8, "specialties": ["reasoning", "math", "csharp"], "vram": 6},
            "llama-3.1-8b": {"priority": 7, "specialties": ["general", "korean", "csharp"], "vram": 7},
            "gemma-4b": {"priority": 6, "specialties": ["general", "korean"], "vram": 4},
            "mistral-7b": {"priority": 4, "specialties": ["general", "fast"], "vram": 7}
        }
        
        # RTX 2080 ìµœì í™” ëª¨ë¸ë§Œ í•„í„°ë§
        available_optimized = []
        for model_name in self.available_models:
            if (model_name in rtx_2080_models and 
                self.available_models[model_name].get('rtx_2080_optimized', False)):
                available_optimized.append(model_name)
        
        if not available_optimized:
            logger.warning("âŒ RTX 2080 ìµœì í™” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ DeepSeek-coder 6.7Bë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return None
            
        # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¹´í…Œê³ ë¦¬ í™•ì¸
        core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "Godot ì—”ì§„", "Godot ë„¤íŠ¸ì›Œí‚¹", "Nakama ì„œë²„"]
        topic_category = question.get("category", "")
        is_core_topic = topic_category in core_categories
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ RTX 2080 ìµœì í™”ì´ê³  ì í•©í•˜ë©´ ê³„ì† ì‚¬ìš©
        if (self.currently_loaded_model and 
            self.currently_loaded_model in available_optimized and
            self._is_model_suitable(self.currently_loaded_model, question)):
            return self.currently_loaded_model
            
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„ (5ê°€ì§€ í•µì‹¬ ì£¼ì œ í¬í•¨)
        question_features = set()
        question_text = question.get("question", "").lower()
        topic_text = question.get("topic", "").lower()
        
        # 1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['code', 'programming', 'script', 'function', 'class', 'method', 'c#', 'csharp']):
            question_features.add('csharp')
            question_features.add('code')
        
        # 2ï¸âƒ£ í•œê¸€ ìš©ì–´ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['korean', 'í•œê¸€', 'í•œêµ­ì–´', 'ë²ˆì—­', 'ìš©ì–´', 'ê°œë…']):
            question_features.add('korean')
        
        # 3ï¸âƒ£ Godot ì—”ì§„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['godot', 'engine', 'ì—”ì§„', 'ë…¸ë“œ', 'ì”¬', 'ì•„í‚¤í…ì²˜']):
            question_features.add('godot')
        
        # 4ï¸âƒ£ Godot ë„¤íŠ¸ì›Œí‚¹ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['multiplayer', 'network', 'ë„¤íŠ¸ì›Œí‚¹', 'rpc', 'ë™ê¸°í™”', 'AIì œì–´']):
            question_features.add('networking')
        
        # 5ï¸âƒ£ Nakama ì„œë²„ íŠ¹ì„±
        if any(word in question_text + topic_text for word in ['nakama', 'server', 'ì„œë²„', 'ë§¤ì¹˜ë©”ì´í‚¹', 'backend']):
            question_features.add('nakama')
        
        # ê¸°íƒ€ íŠ¹ì„±
        if any(word in question_text for word in ['reasoning', 'math', 'ìˆ˜í•™', 'ì¶”ë¡ ', 'ë…¼ë¦¬']):
            question_features.add('reasoning')
        
        # ëª¨ë¸ë³„ ì ìˆ˜ ê³„ì‚°
        model_scores = {}
        for model_name in available_optimized:
            if model_name not in rtx_2080_models:
                continue
                
            model_config = rtx_2080_models[model_name]
            score = model_config['priority']
            
            # ğŸ”¥ í•µì‹¬ ì£¼ì œì—ì„œ DeepSeek-coder ê°•ë ¥ ìš°ì„ ìˆœìœ„
            if model_name == "deepseek-coder-7b":
                # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¼ë©´ ë¬´ì¡°ê±´ DeepSeek-coder ì„ íƒ
                if is_core_topic:
                    score += 50  # í•µì‹¬ ì£¼ì œ íŠ¹ëŒ€ ë³´ë„ˆìŠ¤
                    logger.info(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ '{topic_category}' ê°ì§€! DeepSeek-coder ìµœìš°ì„  ì„ íƒ")
                
                # ì„¸ë¶€ íŠ¹ì„±ë³„ ì¶”ê°€ ë³´ë„ˆìŠ¤
                if 'code' in question_features or 'csharp' in question_features:
                    score += 25  # C# ì½”ë”© íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'korean' in question_features:
                    score += 20  # í•œê¸€ ë²ˆì—­ íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'godot' in question_features:
                    score += 20  # Godot íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'networking' in question_features:
                    score += 15  # ë„¤íŠ¸ì›Œí‚¹ íŠ¹í™” ë³´ë„ˆìŠ¤
                if 'nakama' in question_features:
                    score += 15  # ì„œë²„ íŠ¹í™” ë³´ë„ˆìŠ¤
                
                # ì¼ë°˜ ì§ˆë¬¸ì—ë„ ê¸°ë³¸ ë³´ë„ˆìŠ¤
                score += 10
            
            # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ íŠ¹ê¸° ë¶„ì•¼
            elif model_name == "phi3-mini" and 'reasoning' in question_features:
                score += 15
            elif model_name == "gemma-4b" and 'korean' in question_features and not is_core_topic:
                score += 12  # í•µì‹¬ ì£¼ì œê°€ ì•„ë‹ ë•Œë§Œ
            elif model_name == "llama-3.1-8b" and 'korean' in question_features and not is_core_topic:
                score += 10  # í•µì‹¬ ì£¼ì œê°€ ì•„ë‹ ë•Œë§Œ
            
            # íŠ¹ê¸° ë¶„ì•¼ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            for specialty in model_config['specialties']:
                if specialty in question_features:
                    if model_name == "deepseek-coder-7b":
                        score += 12  # DeepSeek-coderì— ë” ë†’ì€ ë³´ë„ˆìŠ¤
                    else:
                        score += 6
            
            # VRAM íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤ (ë‚®ì€ VRAM ì‚¬ìš©ëŸ‰ ì„ í˜¸)
            score += (8 - model_config['vram'])
            
            model_scores[model_name] = score
        
        # ìµœê³  ì ìˆ˜ ëª¨ë¸ ì„ íƒ
        if model_scores:
            best_model = max(model_scores.items(), key=lambda x: x[1])[0]
            selected_score = model_scores[best_model]
            
            # ë¡œê·¸ ì¶œë ¥ (ì‹¤ì‹œê°„ UIìš©)
            if best_model == "deepseek-coder-7b" and is_core_topic:
                logger.info(f"ğŸ¯ í•µì‹¬ ì£¼ì œ ìµœì í™”: {best_model} ì„ íƒ (ì ìˆ˜: {selected_score}) - {topic_category}")
                print(f"Selected model: {best_model} (ğŸ”¥ DeepSeek-coder í•µì‹¬ ì£¼ì œ)")
            else:
                logger.info(f"ğŸ¯ RTX 2080 ìµœì í™”: {best_model} ì„ íƒ (ì ìˆ˜: {selected_score})")
                print(f"Selected model: {best_model}")
            
            return best_model
        
        # ê¸°ë³¸ ìš°ì„ ìˆœìœ„: DeepSeek > Phi3 > Llama > Gemma > Mistral
        for fallback in ["deepseek-coder-7b", "phi3-mini", "llama-3.1-8b", "gemma-4b", "mistral-7b"]:
            if fallback in available_optimized:
                logger.info(f"ğŸ¯ RTX 2080 ê¸°ë³¸ ëª¨ë¸: {fallback}")
                return fallback
        
        return None
        
    def _is_model_suitable(self, model_name: str, question: Dict[str, Any]) -> bool:
        """ëª¨ë¸ì´ ì§ˆë¬¸ì— ì í•©í•œì§€ í™•ì¸"""
        if model_name not in self.available_models:
            return False
            
        model_features = self.available_models[model_name]['features']
        
        # í•œê¸€ ì§ˆë¬¸ì¸ ê²½ìš°
        if "korean" in question["language"]:
            return "korean" in model_features
            
        # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°
        if question["type"] in ["example", "error", "optimize"]:
            return "code" in model_features or "csharp" in model_features
            
        return True  # ê¸°ë³¸ì ìœ¼ë¡œ ì í•©í•˜ë‹¤ê³  ê°€ì •
        
    async def ask_model(self, model_name: str, question: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ® ëª¨ë¸ì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸° - ì™„ì „ ì œì–´ ëª¨ë“œ"""
        # ëª¨ë¸ ë¡œë“œ í™•ì¸ ë° í•„ìš”ì‹œ ë¡œë“œ
        if not self.load_model(model_name):
            return {"error": f"Model {model_name} failed to load"}
            
        if model_name not in self.model_cache:
            return {"error": f"Model {model_name} not in cache"}
        
        # ğŸ¯ AI ëª¨ë¸ ì™„ì „ ì œì–´: ì—¬ëŸ¬ ë²ˆ ì‹œë„í•˜ì—¬ ìš°ë¦¬ ê¸°ì¤€ì— ë§ëŠ” ë‹µë³€ í™•ë³´
        max_attempts = 3 if self.model_controller else 1
        attempt = 0
        
        while attempt < max_attempts:
            attempt += 1
            
            try:
                model_pipeline = self.model_cache[model_name]["pipeline"]
                
                # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                memory_before = self.get_memory_usage()
                if not self.check_memory_safety():
                    logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_before:.1f}GB")
                
                # ğŸ® ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ (ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ìˆìœ¼ë©´ ë” ì„¸ë°€í•˜ê²Œ)
                if self.model_controller:
                    custom_prompt = self.model_controller.get_custom_prompt(model_name, question.get('type', 'general'))
                    system_prompt = custom_prompt
                else:
                    system_prompt = f"""C# programming and Godot expert. Answer about {question['topic']} in Korean with examples."""
                
                full_prompt = f"{system_prompt}\n\nQ: {question['question']}\nA:"
                
                # ëª¨ë¸ í˜¸ì¶œ (ì—ëŸ¬ ì¶”ì  ê°•í™”)
                start_time = time.time()
                
                # ì…ë ¥ íƒ€ì… í™•ì¸
                if not isinstance(full_prompt, str):
                    raise ValueError(f"í”„ë¡¬í”„íŠ¸ê°€ ë¬¸ìì—´ì´ ì•„ë‹˜: {type(full_prompt)}")
                
                logger.debug(f"ì‹œë„ {attempt}: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ {len(full_prompt)} ë¬¸ì")
                
                # AI ì‘ë‹µ ìƒì„± ì‹œì‘ ì•Œë¦¼
                print(f"ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘... (ëª¨ë¸: {model_name})")
                logger.info(f"AI ì‘ë‹µ ìƒì„± ì‹œì‘: {model_name}")
                
                response = model_pipeline(full_prompt)  # optimized_generateëŠ” promptë§Œ ë°›ìŒ
                
                # ì‘ë‹µ ì™„ë£Œ í™•ì¸
                print(f"âœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ)")
                
                if not response or len(response) == 0:
                    raise ValueError("ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í•¨")
                
                if not isinstance(response[0], dict) or 'generated_text' not in response[0]:
                    raise ValueError(f"ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹: {type(response[0])}")
                
                answer_text = response[0]['generated_text'].strip()
                
                if not answer_text or len(answer_text) < 10:
                    raise ValueError("ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
                
                response_time = time.time() - start_time
                
                # ğŸ¯ í’ˆì§ˆ í‰ê°€ (ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ìˆìœ¼ë©´)
                if self.model_controller:
                    quality = self.model_controller.evaluate_response_quality(question, answer_text, model_name)
                    self.model_controller.log_response_quality(question, answer_text, quality, model_name)
                    
                    if quality.is_acceptable:
                        print(f"ğŸ¯ í’ˆì§ˆ í†µê³¼ (ì‹œë„ {attempt}): {quality.score:.2f}")
                        logger.info(f"âœ… í’ˆì§ˆ ê¸°ì¤€ í†µê³¼: {model_name} (ì ìˆ˜: {quality.score:.2f})")
                    else:
                        print(f"âŒ í’ˆì§ˆ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {quality.score:.2f} - {', '.join(quality.issues)}")
                        
                        # ì¬ì‹œë„ ê²°ì •
                        if self.model_controller.should_retry(quality, model_name, attempt):
                            logger.warning(f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬, ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt}/{max_attempts})")
                            continue
                        else:
                            logger.warning(f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬, í˜„ì¬ ë‹µë³€ ì‚¬ìš©")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
                memory_after = self.get_memory_usage()
                memory_delta = memory_after - memory_before
                
                final_answer = {
                    "model": model_name,
                    "question_id": question["id"],
                    "answer": answer_text,
                    "response_time": response_time,
                    "timestamp": datetime.now().isoformat(),
                    "attempt": attempt,
                    "memory_usage": {
                        "before_gb": memory_before,
                        "after_gb": memory_after,
                        "delta_gb": memory_delta
                    }
                }
                
                # í’ˆì§ˆ ì •ë³´ ì¶”ê°€
                if self.model_controller:
                    final_answer["quality"] = {
                        "score": quality.score,
                        "is_acceptable": quality.is_acceptable,
                        "issues": quality.issues
                    }
                
                return final_answer
                
            except Exception as model_error:
                logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(model_error)}")
                if attempt >= max_attempts:
                    logger.error(f"ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {str(model_error)}")
                    # ğŸ” ì—ëŸ¬ëŠ” error í•„ë“œì—ë§Œ ë„£ê³  answer í•„ë“œëŠ” ë„£ì§€ ì•Šê¸°
                    return {"error": str(model_error), "model": model_name, "attempts": attempt}
                else:
                    # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    await asyncio.sleep(1.0)
                    continue
        
        # ğŸ” ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œì—ë„ error í•„ë“œì—ë§Œ ë„£ê¸°
        return {"error": "ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼", "model": model_name, "attempts": max_attempts}
            
    def analyze_answer(self, question: Dict[str, Any], answer: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹µë³€ ë¶„ì„ ë° ì§€ì‹ ì¶”ì¶œ - ì—„ê²©í•œ ì„±ê³µ/ì‹¤íŒ¨ íŒë‹¨"""
        # ğŸ” 1ì°¨: error í•„ë“œê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì‹¤íŒ¨
        if "error" in answer:
            error_msg = answer["error"]
            logger.error(f"âŒ ë‹µë³€ ì‹¤íŒ¨ (error í•„ë“œ): {error_msg}")
            return {"success": False, "error": error_msg, "model": answer.get("model", "unknown")}
        
        # ğŸ” 2ì°¨: ë‹µë³€ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
        answer_text = answer.get("answer", "")
        if not answer_text or len(answer_text.strip()) < 5:
            logger.error(f"âŒ ë‹µë³€ ì‹¤íŒ¨ (ë¹„ì–´ìˆìŒ): '{answer_text}'")
            return {"success": False, "error": "ë‹µë³€ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ"}
        
        # ğŸ” 3ì°¨: ì‹¤íŒ¨ íŒ¨í„´ ì²´í¬ (ê°•í™”ëœ íŒ¨í„´)
        failure_patterns = [
            "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", 
            "ìƒì„±ì— ì‹¤íŒ¨",
            "ì£„ì†¡í•©ë‹ˆë‹¤",
            "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤",
            "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨"
        ]
        
        answer_lower = answer_text.lower()
        for pattern in failure_patterns:
            if pattern.lower() in answer_lower:
                logger.error(f"âŒ ë‹µë³€ ì‹¤íŒ¨ (ì‹¤íŒ¨ íŒ¨í„´): '{pattern}' ê°ì§€")
                return {"success": False, "error": f"ì‹¤íŒ¨ íŒ¨í„´ ê°ì§€: {pattern}"}
        
        # ğŸ” 4ì°¨: ìµœì†Œ í’ˆì§ˆ ì²´í¬
        if len(answer_text.strip()) < 20:
            logger.error(f"âŒ ë‹µë³€ ì‹¤íŒ¨ (ë„ˆë¬´ ì§§ìŒ): ê¸¸ì´ {len(answer_text)}")
            return {"success": False, "error": f"ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(answer_text)})"}
        
        # âœ… ì—¬ê¸°ê¹Œì§€ í†µê³¼í•˜ë©´ ì„±ê³µ
        logger.info(f"âœ… ë‹µë³€ ì„±ê³µ (ê¸¸ì´: {len(answer_text)})")
            
        analysis = {
            "success": True,
            "quality_score": 0,
            "extracted_knowledge": {},
            "new_patterns": [],
            "improvements": []
        }
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        quality_factors = {
            "length": len(answer_text) > 100,
            "has_code": "```" in answer_text or "class" in answer_text or "public" in answer_text,
            "has_korean": any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in answer_text),
            "has_explanation": any(word in answer_text.lower() for word in ["because", "therefore", "ì´ìœ ", "ë•Œë¬¸", "ë”°ë¼ì„œ"]),
            "has_example": any(word in answer_text.lower() for word in ["example", "ì˜ˆì œ", "ì˜ˆì‹œ", "ë‹¤ìŒ"])
        }
        
        analysis["quality_score"] = sum(1 for factor in quality_factors.values() if factor) / len(quality_factors)
        
        # ì§€ì‹ ì¶”ì¶œ
        if question["type"] == "translate" and quality_factors["has_korean"]:
            # í•œê¸€ ë²ˆì—­ ì €ì¥
            for keyword in question["keywords"]:
                if keyword in answer_text:
                    self.knowledge_base["korean_translations"][keyword] = answer_text[:200]
                    
        elif question["type"] == "example" and quality_factors["has_code"]:
            # ì½”ë“œ íŒ¨í„´ ì €ì¥
            code_pattern = {
                "topic": question["topic"],
                "code": answer_text,
                "language": question["language"]
            }
            self.knowledge_base["csharp_patterns"][question["topic"]] = code_pattern
            
        elif question["type"] == "error":
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ ì €ì¥
            self.knowledge_base["common_errors"][question["topic"]] = answer_text[:300]
            
        # ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬
        if analysis["quality_score"] > 0.7:
            analysis["new_patterns"].append({
                "topic": question["topic"],
                "pattern": "High quality answer",
                "model": answer["model"]
            })
            
        return analysis
        
    def save_qa_pair(self, question: Dict[str, Any], answer: Dict[str, Any], analysis: Dict[str, Any]):
        """ì§ˆë¬¸-ë‹µë³€ ìŒ ì €ì¥"""
        qa_data = {
            "question": question,
            "answer": answer,
            "analysis": analysis,
            "session_id": self.current_session.session_id if self.current_session else None
        }
        
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬
        today = datetime.now().strftime("%Y%m%d")
        daily_dir = self.answers_dir / today
        daily_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        filename = f"{question['id']}.json"
        with open(daily_dir / filename, 'w', encoding='utf-8') as f:
            json.dump(qa_data, f, indent=2, ensure_ascii=False)
            
    async def learning_cycle(self, duration_hours: int = 24):
        """í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        logger.info(f"Starting {duration_hours} hour learning cycle...")
        logger.info(f"Max memory limit: {self.max_memory_gb:.1f}GB")
        
        # ê¸°ì¡´ í•™ìŠµ ì§„í–‰ ìƒíƒœ í‘œì‹œ
        if self.learning_progress["total_hours"] > 0:
            logger.info(f"ğŸ“š ê¸°ì¡´ í•™ìŠµ ì§„í–‰ ìƒíƒœ:")
            logger.info(f"  - ì´ í•™ìŠµ ì‹œê°„: {self.learning_progress['total_hours']:.1f}ì‹œê°„")
            logger.info(f"  - ì´ ì§ˆë¬¸ ìˆ˜: {self.learning_progress['total_questions']}")
            logger.info(f"  - ì„±ê³µí•œ ë‹µë³€: {self.learning_progress['total_successful']}")
            if self.learning_progress['total_questions'] > 0:
                overall_success_rate = (self.learning_progress['total_successful'] / 
                                      self.learning_progress['total_questions'] * 100)
                logger.info(f"  - ì „ì²´ ì„±ê³µë¥ : {overall_success_rate:.1f}%")
            logger.info(f"  - í•™ìŠµí•œ ì£¼ì œ ìˆ˜: {len(self.learning_progress['topics_progress'])}")
            print(f"\nğŸ“Š ëˆ„ì  í•™ìŠµ í†µê³„:")
            print(f"  ì´ {self.learning_progress['total_hours']:.1f}ì‹œê°„ í•™ìŠµ")
            print(f"  {self.learning_progress['total_questions']}ê°œ ì§ˆë¬¸ ì¤‘ {self.learning_progress['total_successful']}ê°œ ì„±ê³µ")
            print(f"  {len(self.learning_progress['topics_progress'])}ê°œ ì£¼ì œ í•™ìŠµ\n")
        
        # ì„¸ì…˜ ì‹œì‘
        self.current_session = LearningSession(
            session_id=datetime.now().strftime("%Y%m%d_%H%M%S"),
            start_time=datetime.now(),
            topics_covered=[],
            questions_asked=0,
            successful_answers=0,
            models_used={model: 0 for model in self.available_models.keys()},
            knowledge_gained=[]
        )
        
        end_time = datetime.now() + timedelta(hours=duration_hours)
        cycle_count = 0
        model_rotation_count = 0
        
        while datetime.now() < end_time:
            cycle_count += 1
            logger.info(f"\n--- Learning Cycle {cycle_count} ---")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            current_memory = self.get_memory_usage()
            memory_percent = self.get_memory_usage_percent()
            logger.info(f"Memory: {current_memory:.1f}GB ({memory_percent:.1f}%)")
            
            # ë©”ëª¨ë¦¬ ì„ê³„ì¹˜ ì´ˆê³¼ì‹œ ëª¨ë¸ êµì²´ (ì €ì‚¬ì–‘ ìµœì í™”: 10ì‚¬ì´í´ë§ˆë‹¤)
            if not self.check_memory_safety() or (cycle_count % 10 == 0):
                self.unload_current_model()
                model_rotation_count += 1
                logger.info(f"Model rotation #{model_rotation_count}")
            
            # ì§„í–‰í˜• ì£¼ì œ ì„ íƒ (ë‚œì´ë„ ê¸°ë°˜)
            if self.progressive_manager:
                # í˜„ì¬ ë‚œì´ë„ í™•ì¸
                current_difficulty = self.progressive_manager.get_current_difficulty()
                
                # ì§„í–‰ ìƒíƒœ ìš”ì•½ ì¶œë ¥ (5ì‚¬ì´í´ë§ˆë‹¤)
                if cycle_count % 5 == 1:
                    summary = self.progressive_manager.get_progress_summary()
                    print(f"\nğŸ“Š í•™ìŠµ ì§„í–‰ ìƒíƒœ:")
                    print(f"  í˜„ì¬ ë‚œì´ë„: {summary['difficulty_name']} (ë ˆë²¨ {current_difficulty})")
                    print(f"  ì „ì²´ ì§„í–‰ë¥ : {summary['overall']['total_questions']}ë¬¸ì œ, ì„±ê³µë¥  {summary['overall']['success_rate']:.1%}")
                    for diff, info in summary['difficulties'].items():
                        status = "âœ… ë§ˆìŠ¤í„°" if info['mastered'] else "ğŸ“– í•™ìŠµì¤‘"
                        print(f"  ë‚œì´ë„ {diff} ({info['name']}): {info['total']}ë¬¸ì œ, ì„±ê³µë¥  {info['rate']:.1%} {status}")
                    print()
                
                # ë‚œì´ë„ì— ë§ëŠ” ì£¼ì œ ì„ íƒ
                topic = self.progressive_manager.select_topic_by_difficulty(self.learning_topics, current_difficulty)
                if not topic:
                    # í´ë°±: ëœë¤ ì„ íƒ
                    topic = random.choice(self.learning_topics)
                    logger.warning(f"ì§„í–‰í˜• ì„ íƒ ì‹¤íŒ¨, ëœë¤ ì„ íƒ: {topic.topic}")
            else:
                # ì§„í–‰í˜• ê´€ë¦¬ìê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ëœë¤ ë°©ì‹
                topic = random.choice(self.learning_topics)
            
            # 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ê°ì§€
            core_categories = ["C# í”„ë¡œê·¸ë˜ë°", "í•œê¸€ ìš©ì–´", "Godot ì—”ì§„", "Godot ë„¤íŠ¸ì›Œí‚¹", "Nakama ì„œë²„"]
            is_core_topic = topic.category in core_categories
            
            # ì§ˆë¬¸ ìƒì„±
            question = self.generate_question(topic)
            
            # ì‹¤ì‹œê°„ UIìš© ì¶œë ¥
            print(f"Topic: {topic.topic} | Category: {topic.category} | Type: {question['type']}")
            if is_core_topic:
                print(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ ê°ì§€: {topic.category} - DeepSeek-coder ìµœìš°ì„  ì‚¬ìš©!")
                logger.info(f"ğŸ”¥ í•µì‹¬ ì£¼ì œ: {topic.category} - {topic.topic}")
            else:
                logger.info(f"ğŸ“š ì¼ë°˜ ì£¼ì œ: {topic.category} - {topic.topic}")
            
            logger.info(f"Topic: {topic.topic} | Type: {question['type']} | Language: {question['language']}")
            logger.info(f"Question: {question['question'][:100]}...")
            
            # ëª¨ë¸ ì„ íƒ (ë©”ëª¨ë¦¬ ê³ ë ¤)
            model_name = self.select_model_for_question(question)
            if not model_name:
                logger.error("No models available")
                break
                
            logger.info(f"Selected model: {model_name}")
            
            # ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
            answer = await self.ask_model(model_name, question)
            
            # ë‹µë³€ ë¶„ì„ (ì—ëŸ¬ ì²´í¬ í¬í•¨)
            analysis = self.analyze_answer(question, answer)
            
            # ğŸ® ì‹¤ì‹œê°„ AI ì œì–´ ìƒíƒœ ì¶œë ¥
            if not analysis.get("success", False):
                error_msg = analysis.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                attempts = answer.get("attempts", 1)
                print(f"âŒ ì œì–´ ì‹¤íŒ¨: {error_msg} (ì‹œë„: {attempts})")
                logger.error(f"ë‹µë³€ ì‹¤íŒ¨: {error_msg}")
            else:
                answer_preview = answer.get('answer', '')[:100]
                attempts = answer.get("attempts", 1)
                quality_info = answer.get("quality", {})
                
                if quality_info:
                    score = quality_info.get("score", 0.0)
                    is_acceptable = quality_info.get("is_acceptable", False)
                    quality_status = "ğŸ¯ í’ˆì§ˆ í†µê³¼" if is_acceptable else "âš ï¸ í’ˆì§ˆ ë¯¸ë‹¬"
                    print(f"âœ… {quality_status}: {answer_preview}... (ì ìˆ˜: {score:.2f}, ì‹œë„: {attempts})")
                else:
                    print(f"âœ… ë‹µë³€ ì„±ê³µ: {answer_preview}... (ì‹œë„: {attempts})")
                
                logger.info(f"ë‹µë³€ ì„±ê³µ (ê¸¸ì´: {len(answer.get('answer', ''))}, í’ˆì§ˆ: {quality_info.get('score', 'N/A')})")
            
            # ì„¸ì…˜ ì—…ë°ì´íŠ¸ (ëª¨ë“  ì‹œë„ ê¸°ë¡)
            self.current_session.questions_asked += 1
            self.current_session.models_used[model_name] += 1
            if topic.topic not in self.current_session.topics_covered:
                self.current_session.topics_covered.append(topic.topic)
                
            # ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì„±ê³µ ì¹´ìš´íŠ¸ ì¦ê°€
            success = analysis.get("success", False)
            if success:
                self.current_session.successful_answers += 1
                logger.info(f"âœ… ì„±ê³µí•œ ë‹µë³€ ìˆ˜: {self.current_session.successful_answers}")
            else:
                logger.warning(f"âŒ ì‹¤íŒ¨í•œ ë‹µë³€. ì´ìœ : {analysis.get('error', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            
            # ì§„í–‰í˜• í•™ìŠµ ê´€ë¦¬ìì— ê²°ê³¼ ì—…ë°ì´íŠ¸
            if self.progressive_manager:
                self.progressive_manager.update_topic_progress(topic.id, topic.difficulty, success)
                    
            # ê²°ê³¼ ì €ì¥
            self.save_qa_pair(question, answer, analysis)
            
            # AI ë‹µë³€ì´ ìˆëŠ” ê²½ìš° í•™ìŠµ ì‹œê°„ í™•ë³´
            if analysis.get("success", False) and answer.get('answer'):
                answer_length = len(answer.get('answer', ''))
                # ë‹µë³€ ê¸¸ì´ì— ë”°ë¥¸ í•™ìŠµ ì‹œê°„ ê³„ì‚° (100ìë‹¹ 2ì´ˆ)
                learning_time = max(5.0, min(30.0, answer_length / 100 * 2))
                
                print(f"\nğŸ“– ë‹µë³€ í•™ìŠµ ì¤‘... ({learning_time:.1f}ì´ˆ)")
                logger.info(f"ë‹µë³€ í•™ìŠµ ì‹œê°„: {learning_time:.1f}ì´ˆ (ë‹µë³€ ê¸¸ì´: {answer_length}ì)")
                
                # ë‹µë³€ ë‚´ìš© ì¼ë¶€ í‘œì‹œ (í•™ìŠµ ì¤‘ì„ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´)
                answer_text = answer.get('answer', '')
                if len(answer_text) > 200:
                    print(f"ğŸ’­ í•™ìŠµ ë‚´ìš©: {answer_text[:200]}...")
                else:
                    print(f"ğŸ’­ í•™ìŠµ ë‚´ìš©: {answer_text}")
                
                await asyncio.sleep(learning_time)
                print(f"âœ… í•™ìŠµ ì™„ë£Œ!\n")
            
            # ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            if cycle_count % 10 == 0:
                self._save_knowledge_base()
                self._save_learning_progress()  # ì§„í–‰ ìƒíƒœë„ í•¨ê»˜ ì €ì¥

            # 50 ì‚¬ì´í´ë§ˆë‹¤ ì›¹ì—ì„œ ìƒˆë¡œìš´ ì •ë³´ ìˆ˜ì§‘
            if INFORMATION_GATHERER_AVAILABLE and cycle_count % 50 == 0:
                print("ğŸŒ ì›¹ì—ì„œ ìƒˆë¡œìš´ ì½”ë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                gatherer = get_information_gatherer()
                asyncio.create_task(gatherer.gather_and_process_csharp_code())
                
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ì €ì‚¬ì–‘ ìµœì í™”)
            if cycle_count % 5 == 0:
                self.save_memory_usage_log()
                gc.collect()  # ë” ë¹ˆë²ˆí•œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì‹¤ì‹œê°„ UIìš© - ì •í™•í•œ ì„±ê³µë¥ )
            success_rate = (self.current_session.successful_answers / 
                          self.current_session.questions_asked * 100)
            failed_count = self.current_session.questions_asked - self.current_session.successful_answers
            print(f"Progress: {self.current_session.questions_asked} questions, {success_rate:.1f}% success ({self.current_session.successful_answers}âœ…/{failed_count}âŒ), {model_rotation_count} rotations")
            logger.info(f"Progress: {self.current_session.questions_asked} questions, "
                       f"{success_rate:.1f}% success rate ({self.current_session.successful_answers} success, {failed_count} failed), {model_rotation_count} rotations")
            
            # ë‹¤ìŒ ì§ˆë¬¸ê¹Œì§€ ì§§ì€ íœ´ì‹
            wait_time = random.uniform(3, 8)
            print(f"â³ ë‹¤ìŒ ì§ˆë¬¸ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
            await asyncio.sleep(wait_time)
            
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        self.unload_current_model()
        
        # ì„¸ì…˜ ì¢…ë£Œ
        self.save_session_summary()
        
    def save_session_summary(self):
        """ì„¸ì…˜ ìš”ì•½ ì €ì¥"""
        if not self.current_session:
            return
            
        summary = {
            "session": asdict(self.current_session),
            "duration": str(datetime.now() - self.current_session.start_time),
            "knowledge_base_size": {
                "patterns": len(self.knowledge_base["csharp_patterns"]),
                "translations": len(self.knowledge_base["korean_translations"]),
                "errors": len(self.knowledge_base["common_errors"])
            }
        }
        
        summary_file = self.learning_dir / f"session_{self.current_session.session_id}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
            
        # ì§„í–‰ ìƒíƒœ ì €ì¥
        self._save_learning_progress()
        
        # ì„¸ì…˜ì„ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€
        if self.current_session.session_id not in self.learning_progress["sessions_completed"]:
            self.learning_progress["sessions_completed"].append(self.current_session.session_id)
            self._save_learning_progress()
            
        logger.info(f"\nSession Summary:")
        logger.info(f"Duration: {summary['duration']}")
        logger.info(f"Questions: {self.current_session.questions_asked}")
        logger.info(f"Success Rate: {self.current_session.successful_answers / max(1, self.current_session.questions_asked) * 100:.1f}%")
        logger.info(f"Topics Covered: {len(self.current_session.topics_covered)}")
        logger.info(f"Models Used: {self.current_session.models_used}")
        
        # ëˆ„ì  í†µê³„ í‘œì‹œ
        logger.info(f"\nğŸ“Š ëˆ„ì  í•™ìŠµ í†µê³„:")
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {self.learning_progress['total_hours']:.1f}ì‹œê°„")
        logger.info(f"ì´ ì§ˆë¬¸ ìˆ˜: {self.learning_progress['total_questions']}")
        logger.info(f"ì´ ì„±ê³µ ë‹µë³€: {self.learning_progress['total_successful']}")
        logger.info(f"ì™„ë£Œëœ ì„¸ì…˜ ìˆ˜: {len(self.learning_progress['sessions_completed'])}")
        
    def generate_learning_report(self):
        """í•™ìŠµ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "generated_at": datetime.now().isoformat(),
            "total_sessions": 0,
            "total_questions": 0,
            "total_successful": 0,
            "topics_mastered": [],
            "korean_vocabulary": len(self.knowledge_base["korean_translations"]),
            "code_patterns": len(self.knowledge_base["csharp_patterns"]),
            "model_performance": {}
        }
        
        # ëª¨ë“  ì„¸ì…˜ ë¶„ì„
        for session_file in self.learning_dir.glob("session_*.json"):
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
                report["total_sessions"] += 1
                report["total_questions"] += session_data["session"]["questions_asked"]
                report["total_successful"] += session_data["session"]["successful_answers"]
                
        # ë³´ê³ ì„œ ì €ì¥
        report_file = self.learning_dir / f"learning_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        logger.info(f"\nLearning Report Generated: {report_file}")
        return report
        
    def save_memory_usage_log(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê·¸ ì €ì¥"""
        if not self.memory_usage_history:
            return
            
        log_file = self.learning_dir / "memory_usage.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.memory_usage_history, f, indent=2, ensure_ascii=False)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (ê¸°ë³¸ 32GB, ëª…ë ¹í–‰ì—ì„œ ë³€ê²½ ê°€ëŠ¥)
    max_memory = 32.0
    if len(sys.argv) > 2:
        try:
            max_memory = float(sys.argv[2])
            logger.info(f"ë©”ëª¨ë¦¬ ì œí•œì„ {max_memory}GBë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
        except ValueError:
            logger.warning("ì˜ëª»ëœ ë©”ëª¨ë¦¬ ì œí•œê°’. ê¸°ë³¸ê°’ 32GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    system = ContinuousLearningSystem(max_memory_gb=max_memory)
    
    if not system.available_models:
        logger.error("No models available. Please run install_llm_models.py first.")
        return
        
    # í•™ìŠµ ì‹œê°„ ì„¤ì • (ê¸°ë³¸ 24ì‹œê°„)
    duration = 24
    if len(sys.argv) > 1:
        try:
            duration = float(sys.argv[1])
        except ValueError:
            logger.error("Invalid duration. Using default 24 hours.")
            
    logger.info(f"Starting continuous learning for {duration} hours...")
    
    # DeepSeek-coder ìš°ì„  í™•ì¸ ë° ì•ˆë‚´
    deepseek_available = (
        "deepseek-coder-7b" in system.available_models and 
        system.available_models["deepseek-coder-7b"].get('status') == 'installed'
    )
    
    # ì‹¤ì‹œê°„ UIìš© DeepSeek-coder ìƒíƒœ ì¶œë ¥
    if deepseek_available:
        print("ğŸ”¥ DeepSeek-coder-v2 6.7B ëª¨ë¸ì„ 5ê°€ì§€ í•µì‹¬ ì£¼ì œì— ìµœìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤:")
        print("   1ï¸âƒ£ C# í”„ë¡œê·¸ë˜ë° â†’ DeepSeek-coder íŠ¹í™”")
        print("   2ï¸âƒ£ í•œê¸€ ìš©ì–´ â†’ DeepSeek-coder ë²ˆì—­")
        print("   3ï¸âƒ£ Godot ì—”ì§„ â†’ DeepSeek-coder ì—”ì§„")
        print("   4ï¸âƒ£ Godot ë„¤íŠ¸ì›Œí‚¹ â†’ DeepSeek-coder ë„¤íŠ¸ì›Œí‚¹")
        print("   5ï¸âƒ£ Nakama ì„œë²„ â†’ DeepSeek-coder ì„œë²„")
        logger.info("ğŸ”¥ DeepSeek-coder-v2 6.7B ëª¨ë¸ì„ 5ê°€ì§€ í•µì‹¬ ì£¼ì œì— ìµœìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤")
    else:
        print("âš ï¸  DeepSeek-coderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ python download_deepseek_coder.pyë¡œ ì„¤ì¹˜í•˜ë©´ ë” ë‚˜ì€ í•™ìŠµ ê°€ëŠ¥")
        logger.warning("âš ï¸  DeepSeek-coderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("ğŸ’¡ python download_deepseek_coder.pyë¡œ ì„¤ì¹˜í•˜ë©´ ë” ë‚˜ì€ í•™ìŠµ ê°€ëŠ¥")
    
    logger.info(f"Available models: {list(system.available_models.keys())}")
    logger.info(f"Memory limit: {max_memory}GB")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    initial_memory = system.get_memory_usage()
    print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}GB / {max_memory}GB")
    print(f"ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(system.available_models.keys())}")
    print(f"â° í•™ìŠµ ì‹œê°„: {duration}ì‹œê°„")
    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    logger.info(f"Initial memory usage: {initial_memory:.1f}GB")
    
    try:
        # í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰
        await system.learning_cycle(duration)
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        system.generate_learning_report()
        
    except KeyboardInterrupt:
        logger.info("\nLearning interrupted by user.")
        system.save_session_summary()
        system.generate_learning_report()
        
    except Exception as e:
        logger.error(f"Error during learning: {str(e)}")
        system.save_session_summary()

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ¯ AutoCI ì €ì‚¬ì–‘ ìµœì í™” AI í•™ìŠµ ì‹œìŠ¤í…œ")
    print("="*60)
    print("ğŸ’» RTX 2080 GPU 8GB, 32GB ë©”ëª¨ë¦¬ í™˜ê²½ ìµœì í™”")
    print("ğŸš€ autoci learn low ëª…ë ¹ì–´ì—ì„œ ì‹¤í–‰ë¨")
    print("ğŸ”¥ DeepSeek-coder-v2 6.7B ìµœìš°ì„  ì‚¬ìš©")
    print("ğŸ“š 5ê°€ì§€ í•µì‹¬ ì£¼ì œ: C#, í•œê¸€, Godot ì—”ì§„, Godot ë„¤íŠ¸ì›Œí‚¹, Nakama ì„œë²„")
    print("="*60 + "\n")
    
    # ì§ì ‘ ì‹¤í–‰
    asyncio.run(main())