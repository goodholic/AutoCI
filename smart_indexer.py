#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ì¸ë±ì„œ - ê³ ê¸‰ ë°ì´í„°ë¥¼ ëª¨ë¸ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©
"""

import os
import sys
import json
import sqlite3
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import hashlib
import pickle

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('smart_indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class SmartIndexer:
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì¸ë±ì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_dir: str = "expert_learning_data"):
        self.data_dir = Path(data_dir)
        self.db_path = self.data_dir / "smart_index.db"
        self.vector_store_path = self.data_dir / "vectors"
        self.vector_store_path.mkdir(exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.init_database()
        
        # ì¸ë±ì‹± í†µê³„
        self.stats = {
            'total_files': 0,
            'total_entries': 0,
            'categories': {},
            'quality_scores': []
        }
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë©”ì¸ ì¸ë±ìŠ¤ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS smart_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_path TEXT NOT NULL,
            category TEXT NOT NULL,
            subcategory TEXT,
            content_type TEXT,
            title TEXT,
            description TEXT,
            content TEXT,
            code_snippets TEXT,
            quality_score REAL,
            complexity_level INTEGER,
            keywords TEXT,
            vector_id TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # ì½”ë“œ ì˜ˆì œ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS code_examples (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            index_id INTEGER,
            language TEXT,
            purpose TEXT,
            code TEXT,
            explanation TEXT,
            performance_notes TEXT,
            FOREIGN KEY (index_id) REFERENCES smart_index(id)
        )
        ''')
        
        # í•™ìŠµ ì¸ì‚¬ì´íŠ¸ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS learning_insights (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            index_id INTEGER,
            insight_type TEXT,
            content TEXT,
            importance_score REAL,
            FOREIGN KEY (index_id) REFERENCES smart_index(id)
        )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON smart_index(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_quality ON smart_index(quality_score)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_complexity ON smart_index(complexity_level)')
        
        conn.commit()
        conn.close()
        
    def index_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì¸ë±ì‹±"""
        logger.info("ğŸš€ ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹± ì‹œì‘...")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
        categories = [
            ('microsoft_docs', self.index_microsoft_docs),
            ('github_projects', self.index_github_projects),
            ('stackoverflow', self.index_stackoverflow),
            ('expert_blogs', self.index_expert_blogs),
            ('design_patterns', self.index_design_patterns),
            ('unity_csharp', self.index_unity_csharp),
            ('performance_tips', self.index_performance_tips)
        ]
        
        for category, indexer_func in categories:
            category_path = self.data_dir / category
            if category_path.exists():
                logger.info(f"ğŸ“ {category} ì¸ë±ì‹± ì¤‘...")
                indexed = indexer_func(category_path)
                self.stats['categories'][category] = indexed
                
        # í†µê³„ ì €ì¥
        self.save_statistics()
        
        # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
        self.create_vector_indices()
        
        # í’ˆì§ˆ ë¶„ì„
        self.analyze_quality()
        
    def index_microsoft_docs(self, path: Path) -> int:
        """Microsoft ë¬¸ì„œ ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self.calculate_quality_score(data)
                
                # ë³µì¡ë„ ë ˆë²¨ ê²°ì •
                complexity = self.determine_complexity(data.get('topic', ''))
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                self.save_to_index(
                    file_path=str(file_path),
                    category='microsoft_docs',
                    subcategory=data.get('topic', 'general'),
                    content_type='documentation',
                    title=data.get('topic', '').replace('-', ' ').title(),
                    description=f"Microsoft official documentation on {data.get('topic', '')}",
                    content=json.dumps(data),
                    quality_score=quality_score,
                    complexity_level=complexity
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
                
        return indexed
        
    def index_github_projects(self, path: Path) -> int:
        """GitHub í”„ë¡œì íŠ¸ ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                quality_score = 0.9  # GitHub ì „ë¬¸ê°€ í”„ë¡œì íŠ¸ëŠ” ë†’ì€ í’ˆì§ˆ
                
                self.save_to_index(
                    file_path=str(file_path),
                    category='github_projects',
                    subcategory='expert_code',
                    content_type='source_code',
                    title=data.get('name', ''),
                    description=data.get('description', ''),
                    content=json.dumps(data),
                    quality_score=quality_score,
                    complexity_level=3
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"GitHub ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def index_stackoverflow(self, path: Path) -> int:
        """Stack Overflow Q&A ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.save_to_index(
                    file_path=str(file_path),
                    category='stackoverflow',
                    subcategory='qa',
                    content_type='qa',
                    title=data.get('query', ''),
                    description='Expert Q&A from Stack Overflow',
                    content=json.dumps(data),
                    quality_score=0.85,
                    complexity_level=2
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"Stack Overflow ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def index_expert_blogs(self, path: Path) -> int:
        """ì „ë¬¸ê°€ ë¸”ë¡œê·¸ ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.save_to_index(
                    file_path=str(file_path),
                    category='expert_blogs',
                    subcategory='articles',
                    content_type='article',
                    title=f"{data.get('author', '')} - Expert Insights",
                    description=f"Expert articles on {', '.join(data.get('topics', []))}",
                    content=json.dumps(data),
                    quality_score=0.88,
                    complexity_level=3
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"ë¸”ë¡œê·¸ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def index_design_patterns(self, path: Path) -> int:
        """ë””ìì¸ íŒ¨í„´ ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.save_to_index(
                    file_path=str(file_path),
                    category='design_patterns',
                    subcategory=data.get('category', 'general'),
                    content_type='pattern',
                    title=data.get('pattern', ''),
                    description=f"Design pattern: {data.get('pattern', '')}",
                    content=json.dumps(data),
                    quality_score=0.95,
                    complexity_level=3
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"íŒ¨í„´ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def index_unity_csharp(self, path: Path) -> int:
        """Unity C# ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.save_to_index(
                    file_path=str(file_path),
                    category='unity_csharp',
                    subcategory='unity_specific',
                    content_type='tutorial',
                    title=data.get('topic', ''),
                    description='Unity-specific C# best practices',
                    content=json.dumps(data),
                    quality_score=0.87,
                    complexity_level=2
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"Unity ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def index_performance_tips(self, path: Path) -> int:
        """ì„±ëŠ¥ íŒ ì¸ë±ì‹±"""
        indexed = 0
        
        for file_path in path.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.save_to_index(
                    file_path=str(file_path),
                    category='performance_tips',
                    subcategory='optimization',
                    content_type='guide',
                    title=data.get('topic', ''),
                    description='Performance optimization guide',
                    content=json.dumps(data),
                    quality_score=0.92,
                    complexity_level=3
                )
                
                indexed += 1
                
            except Exception as e:
                logger.error(f"ì„±ëŠ¥ íŒ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                
        return indexed
        
    def save_to_index(self, **kwargs):
        """ì¸ë±ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(kwargs.get('content', ''))
        kwargs['keywords'] = json.dumps(keywords)
        
        # ë²¡í„° ID ìƒì„±
        vector_id = hashlib.md5(kwargs['file_path'].encode()).hexdigest()
        kwargs['vector_id'] = vector_id
        
        # ë°ì´í„° ì‚½ì…
        columns = ', '.join(kwargs.keys())
        placeholders = ', '.join(['?' for _ in kwargs])
        query = f"INSERT INTO smart_index ({columns}) VALUES ({placeholders})"
        
        cursor.execute(query, list(kwargs.values()))
        conn.commit()
        conn.close()
        
        self.stats['total_entries'] += 1
        self.stats['quality_scores'].append(kwargs.get('quality_score', 0))
        
    def calculate_quality_score(self, data: Dict) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ì½˜í…ì¸  ì¡´ì¬ ì—¬ë¶€
        if data.get('content'): score += 0.1
        if data.get('examples'): score += 0.15
        if data.get('best_practices'): score += 0.15
        if data.get('code_examples'): score += 0.1
        
        return min(score, 1.0)
        
    def determine_complexity(self, topic: str) -> int:
        """ë³µì¡ë„ ë ˆë²¨ ê²°ì • (1-5)"""
        advanced_topics = ['async', 'parallel', 'unsafe', 'expression', 'reflection']
        intermediate_topics = ['linq', 'generics', 'delegates', 'events']
        
        topic_lower = topic.lower()
        
        if any(adv in topic_lower for adv in advanced_topics):
            return 4
        elif any(inter in topic_lower for inter in intermediate_topics):
            return 3
        else:
            return 2
            
    def extract_keywords(self, content: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ NLP ì‚¬ìš©)
        keywords = []
        important_terms = [
            'async', 'await', 'task', 'linq', 'generic', 'interface',
            'abstract', 'virtual', 'override', 'delegate', 'event',
            'lambda', 'expression', 'pattern', 'performance', 'memory'
        ]
        
        content_lower = content.lower()
        for term in important_terms:
            if term in content_lower:
                keywords.append(term)
                
        return keywords
        
    def create_vector_indices(self):
        """ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”¢ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” sentence transformers ë“±ì„ ì‚¬ìš©
        
    def analyze_quality(self):
        """í’ˆì§ˆ ë¶„ì„"""
        if self.stats['quality_scores']:
            avg_quality = np.mean(self.stats['quality_scores'])
            logger.info(f"ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}")
            
    def save_statistics(self):
        """í†µê³„ ì €ì¥"""
        self.stats['total_files'] = sum(self.stats['categories'].values())
        
        # JSON ì €ì¥
        stats_file = self.data_dir / 'indexing_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2)
            
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
        self.generate_report()
        
    def generate_report(self):
        """ì¸ë±ì‹± ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""# ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹± ë¦¬í¬íŠ¸

## ğŸ“Š ì¸ë±ì‹± í†µê³„
- **ì´ íŒŒì¼ ìˆ˜**: {self.stats['total_files']}
- **ì´ ì—”íŠ¸ë¦¬ ìˆ˜**: {self.stats['total_entries']}
- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {np.mean(self.stats['quality_scores']) if self.stats['quality_scores'] else 0:.2f}

## ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ í˜„í™©
"""
        
        for category, count in self.stats['categories'].items():
            report += f"- **{category}**: {count}ê°œ\n"
            
        report += f"""
## ğŸ¯ í’ˆì§ˆ ë¶„í¬
- ê³ í’ˆì§ˆ (0.9+): {sum(1 for s in self.stats['quality_scores'] if s >= 0.9)}ê°œ
- ì¤‘ìƒí’ˆì§ˆ (0.8-0.9): {sum(1 for s in self.stats['quality_scores'] if 0.8 <= s < 0.9)}ê°œ
- ì¤‘í’ˆì§ˆ (0.7-0.8): {sum(1 for s in self.stats['quality_scores'] if 0.7 <= s < 0.8)}ê°œ
- ê¸°íƒ€: {sum(1 for s in self.stats['quality_scores'] if s < 0.7)}ê°œ

## ğŸ’¡ í™œìš© ë°©ë²•
1. ê³ í’ˆì§ˆ ë°ì´í„° ìš°ì„  í•™ìŠµ
2. ë³µì¡ë„ë³„ ë‹¨ê³„ì  í•™ìŠµ
3. ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” í•™ìŠµ

---
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_file = self.data_dir / 'indexing_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"ğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")
        
    def query_index(self, query: str, category: Optional[str] = None, 
                   min_quality: float = 0.7, limit: int = 10) -> List[Dict]:
        """ì¸ë±ìŠ¤ ì¿¼ë¦¬"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        base_query = """
        SELECT * FROM smart_index 
        WHERE quality_score >= ?
        """
        
        params = [min_quality]
        
        if category:
            base_query += " AND category = ?"
            params.append(category)
            
        if query:
            base_query += " AND (title LIKE ? OR description LIKE ? OR keywords LIKE ?)"
            query_param = f"%{query}%"
            params.extend([query_param, query_param, query_param])
            
        base_query += " ORDER BY quality_score DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(base_query, params)
        
        columns = [desc[0] for desc in cursor.description]
        results = []
        
        for row in cursor.fetchall():
            results.append(dict(zip(columns, row)))
            
        conn.close()
        return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    indexer = SmartIndexer()
    
    # ì „ì²´ ì¸ë±ì‹±
    indexer.index_all_data()
    
    print("\nâœ… ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {indexer.stats['total_entries']}ê°œ í•­ëª© ì¸ë±ì‹±")
    print(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤: {indexer.db_path}")
    
    # ìƒ˜í”Œ ì¿¼ë¦¬
    print("\nğŸ” ìƒ˜í”Œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸...")
    results = indexer.query_index("async", min_quality=0.8, limit=5)
    print(f"'async' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")


if __name__ == "__main__":
    main()