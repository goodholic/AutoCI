#!/usr/bin/env python3
"""
C# ì „ë¬¸ê°€ ìˆ˜ì¤€ í•™ìŠµì„ ìœ„í•œ ê³ ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- GitHubì˜ ì¸ê¸° C# í”„ë¡œì íŠ¸ í¬ë¡¤ë§
- Stack Overflow C# ì§ˆë¬¸/ë‹µë³€ ìˆ˜ì§‘
- Microsoft ê³µì‹ ë¬¸ì„œ íŒŒì‹±
- 24ì‹œê°„ ìë™ í•™ìŠµ ì‹œìŠ¤í…œ
"""

import os
import json
import time
import asyncio
import aiohttp
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path
import logging
from bs4 import BeautifulSoup
import re
from collections import defaultdict
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csharp_expert_learning.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CSharpExpertCrawler:
    """C# ì „ë¬¸ ì§€ì‹ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.data_dir = Path("expert_training_data")
        self.data_dir.mkdir(exist_ok=True)
        
        self.sources = {
            "github": self.data_dir / "github_projects.json",
            "stackoverflow": self.data_dir / "stackoverflow_qa.json",
            "microsoft_docs": self.data_dir / "microsoft_docs.json",
            "nuget": self.data_dir / "nuget_packages.json",
            "blogs": self.data_dir / "expert_blogs.json"
        }
        
        self.quality_threshold = 0.8  # ê³ í’ˆì§ˆ ì½”ë“œë§Œ ìˆ˜ì§‘
        self.collected_data = defaultdict(list)
        
    async def crawl_github_top_projects(self):
        """GitHubì˜ ì¸ê¸° C# í”„ë¡œì íŠ¸ í¬ë¡¤ë§"""
        logger.info("ğŸ” GitHub ì¸ê¸° C# í”„ë¡œì íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
        
        # GitHub APIë¥¼ í†µí•œ ì¸ê¸° C# ì €ì¥ì†Œ ê²€ìƒ‰
        headers = {
            'Accept': 'application/vnd.github.v3+json',
            # GitHub í† í°ì´ ìˆë‹¤ë©´ ì¶”ê°€
            # 'Authorization': 'token YOUR_GITHUB_TOKEN'
        }
        
        # ì¸ê¸° C# í”„ë¡œì íŠ¸ ì¹´í…Œê³ ë¦¬
        categories = [
            "stars:>10000 language:csharp",  # 10,000ê°œ ì´ìƒ ìŠ¤íƒ€
            "topic:dotnet stars:>5000",       # .NET ê´€ë ¨
            "topic:aspnetcore stars:>3000",   # ASP.NET Core
            "topic:unity3d language:csharp",  # Unity
            "topic:xamarin language:csharp",  # Xamarin
            "topic:blazor",                   # Blazor
            "topic:wpf language:csharp",      # WPF
            "topic:machine-learning language:csharp"  # ML.NET
        ]
        
        async with aiohttp.ClientSession() as session:
            for category in categories:
                try:
                    url = f"https://api.github.com/search/repositories?q={category}&sort=stars&per_page=20"
                    async with session.get(url, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            for repo in data.get('items', []):
                                await self._analyze_repository(session, repo)
                                
                except Exception as e:
                    logger.error(f"GitHub í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    
                await asyncio.sleep(2)  # Rate limiting
    
    async def _analyze_repository(self, session, repo):
        """ê°œë³„ ì €ì¥ì†Œ ë¶„ì„ ë° ê³ í’ˆì§ˆ ì½”ë“œ ì¶”ì¶œ"""
        repo_name = repo['full_name']
        logger.info(f"ğŸ“¦ ì €ì¥ì†Œ ë¶„ì„: {repo_name}")
        
        # ì£¼ìš” C# íŒŒì¼ ìˆ˜ì§‘
        try:
            # Repository contents API
            contents_url = f"https://api.github.com/repos/{repo_name}/contents"
            
            async def get_cs_files(url, path=""):
                async with session.get(url) as response:
                    if response.status == 200:
                        contents = await response.json()
                        
                        for item in contents:
                            if item['type'] == 'file' and item['name'].endswith('.cs'):
                                # ê³ í’ˆì§ˆ ì½”ë“œ íŒŒì¼ë§Œ ìˆ˜ì§‘
                                if self._is_quality_code_file(item['name']):
                                    await self._download_and_analyze_file(session, item)
                            elif item['type'] == 'dir' and not item['name'].startswith('.'):
                                # í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰ (ê¹Šì´ ì œí•œ)
                                if path.count('/') < 3:
                                    await get_cs_files(item['url'], f"{path}/{item['name']}")
            
            await get_cs_files(contents_url)
            
        except Exception as e:
            logger.error(f"ì €ì¥ì†Œ ë¶„ì„ ì˜¤ë¥˜ {repo_name}: {e}")
    
    def _is_quality_code_file(self, filename):
        """ê³ í’ˆì§ˆ ì½”ë“œ íŒŒì¼ í•„í„°ë§"""
        # ì œì™¸í•  íŒŒì¼ íŒ¨í„´
        exclude_patterns = [
            'AssemblyInfo.cs',
            'Designer.cs',
            '.g.cs',
            '.g.i.cs',
            'TemporaryGeneratedFile',
            'Test.cs' if not 'Unit' in filename else None
        ]
        
        return not any(pattern and pattern in filename for pattern in exclude_patterns)
    
    async def _download_and_analyze_file(self, session, file_info):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„"""
        try:
            async with session.get(file_info['download_url']) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # ì½”ë“œ í’ˆì§ˆ í‰ê°€
                    quality_score = self._evaluate_code_quality(content)
                    
                    if quality_score >= self.quality_threshold:
                        # ê³ í’ˆì§ˆ ì½”ë“œë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì¶”ê°€
                        training_example = self._create_training_example(content, file_info)
                        self.collected_data['github'].append(training_example)
                        
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def _evaluate_code_quality(self, code):
        """ì½”ë“œ í’ˆì§ˆ í‰ê°€ (0~1 ì ìˆ˜) - README 121-129í–‰ ê¸°ì¤€"""
        score = 0.0
        
        # 1. XML ë¬¸ì„œ ì£¼ì„ (20%)
        if re.search(r'///', code):
            score += 0.20
            
        # 2. ë””ìì¸ íŒ¨í„´ (15%) - SOLID, GoF íŒ¨í„´ ì‚¬ìš©
        design_patterns = [
            'interface I', 'abstract class', 'virtual', 'override',
            'Repository', 'Factory', 'Singleton', 'Observer',
            'Strategy', 'Decorator', 'IDisposable'
        ]
        if any(pattern in code for pattern in design_patterns):
            score += 0.15
            
        # 3. í˜„ëŒ€ì  C# ê¸°ëŠ¥ (15%) - async/await, LINQ, íŒ¨í„´ ë§¤ì¹­
        modern_features = [
            'async', 'await', 'var ', '?.', '??', '=>', 'record',
            'init;', 'required', '.Where(', '.Select(', '.OrderBy(',
            'is not null', 'switch expression'
        ]
        if any(feature in code for feature in modern_features):
            score += 0.15
            
        # 4. ì—ëŸ¬ ì²˜ë¦¬ (10%) - try-catch, ì˜ˆì™¸ ì²˜ë¦¬
        if 'try' in code and 'catch' in code:
            score += 0.10
            
        # 5. ì½”ë“œ êµ¬ì¡° (10%) - ì ì ˆí•œ ê¸¸ì´, ëª¨ë“ˆí™”
        lines = len(code.split('\n'))
        if 100 <= lines <= 5000:
            score += 0.10
            
        # 6. í…ŒìŠ¤íŠ¸ ì½”ë“œ (5%) - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í¬í•¨
        test_attributes = ['[Test]', '[Fact]', '[TestMethod]', '[TestClass]']
        if any(test in code for test in test_attributes):
            score += 0.05
            
        # ë‚˜ë¨¸ì§€ 25%ëŠ” ê¸°ë³¸ í’ˆì§ˆ ìš”ì†Œ
        # - namespace ì‚¬ìš© (5%)
        if 'namespace' in code:
            score += 0.05
            
        # - using ë¬¸ ì •ë¦¬ (5%)
        if re.search(r'using\s+\w+', code):
            score += 0.05
            
        # - í´ë˜ìŠ¤/ë©”ì„œë“œ êµ¬ì¡° (5%)
        if 'class' in code and ('public' in code or 'private' in code):
            score += 0.05
            
        # - í”„ë¡œí¼í‹° ì‚¬ìš© (5%)
        if re.search(r'{\s*get;\s*set;\s*}', code):
            score += 0.05
            
        # - ì¼ë°˜ ì£¼ì„ (5%)
        if re.search(r'//[^/]|/\*', code):
            score += 0.05
            
        return min(score, 1.0)  # ìµœëŒ€ 1.0
    
    def _create_training_example(self, code, file_info):
        """í•™ìŠµ ë°ì´í„° ìƒì„±"""
        # ì½”ë“œì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ íŒ¨í„´ ì¶”ì¶œ
        patterns = self._extract_patterns(code)
        
        return {
            'id': hashlib.md5(code.encode()).hexdigest(),
            'source': 'github',
            'file_path': file_info.get('path', ''),
            'repository': file_info.get('repository', {}).get('full_name', ''),
            'code': code,
            'patterns': patterns,
            'timestamp': datetime.now().isoformat(),
            'quality_score': self._evaluate_code_quality(code),
            'training_prompts': self._generate_training_prompts(code, patterns)
        }
    
    def _extract_patterns(self, code):
        """ì½”ë“œì—ì„œ ë””ìì¸ íŒ¨í„´ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¶”ì¶œ"""
        patterns = []
        
        # ë””ìì¸ íŒ¨í„´ ê°ì§€
        pattern_detectors = {
            'Singleton': r'private\s+static\s+\w+\s+_instance',
            'Factory': r'Create\w+|Factory|Build\w+',
            'Repository': r'Repository|IRepository',
            'Service': r'Service|IService',
            'Observer': r'event\s+|EventHandler|INotify',
            'Strategy': r'interface\s+I\w+Strategy',
            'Decorator': r':\s*I\w+.*\{.*private\s+readonly\s+I\w+',
            'Command': r'ICommand|Execute\(',
            'MVVM': r'ViewModel|INotifyPropertyChanged',
            'Dependency Injection': r'private\s+readonly\s+I\w+.*constructor'
        }
        
        for pattern_name, regex in pattern_detectors.items():
            if re.search(regex, code, re.IGNORECASE | re.DOTALL):
                patterns.append(pattern_name)
        
        # C# ê³ ê¸‰ ê¸°ëŠ¥ ê°ì§€
        advanced_features = {
            'LINQ': r'\.Where\(|\.Select\(|\.OrderBy\(',
            'Async/Await': r'async\s+Task|await\s+',
            'Generics': r'<T>|<\w+>|where\s+T\s*:',
            'Extension Methods': r'static\s+\w+\s+\w+\(this\s+',
            'Lambda Expressions': r'=>',
            'Pattern Matching': r'is\s+\w+\s+\w+|switch\s*\{',
            'Nullable Reference Types': r'\w+\?|#nullable',
            'Records': r'record\s+\w+',
            'Tuples': r'\(\w+,\s*\w+\)',
            'Expression Bodies': r'=>\s*[^{]'
        }
        
        for feature_name, regex in advanced_features.items():
            if re.search(regex, code):
                patterns.append(f"Uses {feature_name}")
        
        return patterns
    
    def _generate_training_prompts(self, code, patterns):
        """ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = []
        
        # í´ë˜ìŠ¤ ì¶”ì¶œ
        classes = re.findall(r'public\s+(?:partial\s+)?(?:abstract\s+)?class\s+(\w+)', code)
        for class_name in classes:
            prompts.append({
                'instruction': f"{class_name} í´ë˜ìŠ¤ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ì˜ C# í´ë˜ìŠ¤ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                'context': f"ë‹¤ìŒ íŒ¨í„´ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”: {', '.join(patterns[:3])}",
                'output': code
            })
        
        # ë©”ì„œë“œ ì¶”ì¶œ
        methods = re.findall(r'public\s+(?:async\s+)?(?:static\s+)?(\w+)\s+(\w+)\s*\([^)]*\)', code)
        for return_type, method_name in methods[:5]:  # ìƒìœ„ 5ê°œë§Œ
            prompts.append({
                'instruction': f"{return_type} ë°˜í™˜ íƒ€ì…ì˜ {method_name} ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                'context': "ì—”í„°í”„ë¼ì´ì¦ˆ ìˆ˜ì¤€ì˜ ì—ëŸ¬ ì²˜ë¦¬ì™€ ë¡œê¹…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.",
                'output': code
            })
        
        # íŒ¨í„´ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
        if patterns:
            prompts.append({
                'instruction': f"{patterns[0]} íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” C# ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                'context': "í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                'output': code
            })
        
        return prompts

    async def crawl_stackoverflow(self):
        """Stack Overflowì—ì„œ ê³ í’ˆì§ˆ C# Q&A ìˆ˜ì§‘"""
        logger.info("ğŸ” Stack Overflow C# ì „ë¬¸ ì§€ì‹ ìˆ˜ì§‘ ì‹œì‘...")
        
        # Stack Exchange API ì‚¬ìš©
        base_url = "https://api.stackexchange.com/2.3/questions"
        
        # ë†’ì€ ì ìˆ˜ì˜ C# ì§ˆë¬¸ë“¤
        params = {
            'order': 'desc',
            'sort': 'votes',
            'tagged': 'c#',
            'site': 'stackoverflow',
            'filter': 'withbody',
            'pagesize': 100
        }
        
        async with aiohttp.ClientSession() as session:
            for min_score in [100, 50, 25]:  # ì ìˆ˜ë³„ë¡œ ìˆ˜ì§‘
                params['min'] = min_score
                
                try:
                    async with session.get(base_url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            for question in data.get('items', []):
                                if question.get('is_answered'):
                                    await self._process_stackoverflow_qa(session, question)
                                    
                except Exception as e:
                    logger.error(f"Stack Overflow í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                
                await asyncio.sleep(1)  # Rate limiting
    
    async def _process_stackoverflow_qa(self, session, question):
        """Stack Overflow Q&A ì²˜ë¦¬"""
        try:
            # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            answer_url = f"https://api.stackexchange.com/2.3/questions/{question['question_id']}/answers"
            params = {
                'order': 'desc',
                'sort': 'votes',
                'site': 'stackoverflow',
                'filter': 'withbody'
            }
            
            async with session.get(answer_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    answers = data.get('items', [])
                    
                    if answers and answers[0].get('score', 0) > 10:
                        # ê³ í’ˆì§ˆ ë‹µë³€ë§Œ ìˆ˜ì§‘
                        training_example = {
                            'instruction': self._clean_html(question.get('title', '')),
                            'context': self._clean_html(question.get('body', '')),
                            'output': self._clean_html(answers[0].get('body', '')),
                            'score': answers[0].get('score', 0),
                            'tags': question.get('tags', []),
                            'source': 'stackoverflow',
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.collected_data['stackoverflow'].append(training_example)
                        
        except Exception as e:
            logger.error(f"Stack Overflow Q&A ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _clean_html(self, html_text):
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        soup = BeautifulSoup(html_text, 'html.parser')
        
        # ì½”ë“œ ë¸”ë¡ ë³´ì¡´
        code_blocks = []
        for code in soup.find_all(['code', 'pre']):
            placeholder = f"###CODE_BLOCK_{len(code_blocks)}###"
            code_blocks.append(code.get_text())
            code.replace_with(placeholder)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text()
        
        # ì½”ë“œ ë¸”ë¡ ë³µì›
        for i, code in enumerate(code_blocks):
            text = text.replace(f"###CODE_BLOCK_{i}###", f"\n```csharp\n{code}\n```\n")
        
        return text.strip()

    async def crawl_microsoft_docs(self):
        """Microsoft ê³µì‹ C# ë¬¸ì„œ í¬ë¡¤ë§"""
        logger.info("ğŸ” Microsoft C# ê³µì‹ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì£¼ìš” ë¬¸ì„œ ì¹´í…Œê³ ë¦¬
        doc_categories = [
            "https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/",
            "https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/",
            "https://docs.microsoft.com/en-us/dotnet/api/",
            "https://docs.microsoft.com/en-us/aspnet/core/",
            "https://docs.microsoft.com/en-us/ef/core/",
            "https://docs.microsoft.com/en-us/azure/",
        ]
        
        # ì‹¤ì œ êµ¬í˜„ì‹œ Microsoft Docs API ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” êµ¬ì¡°ë§Œ ì œì‹œ
        
    def save_collected_data(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥"""
        for source, data in self.collected_data.items():
            if data:
                file_path = self.sources[source]
                
                # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
                existing_data = []
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                
                # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
                existing_ids = {item.get('id') for item in existing_data if 'id' in item}
                new_data = [item for item in data if item.get('id') not in existing_ids]
                
                combined_data = existing_data + new_data
                
                # ì €ì¥
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(combined_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… {source} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(new_data)}ê°œ ì¶”ê°€")

class CSharpExpertTrainer:
    """24ì‹œê°„ ìë™ í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.crawler = CSharpExpertCrawler()
        self.model_path = "CodeLlama-7b-Instruct-hf"
        self.training_config = {
            'learning_rate': 1e-5,
            'batch_size': 4,
            'epochs': 3,
            'gradient_accumulation_steps': 8,
            'warmup_steps': 500,
            'save_steps': 1000,
            'evaluation_strategy': 'steps',
            'eval_steps': 500
        }
        self.training_history = []
        
    async def continuous_learning_cycle(self):
        """24ì‹œê°„ ì§€ì† í•™ìŠµ ì‚¬ì´í´"""
        logger.info("ğŸš€ 24ì‹œê°„ C# ì „ë¬¸ê°€ í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘...")
        
        while True:
            try:
                # 1. ë°ì´í„° ìˆ˜ì§‘ (4ì‹œê°„)
                logger.info("ğŸ“š Phase 1: ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
                await self._collect_data_phase()
                
                # 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦ (1ì‹œê°„)
                logger.info("ğŸ” Phase 2: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦...")
                await self._preprocess_data_phase()
                
                # 3. ëª¨ë¸ í•™ìŠµ (6ì‹œê°„)
                logger.info("ğŸ§  Phase 3: ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
                await self._training_phase()
                
                # 4. ëª¨ë¸ í‰ê°€ ë° ë°°í¬ (1ì‹œê°„)
                logger.info("ğŸ“Š Phase 4: ëª¨ë¸ í‰ê°€ ë° ë°°í¬...")
                await self._evaluation_phase()
                
                # 5. ì½”ë“œ ìë™ ê°œì„  ì„œë¹„ìŠ¤ (12ì‹œê°„)
                logger.info("ğŸ”§ Phase 5: ì½”ë“œ ìë™ ê°œì„  ì„œë¹„ìŠ¤ ì‹¤í–‰...")
                await self._code_improvement_service()
                
            except Exception as e:
                logger.error(f"í•™ìŠµ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸° í›„ ì¬ì‹œë„
    
    async def _collect_data_phase(self):
        """ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„"""
        tasks = [
            self.crawler.crawl_github_top_projects(),
            self.crawler.crawl_stackoverflow(),
            self.crawler.crawl_microsoft_docs()
        ]
        
        # ë³‘ë ¬ ìˆ˜ì§‘
        await asyncio.gather(*tasks)
        
        # ë°ì´í„° ì €ì¥
        self.crawler.save_collected_data()
        
        # ìˆ˜ì§‘ í†µê³„
        total_collected = sum(len(data) for data in self.crawler.collected_data.values())
        logger.info(f"âœ… ì´ {total_collected}ê°œì˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    
    async def _preprocess_data_phase(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„"""
        # ëª¨ë“  ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ
        all_training_data = []
        
        for source_file in self.crawler.sources.values():
            if source_file.exists():
                with open(source_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_training_data.extend(data)
        
        # ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ í•„í„°ë§
        high_quality_data = [
            item for item in all_training_data 
            if item.get('quality_score', 0) >= 0.7
        ]
        
        # í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
        training_dataset = self._create_training_dataset(high_quality_data)
        
        # ì €ì¥
        dataset_path = self.crawler.data_dir / "training_dataset.json"
        with open(dataset_path, 'w', encoding='utf-8') as f:
            json.dump(training_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… {len(training_dataset)}ê°œì˜ ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    
    def _create_training_dataset(self, data):
        """í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±"""
        dataset = []
        
        for item in data:
            # ë‹¤ì–‘í•œ í•™ìŠµ ì˜ˆì œ ìƒì„±
            if 'training_prompts' in item:
                dataset.extend(item['training_prompts'])
            
            # Stack Overflow ë°ì´í„°
            if item.get('source') == 'stackoverflow':
                dataset.append({
                    'instruction': item.get('instruction', ''),
                    'input': item.get('context', ''),
                    'output': item.get('output', '')
                })
            
            # GitHub ì½”ë“œ
            if item.get('source') == 'github' and 'code' in item:
                # ì½”ë“œ ì„¤ëª… ìƒì„±
                dataset.append({
                    'instruction': "ë‹¤ìŒ C# ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    'input': item['code'][:1000],  # ì²˜ìŒ 1000ì
                    'output': f"ì´ ì½”ë“œëŠ” {', '.join(item.get('patterns', []))} íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤..."
                })
                
                # ì½”ë“œ ê°œì„  ì œì•ˆ
                dataset.append({
                    'instruction': "ë‹¤ìŒ C# ì½”ë“œë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”.",
                    'input': item['code'][:1000],
                    'output': "ê°œì„ ëœ ì½”ë“œ:\n```csharp\n// ë” ë‚˜ì€ ì—ëŸ¬ ì²˜ë¦¬ì™€ ì„±ëŠ¥ ìµœì í™” ì ìš©\n```"
                })
        
        return dataset
    
    async def _training_phase(self):
        """ëª¨ë¸ í•™ìŠµ ë‹¨ê³„"""
        # ì‹¤ì œ í•™ìŠµ ì½”ë“œ (fine_tune.py í™œìš©)
        import subprocess
        
        logger.info("ğŸƒ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        cmd = [
            "python", "MyAIWebApp/Models/fine_tune.py",
            "--data", str(self.crawler.data_dir / "training_dataset.json"),
            "--model", self.model_path
        ]
        
        process = subprocess.run(cmd, capture_output=True, text=True)
        
        if process.returncode == 0:
            logger.info("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            
            # í•™ìŠµ ê¸°ë¡ ì €ì¥
            self.training_history.append({
                'timestamp': datetime.now().isoformat(),
                'dataset_size': len(json.load(open(self.crawler.data_dir / "training_dataset.json"))),
                'status': 'success'
            })
        else:
            logger.error(f"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {process.stderr}")
    
    async def _evaluation_phase(self):
        """ëª¨ë¸ í‰ê°€ ë‹¨ê³„"""
        logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
        test_cases = [
            {
                'prompt': "C#ì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                'expected_quality': 'expert'
            },
            {
                'prompt': "Entity Framework Coreì—ì„œ ë³µì¡í•œ ì¿¼ë¦¬ ìµœì í™” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                'expected_quality': 'expert'
            },
            {
                'prompt': "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì—ì„œ ë¶„ì‚° íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ë°©ë²•ì„ C#ìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”.",
                'expected_quality': 'expert'
            }
        ]
        
        # í‰ê°€ ìˆ˜í–‰
        evaluation_results = await self._run_evaluation(test_cases)
        
        # ê²°ê³¼ ì €ì¥
        eval_path = self.crawler.data_dir / "evaluation_results.json"
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ: í‰ê·  ì ìˆ˜ {evaluation_results.get('average_score', 0):.2f}")
    
    async def _run_evaluation(self, test_cases):
        """í‰ê°€ ì‹¤í–‰"""
        # ì‹¤ì œ í‰ê°€ ë¡œì§ êµ¬í˜„
        results = {
            'test_cases': test_cases,
            'scores': [],
            'average_score': 0
        }
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ ì¶œë ¥ì„ í‰ê°€
        # ...
        
        return results
    
    async def _code_improvement_service(self):
        """24ì‹œê°„ ì½”ë“œ ê°œì„  ì„œë¹„ìŠ¤"""
        logger.info("ğŸ”§ ìë™ ì½”ë“œ ê°œì„  ì„œë¹„ìŠ¤ ì‹œì‘...")
        
        # ëª¨ë‹ˆí„°ë§í•  ë””ë ‰í† ë¦¬
        watch_dirs = [
            Path("/mnt/c/Users/super/Desktop/Unity Project(25ë…„ë„ ì œì‘)"),
            Path(".")  # í˜„ì¬ í”„ë¡œì íŠ¸
        ]
        
        # 12ì‹œê°„ ë™ì•ˆ ì„œë¹„ìŠ¤ ì‹¤í–‰
        end_time = datetime.now() + timedelta(hours=12)
        
        while datetime.now() < end_time:
            for watch_dir in watch_dirs:
                if watch_dir.exists():
                    await self._scan_and_improve_code(watch_dir)
            
            await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ìŠ¤ìº”
    
    async def _scan_and_improve_code(self, directory):
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ì½”ë“œ ê°œì„ """
        cs_files = list(directory.rglob("*.cs"))
        
        for cs_file in cs_files[:10]:  # í•œ ë²ˆì— 10ê°œì”© ì²˜ë¦¬
            try:
                # ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ë§Œ
                if (datetime.now() - datetime.fromtimestamp(cs_file.stat().st_mtime)) < timedelta(hours=24):
                    await self._improve_single_file(cs_file)
            except Exception as e:
                logger.error(f"íŒŒì¼ ê°œì„  ì˜¤ë¥˜ {cs_file}: {e}")
    
    async def _improve_single_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ ê°œì„ """
        logger.info(f"ğŸ” ì½”ë“œ ë¶„ì„ ì¤‘: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_code = f.read()
        
        # ì½”ë“œ í’ˆì§ˆ í‰ê°€
        quality_score = self.crawler._evaluate_code_quality(original_code)
        
        if quality_score < 0.7:  # í’ˆì§ˆì´ ë‚®ì€ ê²½ìš°ë§Œ ê°œì„ 
            logger.info(f"ğŸ“ˆ ì½”ë“œ ê°œì„  í•„ìš” (í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f})")
            
            # AIë¥¼ í†µí•œ ì½”ë“œ ê°œì„  ì œì•ˆ
            improvements = await self._generate_improvements(original_code)
            
            # ê°œì„  ì‚¬í•­ ì €ì¥
            improvement_file = file_path.parent / f"{file_path.stem}_improvements.md"
            with open(improvement_file, 'w', encoding='utf-8') as f:
                f.write(f"# {file_path.name} ê°œì„  ì œì•ˆ\n\n")
                f.write(f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f} â†’ {improvements.get('new_score', 0):.2f}\n\n")
                f.write("## ê°œì„  ì‚¬í•­\n\n")
                for improvement in improvements.get('suggestions', []):
                    f.write(f"- {improvement}\n")
                f.write("\n## ê°œì„ ëœ ì½”ë“œ\n\n```csharp\n")
                f.write(improvements.get('improved_code', ''))
                f.write("\n```\n")
            
            logger.info(f"âœ… ê°œì„  ì œì•ˆ ì €ì¥: {improvement_file}")
    
    async def _generate_improvements(self, code):
        """AIë¥¼ í†µí•œ ì½”ë“œ ê°œì„  ìƒì„±"""
        # ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ êµ¬ì¡°ë§Œ ì œê³µ
        
        return {
            'suggestions': [
                "null ì²´í¬ ì¶”ê°€ í•„ìš”",
                "ë¹„ë™ê¸° ë©”ì„œë“œë¡œ ë³€í™˜ ê¶Œì¥",
                "SOLID ì›ì¹™ ì ìš© í•„ìš”",
                "ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” í•„ìš”"
            ],
            'improved_code': code + "\n// AIê°€ ê°œì„ í•œ ì½”ë“œ",
            'new_score': 0.85
        }

class LearningDashboard:
    """í•™ìŠµ ì§„í–‰ ìƒí™© ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self):
        self.stats_file = Path("learning_stats.json")
        
    def update_stats(self, new_data):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        stats = self.load_stats()
        
        stats['last_update'] = datetime.now().isoformat()
        stats['total_data_collected'] = stats.get('total_data_collected', 0) + new_data.get('collected', 0)
        stats['total_training_hours'] = stats.get('total_training_hours', 0) + new_data.get('training_hours', 0)
        stats['model_improvements'] = stats.get('model_improvements', [])
        
        if 'improvement' in new_data:
            stats['model_improvements'].append({
                'timestamp': datetime.now().isoformat(),
                'score_change': new_data['improvement']
            })
        
        self.save_stats(stats)
    
    def load_stats(self):
        """í†µê³„ ë¡œë“œ"""
        if self.stats_file.exists():
            with open(self.stats_file, 'r') as f:
                return json.load(f)
        return {}
    
    def save_stats(self, stats):
        """í†µê³„ ì €ì¥"""
        with open(self.stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
    
    def generate_report(self):
        """í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±"""
        stats = self.load_stats()
        
        report = f"""
# C# Expert AI í•™ìŠµ ë¦¬í¬íŠ¸

## ğŸ“Š ì „ì²´ í†µê³„
- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {stats.get('last_update', 'N/A')}
- ì´ ìˆ˜ì§‘ ë°ì´í„°: {stats.get('total_data_collected', 0):,}ê°œ
- ì´ í•™ìŠµ ì‹œê°„: {stats.get('total_training_hours', 0):.1f}ì‹œê°„
- ëª¨ë¸ ê°œì„  íšŸìˆ˜: {len(stats.get('model_improvements', []))}íšŒ

## ğŸ“ˆ ìµœê·¼ ì„±ëŠ¥ í–¥ìƒ
"""
        
        recent_improvements = stats.get('model_improvements', [])[-5:]
        for imp in recent_improvements:
            report += f"- {imp['timestamp']}: {imp['score_change']:+.2f}ì \n"
        
        return report

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    trainer = CSharpExpertTrainer()
    dashboard = LearningDashboard()
    
    # 24ì‹œê°„ ìë™ í•™ìŠµ ì‹œì‘
    await trainer.continuous_learning_cycle()

if __name__ == "__main__":
    asyncio.run(main())