#!/usr/bin/env python3
"""
AutoCI ê³ ê¸‰ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í•™ìŠµ AI
ChatGPT ìˆ˜ì¤€ì˜ ëŒ€í™”í˜• AI with ì‹¤ì‹œê°„ í•™ìŠµ
"""

import os
import sys
import time
import json
import sqlite3
import threading
import logging
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import pickle
import hashlib

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, Dataset
    from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ConversationContext:
    """ëŒ€í™” ë§¥ë½ ì •ë³´"""
    conversation_id: str
    user_id: str
    messages: List[Dict[str, str]]  # {"role": "user/assistant", "content": "..."}
    context_embeddings: Optional[torch.Tensor]
    topic: str
    sentiment: str
    importance_score: float
    created_at: str
    last_updated: str

@dataclass
class LearningFeedback:
    """í•™ìŠµ í”¼ë“œë°± ì •ë³´"""
    conversation_id: str
    message_id: str
    feedback_type: str  # "positive", "negative", "correction"
    feedback_score: float  # -1.0 to 1.0
    user_correction: Optional[str]
    context_relevance: float
    timestamp: str

class KoreanTransformerModel(nn.Module):
    """í•œêµ­ì–´ íŠ¹í™” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸"""
    
    def __init__(self, model_name: str = "klue/bert-base", vocab_size: int = 32000,
                 hidden_size: int = 768, num_layers: int = 12, num_heads: int = 12,
                 max_length: int = 512):
        super(KoreanTransformerModel, self).__init__()
        
        # ì‚¬ì „ í›ˆë ¨ëœ í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ
        if TRANSFORMERS_AVAILABLE:
            try:
                self.base_model = AutoModel.from_pretrained(model_name)
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                hidden_size = self.base_model.config.hidden_size
            except:
                logger.warning("ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ì‚¬ìš©")
                self.base_model = None
                self.tokenizer = None
        else:
            self.base_model = None
            self.tokenizer = None
        
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.max_length = max_length
        
        # ì‚¬ìš©ì ì •ì˜ ë ˆì´ì–´ë“¤
        if self.base_model is None:
            # ì„ë² ë”© ë ˆì´ì–´
            self.token_embedding = nn.Embedding(vocab_size, hidden_size)
            self.position_embedding = nn.Embedding(max_length, hidden_size)
            
            # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
            self.transformer_layers = nn.ModuleList([
                nn.TransformerEncoderLayer(
                    d_model=hidden_size,
                    nhead=num_heads,
                    dim_feedforward=hidden_size * 4,
                    dropout=0.1,
                    batch_first=True
                ) for _ in range(num_layers)
            ])
        
        # ëŒ€í™”í˜• AIë¥¼ ìœ„í•œ íŠ¹í™” ë ˆì´ì–´ë“¤
        self.context_encoder = nn.Linear(hidden_size, hidden_size)
        self.response_generator = nn.Linear(hidden_size, vocab_size)
        self.sentiment_classifier = nn.Linear(hidden_size, 5)  # 5ê°€ì§€ ê°ì •
        self.topic_classifier = nn.Linear(hidden_size, 20)     # 20ê°€ì§€ ì£¼ì œ
        self.quality_scorer = nn.Linear(hidden_size, 1)        # ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜
        
        # ë©”ëª¨ë¦¬ ì–´í…ì…˜ (ëŒ€í™” ë§¥ë½ ìœ ì§€)
        self.memory_attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)
        self.memory_gate = nn.Linear(hidden_size * 2, hidden_size)
        
        # í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ ì´ˆê¸°í™”
        self.init_weights()
    
    def init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def encode_text(self, text: str, max_length: int = None) -> torch.Tensor:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        if max_length is None:
            max_length = self.max_length
        
        if self.tokenizer and self.base_model:
            # ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding="max_length"
            )
            
            with torch.no_grad():
                outputs = self.base_model(**inputs)
                return outputs.last_hidden_state.mean(dim=1)  # [batch_size, hidden_size]
        else:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì¸ì½”ë”©
            words = text.split()[:max_length]
            token_ids = [hash(word) % self.vocab_size for word in words]
            
            # íŒ¨ë”©
            while len(token_ids) < max_length:
                token_ids.append(0)
            
            token_tensor = torch.tensor([token_ids], dtype=torch.long)
            position_ids = torch.arange(max_length).unsqueeze(0)
            
            # ì„ë² ë”©
            token_emb = self.token_embedding(token_tensor)
            pos_emb = self.position_embedding(position_ids)
            
            embeddings = token_emb + pos_emb
            
            # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í†µê³¼
            for layer in self.transformer_layers:
                embeddings = layer(embeddings)
            
            return embeddings.mean(dim=1)  # [batch_size, hidden_size]
    
    def forward(self, input_text: str, context_memory: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_encoding = self.encode_text(input_text)  # [1, hidden_size]
        
        # ì»¨í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        context_vector = self.context_encoder(text_encoding)
        
        # ë©”ëª¨ë¦¬ ì–´í…ì…˜ (ì´ì „ ëŒ€í™” ë§¥ë½ í™œìš©)
        if context_memory is not None:
            attended_memory, attention_weights = self.memory_attention(
                context_vector.unsqueeze(1),  # query
                context_memory.unsqueeze(0),  # key, value
                context_memory.unsqueeze(0)
            )
            
            # ê²Œì´íŠ¸ë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìœµí•©
            combined = torch.cat([context_vector, attended_memory.squeeze(1)], dim=-1)
            gated_context = torch.sigmoid(self.memory_gate(combined))
            context_vector = context_vector * gated_context + attended_memory.squeeze(1) * (1 - gated_context)
        
        # ë‹¤ì–‘í•œ ì¶œë ¥ ìƒì„±
        outputs = {
            "context_encoding": context_vector,
            "response_logits": self.response_generator(context_vector),
            "sentiment_logits": self.sentiment_classifier(context_vector),
            "topic_logits": self.topic_classifier(context_vector),
            "quality_score": torch.sigmoid(self.quality_scorer(context_vector))
        }
        
        return outputs
    
    def generate_response(self, input_text: str, context_memory: Optional[torch.Tensor] = None,
                         max_response_length: int = 100) -> Tuple[str, float]:
        """ì‘ë‹µ ìƒì„±"""
        
        with torch.no_grad():
            outputs = self.forward(input_text, context_memory)
            
            # ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜
            quality_score = outputs["quality_score"].item()
            
            # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë””ì½”ë”© í•„ìš”)
            response = self._decode_response(outputs["response_logits"], input_text)
            
            return response, quality_score
    
    def _decode_response(self, response_logits: torch.Tensor, input_text: str) -> str:
        """ì‘ë‹µ ë””ì½”ë”© (ê°„ì†Œí™”ëœ ë²„ì „)"""
        
        # ì…ë ¥ì— ë”°ë¥¸ í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ (ì‹¤ì œë¡œëŠ” beam search ë“± ì‚¬ìš©)
        input_lower = input_text.lower()
        
        # Unity/C# ê´€ë ¨
        if any(word in input_lower for word in ["unity", "ìœ ë‹ˆí‹°", "ê²Œì„", "ì˜¤ë¸Œì íŠ¸"]):
            responses = [
                "Unity ê°œë°œì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "Unityì—ì„œ GameObjectëŠ” ì”¬ì˜ ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.",
                "Unity ìŠ¤í¬ë¦½íŠ¸ëŠ” C#ìœ¼ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.",
                "Unityì—ì„œëŠ” ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        
        # C# í”„ë¡œê·¸ë˜ë°
        elif any(word in input_lower for word in ["c#", "ì½”ë“œ", "í”„ë¡œê·¸ë˜ë°", "ìŠ¤í¬ë¦½íŠ¸"]):
            responses = [
                "C# í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•´ ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "C#ì€ ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
                "LINQë¥¼ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì²˜ë¦¬ê°€ ê°„í¸í•©ë‹ˆë‹¤.",
                "async/awaitë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        
        # ì¸ì‚¬
        elif any(word in input_lower for word in ["ì•ˆë…•", "hello", "hi", "ë°˜ê°€"]):
            responses = [
                "ì•ˆë…•í•˜ì„¸ìš”! AutoCIì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?",
                "ë°˜ê°‘ìŠµë‹ˆë‹¤! Unity ê°œë°œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.",
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ë„ ì¦ê±°ìš´ ê°œë°œ ë˜ì„¸ìš”!"
            ]
        
        # ê°ì‚¬ ì¸ì‚¬
        elif any(word in input_lower for word in ["ê³ ë§ˆ", "ê°ì‚¬", "thank"]):
            responses = [
                "ì²œë§Œì—ìš”! ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.",
                "ë„ì›€ì´ ë˜ì–´ì„œ ê¸°ë»ìš”! ê³„ì†í•´ì„œ ì¢‹ì€ ê°œë°œ ë˜ì„¸ìš”.",
                "ë³„ë§ì”€ì„ìš”! AutoCIëŠ” í•­ìƒ ì—¬ëŸ¬ë¶„ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            ]
        
        # ì˜¤ë¥˜/ë¬¸ì œ
        elif any(word in input_lower for word in ["ì˜¤ë¥˜", "ì—ëŸ¬", "error", "ë¬¸ì œ", "ë²„ê·¸"]):
            responses = [
                "ì˜¤ë¥˜ í•´ê²°ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí–ˆë‚˜ìš”?",
                "ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ í•´ê²° ë°©ë²•ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ë¬¸ì œ ìƒí™©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        
        # ê¸°ë³¸ ì‘ë‹µ
        else:
            responses = [
                "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "ì¢‹ì€ ì§ˆë¬¸ì´ë„¤ìš”! ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                "Unityë‚˜ C# ê°œë°œì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ì‹œë¼ë©´ ì–¸ì œë“  ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            ]
        
        # ì‘ë‹µ ë¡œì§“ì„ ì´ìš©í•œ ì„ íƒ (ê°„ì†Œí™”)
        response_index = abs(hash(input_text)) % len(responses)
        return responses[response_index]

class AdvancedMemorySystem:
    """ê³ ê¸‰ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: str = "advanced_memory.db"):
        self.db_path = db_path
        self.short_term_memory = {}  # conversation_id -> ConversationContext
        self.working_memory = {}     # user_id -> recent contexts
        self.episodic_memory = []    # long-term conversation episodes
        
        # ë©”ëª¨ë¦¬ ì„¤ì •
        self.max_short_term = 100
        self.max_working_memory = 10
        self.max_episodic_memory = 1000
        
        self.init_database()
        logger.info("ğŸ§  ê³ ê¸‰ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ëŒ€í™” ë§¥ë½ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS conversation_contexts (
                    conversation_id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    messages TEXT NOT NULL,
                    context_embeddings BLOB,
                    topic TEXT,
                    sentiment TEXT,
                    importance_score REAL,
                    created_at TEXT,
                    last_updated TEXT
                )
            ''')
            
            # í•™ìŠµ í”¼ë“œë°± í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_feedbacks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    conversation_id TEXT NOT NULL,
                    message_id TEXT NOT NULL,
                    feedback_type TEXT NOT NULL,
                    feedback_score REAL NOT NULL,
                    user_correction TEXT,
                    context_relevance REAL,
                    timestamp TEXT NOT NULL
                )
            ''')
            
            # ì‚¬ìš©ì í”„ë¡œí•„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_profiles (
                    user_id TEXT PRIMARY KEY,
                    name TEXT,
                    preferences TEXT,
                    conversation_count INTEGER DEFAULT 0,
                    avg_satisfaction REAL DEFAULT 0.5,
                    topics_of_interest TEXT,
                    last_interaction TEXT
                )
            ''')
            
            conn.commit()
    
    def create_conversation_context(self, user_id: str, initial_message: str) -> ConversationContext:
        """ìƒˆ ëŒ€í™” ë§¥ë½ ìƒì„±"""
        conversation_id = f"conv_{int(time.time())}_{hash(user_id) % 10000}"
        
        context = ConversationContext(
            conversation_id=conversation_id,
            user_id=user_id,
            messages=[{"role": "user", "content": initial_message}],
            context_embeddings=None,
            topic="general",
            sentiment="neutral",
            importance_score=0.5,
            created_at=datetime.now().isoformat(),
            last_updated=datetime.now().isoformat()
        )
        
        self.short_term_memory[conversation_id] = context
        
        # ì‚¬ìš©ìë³„ ì‘ì—… ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        if user_id not in self.working_memory:
            self.working_memory[user_id] = []
        
        self.working_memory[user_id].append(context)
        if len(self.working_memory[user_id]) > self.max_working_memory:
            self.working_memory[user_id].pop(0)
        
        return context
    
    def update_conversation_context(self, conversation_id: str, message: Dict[str, str],
                                  topic: str = None, sentiment: str = None):
        """ëŒ€í™” ë§¥ë½ ì—…ë°ì´íŠ¸"""
        if conversation_id in self.short_term_memory:
            context = self.short_term_memory[conversation_id]
            context.messages.append(message)
            context.last_updated = datetime.now().isoformat()
            
            if topic:
                context.topic = topic
            if sentiment:
                context.sentiment = sentiment
            
            # ì¤‘ìš”ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸
            context.importance_score = self._calculate_importance_score(context)
    
    def _calculate_importance_score(self, context: ConversationContext) -> float:
        """ëŒ€í™” ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ëŒ€í™” ê¸¸ì´
        message_count = len(context.messages)
        score += min(message_count * 0.1, 0.3)
        
        # ì£¼ì œë³„ ê°€ì¤‘ì¹˜
        topic_weights = {
            "unity": 0.9,
            "csharp": 0.8,
            "programming": 0.7,
            "error": 0.8,
            "general": 0.5
        }
        score += topic_weights.get(context.topic, 0.5) * 0.2
        
        # ê°ì •ë³„ ê°€ì¤‘ì¹˜
        sentiment_weights = {
            "positive": 0.1,
            "negative": 0.2,  # ë¶€ì •ì  ê°ì •ì€ ë” ì¤‘ìš”
            "frustrated": 0.3,
            "satisfied": 0.1,
            "neutral": 0.0
        }
        score += sentiment_weights.get(context.sentiment, 0.0)
        
        return min(score, 1.0)
    
    def get_context_memory(self, user_id: str, conversation_id: str = None) -> Optional[torch.Tensor]:
        """ë§¥ë½ ë©”ëª¨ë¦¬ ë²¡í„° ë°˜í™˜"""
        relevant_contexts = []
        
        if conversation_id and conversation_id in self.short_term_memory:
            relevant_contexts.append(self.short_term_memory[conversation_id])
        
        # ì‚¬ìš©ìì˜ ìµœê·¼ ëŒ€í™”ë“¤
        if user_id in self.working_memory:
            relevant_contexts.extend(self.working_memory[user_id][-3:])  # ìµœê·¼ 3ê°œ
        
        if not relevant_contexts:
            return None
        
        # ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„±
        context_vectors = []
        for context in relevant_contexts:
            if context.context_embeddings is not None:
                context_vectors.append(context.context_embeddings)
            else:
                # ê°„ë‹¨í•œ ë²¡í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ë¡œ ì¸ì½”ë”©)
                dummy_vector = torch.randn(768)  # hidden_size
                context_vectors.append(dummy_vector)
        
        if context_vectors:
            return torch.stack(context_vectors).mean(dim=0)
        
        return None
    
    def store_learning_feedback(self, feedback: LearningFeedback):
        """í•™ìŠµ í”¼ë“œë°± ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO learning_feedbacks 
                (conversation_id, message_id, feedback_type, feedback_score, 
                 user_correction, context_relevance, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                feedback.conversation_id,
                feedback.message_id,
                feedback.feedback_type,
                feedback.feedback_score,
                feedback.user_correction,
                feedback.context_relevance,
                feedback.timestamp
            ))
            conn.commit()
    
    def save_context_to_long_term(self, context: ConversationContext):
        """ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥"""
        if context.importance_score > 0.6:  # ì¤‘ìš”í•œ ëŒ€í™”ë§Œ
            # ì„ë² ë”© ì§ë ¬í™”
            embeddings_blob = None
            if context.context_embeddings is not None:
                embeddings_blob = pickle.dumps(context.context_embeddings.cpu().numpy())
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT OR REPLACE INTO conversation_contexts 
                    (conversation_id, user_id, messages, context_embeddings, 
                     topic, sentiment, importance_score, created_at, last_updated)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    context.conversation_id,
                    context.user_id,
                    json.dumps(context.messages, ensure_ascii=False),
                    embeddings_blob,
                    context.topic,
                    context.sentiment,
                    context.importance_score,
                    context.created_at,
                    context.last_updated
                ))
                conn.commit()

class RealTimeLearningEngine:
    """ì‹¤ì‹œê°„ í•™ìŠµ ì—”ì§„"""
    
    def __init__(self, model: KoreanTransformerModel, memory_system: AdvancedMemorySystem):
        self.model = model
        self.memory_system = memory_system
        self.optimizer = optim.AdamW(model.parameters(), lr=1e-5)
        self.scheduler = None
        
        # í•™ìŠµ ì„¤ì •
        self.learning_enabled = True
        self.immediate_learning_threshold = 0.7  # ì¦‰ì‹œ í•™ìŠµ ì„ê³„ê°’
        self.batch_learning_size = 10
        
        # í”¼ë“œë°± ë²„í¼
        self.feedback_buffer = []
        self.learning_stats = {
            "total_updates": 0,
            "positive_feedbacks": 0,
            "negative_feedbacks": 0,
            "corrections_applied": 0,
            "last_learning_time": None
        }
        
        logger.info("ğŸ¯ ì‹¤ì‹œê°„ í•™ìŠµ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_user_feedback(self, conversation_id: str, user_input: str, 
                            ai_response: str, user_feedback: str) -> LearningFeedback:
        """ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬"""
        
        # í”¼ë“œë°± ë¶„ì„
        feedback_score, feedback_type, correction = self._analyze_feedback(user_feedback)
        
        # ë§¥ë½ ê´€ë ¨ì„± ê³„ì‚°
        context_relevance = self._calculate_context_relevance(user_input, ai_response)
        
        # í”¼ë“œë°± ê°ì²´ ìƒì„±
        feedback = LearningFeedback(
            conversation_id=conversation_id,
            message_id=f"msg_{int(time.time())}",
            feedback_type=feedback_type,
            feedback_score=feedback_score,
            user_correction=correction,
            context_relevance=context_relevance,
            timestamp=datetime.now().isoformat()
        )
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥
        self.memory_system.store_learning_feedback(feedback)
        self.feedback_buffer.append(feedback)
        
        # ì¦‰ì‹œ í•™ìŠµ ì—¬ë¶€ ê²°ì •
        if abs(feedback_score) > self.immediate_learning_threshold:
            self._immediate_learning(feedback, user_input, ai_response)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if feedback_score > 0:
            self.learning_stats["positive_feedbacks"] += 1
        else:
            self.learning_stats["negative_feedbacks"] += 1
        
        if correction:
            self.learning_stats["corrections_applied"] += 1
        
        return feedback
    
    def _analyze_feedback(self, feedback_text: str) -> Tuple[float, str, Optional[str]]:
        """í”¼ë“œë°± ë¶„ì„"""
        feedback_lower = feedback_text.lower()
        
        # ê¸ì •ì  í”¼ë“œë°±
        positive_patterns = [
            "ì¢‹ì•„", "ë§ì•„", "ì •í™•í•´", "ë„ì›€ëì–´", "ê³ ë§ˆì›Œ", "ì˜í–ˆì–´", "í›Œë¥­í•´",
            "ì™„ë²½í•´", "ìµœê³ ", "ê°ì‚¬", "ì¢‹ì€", "ë§ìŠµë‹ˆë‹¤", "ì •í™•í•©ë‹ˆë‹¤"
        ]
        
        # ë¶€ì •ì  í”¼ë“œë°±
        negative_patterns = [
            "í‹€ë ¤", "ì•„ë‹ˆì•¼", "ì´ìƒí•´", "ë³„ë¡œ", "ë‹¤ì‹œ", "ì˜ëª»", "ë‚˜ë¹ ", "ì—‰í„°ë¦¬",
            "í‹€ë ¸", "ì˜ëª»ë", "ì´ìƒí•˜", "ë¬¸ì œ", "ì˜¤ë¥˜"
        ]
        
        # ìˆ˜ì • íŒ¨í„´
        correction_patterns = [
            "ì•„ë‹ˆë¼", "ëŒ€ì‹ ", "ì •í™•íˆëŠ”", "ì‚¬ì‹¤ì€", "ì‹¤ì œë¡œëŠ”", "ì˜¬ë°”ë¥¸", "ìˆ˜ì •"
        ]
        
        score = 0.0
        feedback_type = "neutral"
        correction = None
        
        # ê¸ì •ì  í”¼ë“œë°± í™•ì¸
        positive_count = sum(1 for pattern in positive_patterns if pattern in feedback_lower)
        if positive_count > 0:
            score = min(1.0, positive_count * 0.3 + 0.4)
            feedback_type = "positive"
        
        # ë¶€ì •ì  í”¼ë“œë°± í™•ì¸
        negative_count = sum(1 for pattern in negative_patterns if pattern in feedback_lower)
        if negative_count > 0:
            score = -min(1.0, negative_count * 0.3 + 0.4)
            feedback_type = "negative"
        
        # ìˆ˜ì • ì‚¬í•­ í™•ì¸
        correction_count = sum(1 for pattern in correction_patterns if pattern in feedback_lower)
        if correction_count > 0:
            feedback_type = "correction"
            correction = feedback_text  # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •ìœ¼ë¡œ ê°„ì£¼
            score = -0.5  # ìˆ˜ì •ì€ ì•½ê°„ ë¶€ì •ì 
        
        return score, feedback_type, correction
    
    def _calculate_context_relevance(self, user_input: str, ai_response: str) -> float:
        """ë§¥ë½ ê´€ë ¨ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ê³„ì‚°
        user_words = set(user_input.lower().split())
        response_words = set(ai_response.lower().split())
        
        if not user_words:
            return 0.0
        
        # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨
        common_words = user_words.intersection(response_words)
        relevance = len(common_words) / len(user_words)
        
        return min(relevance, 1.0)
    
    def _immediate_learning(self, feedback: LearningFeedback, user_input: str, ai_response: str):
        """ì¦‰ì‹œ í•™ìŠµ ì‹¤í–‰"""
        if not self.learning_enabled:
            return
        
        logger.info(f"ğŸ¯ ì¦‰ì‹œ í•™ìŠµ ì‹œì‘: {feedback.feedback_type} (ì ìˆ˜: {feedback.feedback_score:.2f})")
        
        try:
            # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
            self.model.train()
            
            # ì…ë ¥ê³¼ ì‘ë‹µì„ ëª¨ë¸ì— í†µê³¼
            outputs = self.model(user_input)
            
            # í”¼ë“œë°± ê¸°ë°˜ ì†ì‹¤ ê³„ì‚°
            if feedback.feedback_type == "positive":
                # ê¸ì •ì  í”¼ë“œë°±: í’ˆì§ˆ ì ìˆ˜ë¥¼ ë†’ì´ë„ë¡ í•™ìŠµ
                target_quality = torch.tensor([[1.0]])
                quality_loss = F.mse_loss(outputs["quality_score"], target_quality)
                loss = quality_loss
                
            elif feedback.feedback_type == "negative":
                # ë¶€ì •ì  í”¼ë“œë°±: í’ˆì§ˆ ì ìˆ˜ë¥¼ ë‚®ì¶”ë„ë¡ í•™ìŠµ
                target_quality = torch.tensor([[0.0]])
                quality_loss = F.mse_loss(outputs["quality_score"], target_quality)
                loss = quality_loss
                
            elif feedback.feedback_type == "correction" and feedback.user_correction:
                # ìˆ˜ì • í”¼ë“œë°±: ì˜¬ë°”ë¥¸ ì‘ë‹µì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í’ˆì§ˆ ì ìˆ˜ë¥¼ ì¡°ì •
                target_quality = torch.tensor([[0.3]])  # ì¤‘ê°„ ì •ë„ í’ˆì§ˆ
                quality_loss = F.mse_loss(outputs["quality_score"], target_quality)
                loss = quality_loss
            
            else:
                return  # í•™ìŠµí•  ê²ƒì´ ì—†ìŒ
            
            # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            self.learning_stats["total_updates"] += 1
            self.learning_stats["last_learning_time"] = datetime.now().isoformat()
            
            logger.info(f"âœ… ì¦‰ì‹œ í•™ìŠµ ì™„ë£Œ: ì†ì‹¤={loss.item():.4f}")
            
        except Exception as e:
            logger.error(f"âŒ ì¦‰ì‹œ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def batch_learning(self):
        """ë°°ì¹˜ í•™ìŠµ ì‹¤í–‰"""
        if not self.learning_enabled or len(self.feedback_buffer) < self.batch_learning_size:
            return
        
        logger.info(f"ğŸ“š ë°°ì¹˜ í•™ìŠµ ì‹œì‘: {len(self.feedback_buffer)}ê°œ í”¼ë“œë°±")
        
        try:
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch_feedbacks = self.feedback_buffer[-self.batch_learning_size:]
            
            total_loss = 0.0
            valid_updates = 0
            
            for feedback in batch_feedbacks:
                # ê° í”¼ë“œë°±ì— ëŒ€í•´ í•™ìŠµ ìˆ˜í–‰ (ê°„ì†Œí™”)
                if abs(feedback.feedback_score) > 0.3:  # ì˜ë¯¸ìˆëŠ” í”¼ë“œë°±ë§Œ
                    # ì—¬ê¸°ì„œ ì‹¤ì œ í•™ìŠµ ë¡œì§ êµ¬í˜„
                    # í˜„ì¬ëŠ” í†µê³„ë§Œ ì—…ë°ì´íŠ¸
                    valid_updates += 1
            
            if valid_updates > 0:
                self.learning_stats["total_updates"] += valid_updates
                self.learning_stats["last_learning_time"] = datetime.now().isoformat()
                
                # ì²˜ë¦¬ëœ í”¼ë“œë°± ì œê±°
                self.feedback_buffer = self.feedback_buffer[self.batch_learning_size:]
                
                logger.info(f"âœ… ë°°ì¹˜ í•™ìŠµ ì™„ë£Œ: {valid_updates}ê°œ ì—…ë°ì´íŠ¸")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def get_learning_stats(self) -> Dict:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return self.learning_stats.copy()

class AdvancedAutoCI:
    """ê³ ê¸‰ AutoCI ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_path: str = "advanced_autoci_model.pth"):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model = KoreanTransformerModel()
        self.memory_system = AdvancedMemorySystem()
        self.learning_engine = RealTimeLearningEngine(self.model, self.memory_system)
        
        # í˜„ì¬ ëŒ€í™” ìƒíƒœ
        self.current_conversations = {}  # user_id -> conversation_id
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model()
        
        logger.info("ğŸ¤– ê³ ê¸‰ AutoCI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(self.model_path):
            try:
                checkpoint = torch.load(self.model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint["model_state_dict"])
                self.learning_engine.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
                self.learning_engine.learning_stats = checkpoint.get("learning_stats", self.learning_engine.learning_stats)
                logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘")
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            checkpoint = {
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.learning_engine.optimizer.state_dict(),
                "learning_stats": self.learning_engine.learning_stats,
                "timestamp": datetime.now().isoformat()
            }
            
            torch.save(checkpoint, self.model_path)
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_path}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def chat(self, user_id: str, user_input: str) -> Tuple[str, str]:
        """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
        
        # í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        if user_id not in self.current_conversations:
            context = self.memory_system.create_conversation_context(user_id, user_input)
            self.current_conversations[user_id] = context.conversation_id
        else:
            conversation_id = self.current_conversations[user_id]
            context = self.memory_system.short_term_memory.get(conversation_id)
            if context:
                self.memory_system.update_conversation_context(
                    conversation_id, 
                    {"role": "user", "content": user_input}
                )
        
        # ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
        context_memory = self.memory_system.get_context_memory(user_id, self.current_conversations[user_id])
        
        # ì‘ë‹µ ìƒì„±
        ai_response, confidence = self.model.generate_response(user_input, context_memory)
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        if context:
            self.memory_system.update_conversation_context(
                context.conversation_id,
                {"role": "assistant", "content": ai_response}
            )
        
        return ai_response, self.current_conversations[user_id]
    
    def process_feedback(self, user_id: str, conversation_id: str, 
                        user_input: str, ai_response: str, feedback: str) -> bool:
        """í”¼ë“œë°± ì²˜ë¦¬"""
        try:
            feedback_obj = self.learning_engine.process_user_feedback(
                conversation_id, user_input, ai_response, feedback
            )
            
            logger.info(f"ğŸ“ í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ: {feedback_obj.feedback_type} ({feedback_obj.feedback_score:.2f})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("\nğŸ¤– ê³ ê¸‰ AutoCIì™€ ëŒ€í™”í•˜ê¸°")
        print("ì¢…ë£Œ: 'quit', í”¼ë“œë°±: 'feedback', ìƒíƒœ: 'status'")
        print("=" * 60)
        
        user_id = "user_001"  # ë°ëª¨ìš© ê³ ì • ì‚¬ìš©ì ID
        last_response = ""
        last_conversation_id = ""
        last_user_input = ""
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ì‚¬ìš©ì: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                
                elif user_input.lower() == 'status':
                    stats = self.learning_engine.get_learning_stats()
                    print(f"\nğŸ“Š í•™ìŠµ í†µê³„:")
                    for key, value in stats.items():
                        print(f"  {key}: {value}")
                    continue
                
                elif user_input.lower() == 'feedback' and last_response:
                    feedback_input = input("í”¼ë“œë°±ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if feedback_input:
                        success = self.process_feedback(
                            user_id, last_conversation_id, 
                            last_user_input, last_response, feedback_input
                        )
                        if success:
                            print("âœ… í”¼ë“œë°±ì´ í•™ìŠµì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        else:
                            print("âŒ í”¼ë“œë°± ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # AI ì‘ë‹µ ìƒì„±
                ai_response, conversation_id = self.chat(user_id, user_input)
                print(f"\nğŸ¤– AutoCI: {ai_response}")
                
                # ë‹¤ìŒ í”¼ë“œë°±ì„ ìœ„í•´ ì €ì¥
                last_response = ai_response
                last_conversation_id = conversation_id
                last_user_input = user_input
                
                # ìë™ í”¼ë“œë°± ìš”ì²­ (ê°€ë”)
                import random
                if random.random() < 0.3:  # 30% í™•ë¥ 
                    print("\nğŸ’¡ ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”? (ì¢‹ì•„/ë³„ë¡œ/í‹€ë ¤)")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ëª¨ë¸ ì €ì¥
        self.save_model()
        print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AutoCI ê³ ê¸‰ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í•™ìŠµ AI")
    print("=" * 60)
    
    if not TRANSFORMERS_AVAILABLE:
        print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹: pip install transformers torch")
        print("ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
    
    try:
        # ê³ ê¸‰ AutoCI ì´ˆê¸°í™”
        autoci = AdvancedAutoCI()
        
        # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì‹œì‘
        autoci.interactive_chat()
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())