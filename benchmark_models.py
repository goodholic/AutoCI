#!/usr/bin/env python3
"""
AutoCI v3.0 - AI ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import psutil
from datetime import datetime
from pathlib import Path

print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...")

try:
    import torch
    print("âœ… PyTorch ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("ğŸ“¦ ì„¤ì¹˜: pip install torch")
    sys.exit(1)

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    print("âœ… Transformers ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âŒ Transformersê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("ğŸ“¦ ì„¤ì¹˜: pip install transformers")
    sys.exit(1)

try:
    import nvidia_ml_py3 as nvml
    gpu_support = True
    print("âœ… NVIDIA GPU ì§€ì› ê°€ëŠ¥")
except ImportError:
    gpu_support = False
    print("âš ï¸ NVIDIA GPU ëª¨ë‹ˆí„°ë§ ë¶ˆê°€ (nvidia-ml-py3 ë¯¸ì„¤ì¹˜)")

class ModelBenchmark:
    """AI ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = {}
        self.test_prompts = [
            "ê°„ë‹¨í•œ 2D í”Œë«í¬ë¨¸ ê²Œì„ì„ ë§Œë“¤ì–´ì¤˜",
            "C# ìŠ¤í¬ë¦½íŠ¸ë¡œ í”Œë ˆì´ì–´ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì‘ì„±í•´ì¤˜", 
            "ê²Œì„ì— ì í”„ ë©”ì»¤ë‹ˆì¦˜ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜",
            "ìœ ë‹ˆí‹°ì—ì„œ ì  AIë¥¼ êµ¬í˜„í•˜ëŠ” ì½”ë“œë¥¼ ë³´ì—¬ì¤˜",
            "í•œêµ­ì–´ë¡œ ê²Œì„ ê°œë°œ íŒì„ ì„¤ëª…í•´ì¤˜"
        ]
        
        # GPU ì •ë³´ ì´ˆê¸°í™”
        self.gpu_available = False
        self.gpu_count = 0
        
        if gpu_support:
            try:
                nvml.nvmlInit()
                self.gpu_available = True
                self.gpu_count = nvml.nvmlDeviceGetCount()
            except:
                self.gpu_available = False
    
    def get_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total // (1024**3),  # GB
            "memory_available": psutil.virtual_memory().available // (1024**3),  # GB
            "gpu_available": self.gpu_available,
            "gpu_count": self.gpu_count,
            "torch_version": torch.__version__,
            "python_version": sys.version.split()[0],
            "cuda_available": torch.cuda.is_available()
        }
        
        if self.gpu_available and gpu_support:
            gpu_info = []
            for i in range(self.gpu_count):
                try:
                    handle = nvml.nvmlDeviceGetHandleByIndex(i)
                    name = nvml.nvmlDeviceGetName(handle).decode()
                    memory = nvml.nvmlDeviceGetMemoryInfo(handle)
                    gpu_info.append({
                        "name": name,
                        "memory_total": memory.total // (1024**2),  # MB
                        "memory_free": memory.free // (1024**2)   # MB
                    })
                except Exception as e:
                    gpu_info.append({"error": str(e)})
            info["gpu_info"] = gpu_info
        
        return info
    
    def find_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°"""
        models = {}
        
        # ê¸°ë³¸ ëª¨ë¸ í™•ì¸
        if os.path.exists("CodeLlama-7b-Instruct-hf"):
            models["CodeLlama-7B"] = {
                "path": "CodeLlama-7b-Instruct-hf",
                "size": "7B",
                "type": "ì½”ë”© ì „ë¬¸"
            }
        
        # ê³ ê¸‰ ëª¨ë¸ë“¤ í™•ì¸
        model_paths = {
            "Llama-3.1-70B": {
                "path": "models/Llama-3.1-70B-Instruct",
                "size": "70B", 
                "type": "ê³ ê¸‰ ì¶”ë¡ "
            },
            "Qwen2.5-72B": {
                "path": "models/Qwen2.5-72B-Instruct",
                "size": "72B",
                "type": "í•œêµ­ì–´ ìµœì í™”"
            },
            "DeepSeek-V2.5": {
                "path": "models/DeepSeek-V2.5", 
                "size": "236B",
                "type": "ì½”ë”© ì „ë¬¸"
            }
        }
        
        for name, info in model_paths.items():
            if os.path.exists(info["path"]):
                models[name] = info
        
        return models
    
    def benchmark_model(self, model_name, model_path):
        """ê°œë³„ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ§  {model_name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        result = {
            "model_name": model_name,
            "model_path": model_path,
            "tests": [],
            "errors": []
        }
        
        try:
            # ëª¨ë¸ ë¡œë”© ì‹œê°„ ì¸¡ì •
            load_start = time.time()
            
            print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘ (4-bit ì–‘ìí™”)...")
            
            # ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥¸ ë¡œë”© ì „ëµ ì‚¬ìš©
            if "7B" in model_name:
                # ì‘ì€ ëª¨ë¸ì€ ì¼ë°˜ ë¡œë”©
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
            else:
                # í° ëª¨ë¸ì€ 4-bit ì–‘ìí™”
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    load_in_4bit=True
                )
            
            load_time = time.time() - load_start
            result["load_time"] = load_time
            
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            memory_used = psutil.virtual_memory().used // (1024**3)  # GB
            result["memory_used_gb"] = memory_used
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ (ì‹œê°„ ì ˆì•½)
            test_prompt = self.test_prompts[0]  # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸: {test_prompt[:30]}...")
            
            test_result = self.run_single_test(model, tokenizer, test_prompt)
            result["tests"].append(test_result)
            result["average_response_time"] = test_result["response_time"]
            result["average_tokens_per_second"] = test_result["tokens_per_second"]
            
            print(f"   ì‘ë‹µ ì‹œê°„: {test_result['response_time']:.2f}ì´ˆ")
            print(f"   í† í° ì†ë„: {test_result['tokens_per_second']:.1f} tokens/sec")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_used}GB")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            del tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            error_msg = f"âŒ {model_name} ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {str(e)}"
            print(error_msg)
            result["errors"].append(error_msg)
        
        return result
    
    def run_single_test(self, model, tokenizer, prompt):
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            # í† í°í™”
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.to(model.device)
            
            input_length = inputs.shape[1]
            
            # ìƒì„± (ì§§ê²Œ)
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=input_length + 20,  # 20ê°œ í† í°ë§Œ ìƒì„± (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
                    do_sample=False,  # ê²°ì •ì  ìƒì„±
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            response_time = time.time() - start_time
            
            # ìƒì„±ëœ í† í° ìˆ˜ ê³„ì‚°
            generated_length = outputs.shape[1] - input_length
            tokens_per_second = generated_length / response_time if response_time > 0 else 0
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
            generated_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
            
            return {
                "prompt": prompt,
                "response_time": response_time,
                "generated_tokens": generated_length,
                "tokens_per_second": tokens_per_second,
                "generated_text": generated_text[:50] + "..." if len(generated_text) > 50 else generated_text
            }
            
        except Exception as e:
            return {
                "prompt": prompt,
                "response_time": 999.0,
                "generated_tokens": 0,
                "tokens_per_second": 0.0,
                "error": str(e)
            }
    
    def run_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ AutoCI v3.0 - AI ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 50)
        print()
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
        system_info = self.get_system_info()
        print("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU: {system_info['cpu_count']}ì½”ì–´")
        print(f"   ë©”ëª¨ë¦¬: {system_info['memory_total']}GB (ì‚¬ìš© ê°€ëŠ¥: {system_info['memory_available']}GB)")
        print(f"   CUDA: {'ì‚¬ìš© ê°€ëŠ¥' if system_info['cuda_available'] else 'ì‚¬ìš© ë¶ˆê°€'}")
        
        if system_info['gpu_available'] and 'gpu_info' in system_info:
            for i, gpu in enumerate(system_info['gpu_info']):
                if 'error' not in gpu:
                    print(f"   GPU {i}: {gpu['name']} ({gpu['memory_total']}MB)")
        print()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°
        available_models = self.find_available_models()
        
        if not available_models:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
            print("   ./download_advanced_models.sh")
            print("   ë˜ëŠ” ê¸°ë³¸ ëª¨ë¸: git clone https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf")
            return
        
        print(f"ğŸ“‹ ë°œê²¬ëœ ëª¨ë¸: {len(available_models)}ê°œ")
        for name, info in available_models.items():
            print(f"   â€¢ {name} ({info['size']}) - {info['type']}")
        print()
        
        # ê° ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark_results = {
            "timestamp": datetime.now().isoformat(),
            "system_info": system_info,
            "model_results": []
        }
        
        for model_name, model_info in available_models.items():
            result = self.benchmark_model(model_name, model_info["path"])
            result.update(model_info)
            benchmark_results["model_results"].append(result)
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(benchmark_results)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_summary(benchmark_results)
    
    def save_results(self, results):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_results_{timestamp}.json"
        
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filename}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def print_summary(self, results):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        model_results = results["model_results"]
        
        if not model_results:
            print("âŒ ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (í† í°/ì´ˆ ê¸°ì¤€)
        successful_results = [r for r in model_results if "average_tokens_per_second" in r and r["average_tokens_per_second"] > 0]
        if successful_results:
            successful_results.sort(key=lambda x: x["average_tokens_per_second"], reverse=True)
        
        if successful_results:
            print("\nğŸ† ì„±ëŠ¥ ìˆœìœ„ (í† í°/ì´ˆ ê¸°ì¤€):")
            for i, result in enumerate(successful_results, 1):
                print(f"{i}. {result['model_name']} ({result['size']})")
                print(f"   â€¢ í† í° ì†ë„: {result['average_tokens_per_second']:.1f} tokens/sec")
                print(f"   â€¢ ì‘ë‹µ ì‹œê°„: {result['average_response_time']:.2f}ì´ˆ")
                print(f"   â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©: {result['memory_used_gb']}GB")
                print(f"   â€¢ ë¡œë”© ì‹œê°„: {result['load_time']:.2f}ì´ˆ")
                print(f"   â€¢ íŠ¹í™” ë¶„ì•¼: {result['type']}")
                print()
        
        # ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤
        failed_results = [r for r in model_results if r.get("errors")]
        if failed_results:
            print("âŒ ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for result in failed_results:
                print(f"   â€¢ {result['model_name']}: {result['errors'][0] if result['errors'] else 'Unknown error'}")
            print()
        
        # ê¶Œì¥ì‚¬í•­
        print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        memory_total = results["system_info"]["memory_total"]
        
        if successful_results:
            best_model = successful_results[0]
            print(f"   â€¢ ìµœê³  ì„±ëŠ¥: {best_model['model_name']} ({best_model['average_tokens_per_second']:.1f} tokens/sec)")
        
        if memory_total >= 32:
            print("   â€¢ 32GB+ RAM: ëª¨ë“  ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
        elif memory_total >= 16:
            print("   â€¢ 16GB RAM: ê¸°ë³¸ ëª¨ë¸(CodeLlama-7B) ë˜ëŠ” 4-bit ì–‘ìí™” ëª¨ë¸ ê¶Œì¥")
        else:
            print("   â€¢ 16GB ë¯¸ë§Œ RAM: ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”")
        
        print("\nğŸš€ AutoCI ì‹œì‘ ëª…ë ¹ì–´:")
        if successful_results:
            print("   source autoci_env/bin/activate")
            print("   python start_autoci_agent.py --advanced-models")
        else:
            print("   ë¨¼ì € ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ./download_advanced_models.sh")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help"]:
        print("AutoCI v3.0 - AI ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("ì‚¬ìš©ë²•: python benchmark_models.py")
        print("")
        print("ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìš´ë¡œë“œëœ AI ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        print("í…ŒìŠ¤íŠ¸ í•­ëª©:")
        print("  â€¢ ëª¨ë¸ ë¡œë”© ì‹œê°„")
        print("  â€¢ ì‘ë‹µ ìƒì„± ì†ë„")
        print("  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        print("  â€¢ í† í° ìƒì„± ì†ë„")
        print("")
        print("âš ï¸ ì£¼ì˜: í° ëª¨ë¸ë“¤ì€ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì‹œìŠ¤í…œ ì‚¬ì–‘ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    benchmark = ModelBenchmark()
    benchmark.run_benchmark()

if __name__ == "__main__":
    main() 