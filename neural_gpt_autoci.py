#!/usr/bin/env python3
"""
Neural GPT AutoCI - ì™„ì „í•œ ì‹ ê²½ë§ ê¸°ë°˜ AutoCI
ê·œì¹™ ê¸°ë°˜/íŒ¨í„´ ë§¤ì¹­ ì™„ì „ ì œê±°, ìˆœìˆ˜ ì‹ ê²½ë§ë§Œ ì‚¬ìš©
ChatGPT ìˆ˜ì¤€ì˜ ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„° íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
"""

import os
import sys
import time
import json
import math
import logging
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
import pickle
import threading
import queue

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import DataLoader, Dataset, DistributedSampler
    from torch.nn.parallel import DistributedDataParallel as DDP
    import torch.distributed as dist
    from transformers import (
        AutoTokenizer, AutoModel, AutoConfig,
        GPT2LMHeadModel, GPT2Config,
        get_linear_schedule_with_warmup,
        DataCollatorForLanguageModeling
    )
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch/Transformers í•„ìˆ˜! ì„¤ì¹˜: pip install torch transformers")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """ì‹ ê²½ë§ ëª¨ë¸ ì„¤ì •"""
    vocab_size: int = 50000          # ì–´íœ˜ í¬ê¸°
    hidden_size: int = 4096          # ì€ë‹‰ì¸µ í¬ê¸° (GPT-3: 12288)
    num_layers: int = 32             # ë ˆì´ì–´ ìˆ˜ (GPT-3: 96)
    num_heads: int = 32              # ì–´í…ì…˜ í—¤ë“œ ìˆ˜ (GPT-3: 96)
    intermediate_size: int = 16384   # FFN ì¤‘ê°„ì¸µ í¬ê¸° (GPT-3: 49152)
    max_position_embeddings: int = 2048  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    dropout: float = 0.1             # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    layer_norm_epsilon: float = 1e-5
    
    # í•™ìŠµ ì„¤ì •
    learning_rate: float = 1e-4
    warmup_steps: int = 10000
    max_steps: int = 1000000
    batch_size: int = 8              # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
    gradient_accumulation_steps: int = 32
    
    @property
    def total_parameters(self) -> int:
        """ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        # ì„ë² ë”©: vocab_size * hidden_size + max_pos * hidden_size
        embeddings = self.vocab_size * self.hidden_size + self.max_position_embeddings * self.hidden_size
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡: ê° ë¸”ë¡ë‹¹ ì•½ 12 * hidden_size^2
        # (attention: 4 * hidden_size^2, ffn: 8 * hidden_size^2)
        transformer_blocks = self.num_layers * 12 * self.hidden_size * self.hidden_size
        
        # ì¶œë ¥ ë ˆì´ì–´: hidden_size * vocab_size
        output_layer = self.hidden_size * self.vocab_size
        
        total = embeddings + transformer_blocks + output_layer
        return total

class MultiHeadAttention(nn.Module):
    """ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (ChatGPT ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_heads
        self.head_dim = config.hidden_size // config.num_heads
        
        assert self.head_dim * config.num_heads == config.hidden_size
        
        # Query, Key, Value í”„ë¡œì ì…˜
        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        
        self.dropout = nn.Dropout(config.dropout)
        self.scale = math.sqrt(self.head_dim)
        
        # ì¸ê³¼ì  ë§ˆìŠ¤í¬ (GPT ìŠ¤íƒ€ì¼)
        self.register_buffer(
            "causal_mask",
            torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings))
            .view(1, 1, config.max_position_embeddings, config.max_position_embeddings)
        )
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = hidden_states.size()
        
        # Q, K, V ê³„ì‚°
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)
        
        # ë©€í‹°í—¤ë“œë¡œ reshape
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°
        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale
        
        # ì¸ê³¼ì  ë§ˆìŠ¤í¬ ì ìš© (GPT ìŠ¤íƒ€ì¼)
        causal_mask = self.causal_mask[:, :, :seq_len, :seq_len]
        attention_scores = attention_scores.masked_fill(causal_mask == 0, -1e9)
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # ì–´í…ì…˜ ì ìš©
        context = torch.matmul(attention_weights, value)
        
        # í—¤ë“œ ê²°í•©
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.out_proj(context)
        
        return output

class FeedForward(nn.Module):
    """í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (ChatGPT ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # GELU í™œì„±í™” í•¨ìˆ˜ (GPT ìŠ¤íƒ€ì¼)
        x = self.linear1(x)
        x = F.gelu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ (ChatGPT ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)
        self.ln1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
        self.ln2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # Pre-LayerNorm (GPT ìŠ¤íƒ€ì¼)
        residual = hidden_states
        hidden_states = self.ln1(hidden_states)
        hidden_states = self.attention(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = residual + hidden_states
        
        # FFN
        residual = hidden_states
        hidden_states = self.ln2(hidden_states)
        hidden_states = self.feed_forward(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = residual + hidden_states
        
        return hidden_states

class NeuralGPTAutoCI(nn.Module):
    """ì™„ì „í•œ ì‹ ê²½ë§ ê¸°ë°˜ AutoCI (ê·œì¹™ ì—†ìŒ)"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # í† í° ì„ë² ë”©
        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.num_layers)
        ])
        
        # ìµœì¢… ë ˆì´ì–´ ì •ê·œí™”
        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)
        
        # ì¶œë ¥ í—¤ë“œ (ë‹¤ìŒ í† í° ì˜ˆì¸¡)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
        
        # ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ§  ì‹ ê²½ë§ ì´ˆê¸°í™”: {total_params:,} íŒŒë¼ë¯¸í„° ({total_params/1e9:.1f}B)")
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (GPT ìŠ¤íƒ€ì¼)"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len = input_ids.size()
        
        # ìœ„ì¹˜ ID ìƒì„±
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # ì„ë² ë”©
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        hidden_states = token_embeds + position_embeds
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ë“¤ í†µê³¼
        for transformer_block in self.transformer_blocks:
            hidden_states = transformer_block(hidden_states)
        
        # ìµœì¢… ì •ê·œí™”
        hidden_states = self.final_layer_norm(hidden_states)
        
        # ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë¡œì§“
        logits = self.lm_head(hidden_states)
        
        return logits
    
    def generate(self, tokenizer, prompt: str, max_length: int = 256, 
                temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:
        """ìˆœìˆ˜ ì‹ ê²½ë§ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± (ê·œì¹™ ì—†ìŒ)"""
        
        self.eval()
        device = next(self.parameters()).device
        
        # í”„ë¡¬í”„íŠ¸ í† í°í™”
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            for _ in range(max_length):
                # ëª¨ë¸ ì˜ˆì¸¡
                logits = self.forward(input_ids)
                next_token_logits = logits[0, -1, :] / temperature
                
                # Top-k í•„í„°ë§
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, -float('inf'))
                    next_token_logits[top_k_indices] = top_k_logits
                
                # Top-p (nucleus) ìƒ˜í”Œë§
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    # top_p ì„ê³„ê°’ ì´í›„ í† í°ë“¤ ì œê±°
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    indices_to_remove = sorted_indices[sorted_indices_to_remove]
                    next_token_logits[indices_to_remove] = -float('inf')
                
                # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # ì‹œí€€ìŠ¤ì— ì¶”ê°€
                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)
                
                # ì¢…ë£Œ í† í° í™•ì¸
                if next_token.item() == tokenizer.eos_token_id:
                    break
                
                # ìµœëŒ€ ê¸¸ì´ ì œí•œ
                if input_ids.size(1) >= self.config.max_position_embeddings:
                    break
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
        response = generated_text[len(prompt):].strip()
        
        return response

class LargeScaleDataset(Dataset):
    """ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_files: List[str], tokenizer, max_length: int = 1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        logger.info("ğŸ“š ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        for data_file in data_files:
            self._load_data_file(data_file)
        
        logger.info(f"âœ… ì´ {len(self.examples):,} ê°œ í•™ìŠµ ì˜ˆì œ ë¡œë“œë¨")
    
    def _load_data_file(self, file_path: str):
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {file_path}")
            return
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line_num % 10000 == 0:
                        logger.info(f"ğŸ“– {file_path}: {line_num:,} ì¤„ ì²˜ë¦¬ë¨")
                    
                    try:
                        data = json.loads(line.strip())
                        if self._is_valid_conversation(data):
                            text = self._format_conversation(data)
                            if text:
                                self.examples.append(text)
                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        if line_num % 1000 == 0:  # ê°€ë”ë§Œ ë¡œê·¸
                            logger.warning(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜ (ì¤„ {line_num}): {e}")
                        continue
        
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
    
    def _is_valid_conversation(self, data: Dict) -> bool:
        """ìœ íš¨í•œ ëŒ€í™” ë°ì´í„°ì¸ì§€ í™•ì¸"""
        return (
            isinstance(data, dict) and
            'user_message' in data and
            'ai_response' in data and
            isinstance(data['user_message'], str) and
            isinstance(data['ai_response'], str) and
            len(data['user_message'].strip()) > 0 and
            len(data['ai_response'].strip()) > 0 and
            len(data['user_message']) < 1000 and  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
            len(data['ai_response']) < 2000
        )
    
    def _format_conversation(self, data: Dict) -> Optional[str]:
        """ëŒ€í™”ë¥¼ í•™ìŠµìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        user_msg = data['user_message'].strip()
        ai_msg = data['ai_response'].strip()
        
        # ChatGPT ìŠ¤íƒ€ì¼ í¬ë§·
        formatted = f"<|user|>{user_msg}<|assistant|>{ai_msg}<|endoftext|>"
        
        return formatted
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        text = self.examples[idx]
        
        # í† í°í™”
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        input_ids = encoding.input_ids.squeeze()
        attention_mask = encoding.attention_mask.squeeze()
        
        # ë ˆì´ë¸”ì€ input_idsì™€ ë™ì¼ (ì–¸ì–´ ëª¨ë¸ë§)
        labels = input_ids.clone()
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

class NeuralTrainer:
    """ìˆœìˆ˜ ì‹ ê²½ë§ í•™ìŠµê¸° (ê·œì¹™ ì—†ìŒ)"""
    
    def __init__(self, model: NeuralGPTAutoCI, config: ModelConfig, tokenizer):
        self.model = model
        self.config = config
        self.tokenizer = tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        self.model = self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            betas=(0.9, 0.95),
            weight_decay=0.1
        )
        
        # í•™ìŠµ í†µê³„
        self.train_stats = {
            "step": 0,
            "epoch": 0,
            "total_loss": 0.0,
            "best_loss": float('inf'),
            "learning_rate": config.learning_rate
        }
        
        logger.info(f"ğŸ¯ ì‹ ê²½ë§ í•™ìŠµê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def create_dataloader(self, dataset: LargeScaleDataset, batch_size: int = None) -> DataLoader:
        """ë°ì´í„°ë¡œë” ìƒì„±"""
        if batch_size is None:
            batch_size = self.config.batch_size
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> float:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
        self.model.train()
        
        # ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™
        input_ids = batch["input_ids"].to(self.device)
        attention_mask = batch["attention_mask"].to(self.device)
        labels = batch["labels"].to(self.device)
        
        # ìˆœì „íŒŒ
        logits = self.model(input_ids, attention_mask)
        
        # ì†ì‹¤ ê³„ì‚° (ë‹¤ìŒ í† í° ì˜ˆì¸¡)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        loss = F.cross_entropy(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1),
            ignore_index=self.tokenizer.pad_token_id
        )
        
        # ì—­ì „íŒŒ
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return loss.item()
    
    def train_epoch(self, dataloader: DataLoader) -> float:
        """ì „ì²´ ì—í¬í¬ í•™ìŠµ"""
        total_loss = 0.0
        num_batches = len(dataloader)
        
        logger.info(f"ğŸš€ ì—í¬í¬ {self.train_stats['epoch']} í•™ìŠµ ì‹œì‘ ({num_batches:,} ë°°ì¹˜)")
        
        for batch_idx, batch in enumerate(dataloader):
            # í•™ìŠµ ìŠ¤í…
            loss = self.train_step(batch)
            total_loss += loss
            
            self.train_stats["step"] += 1
            self.train_stats["total_loss"] += loss
            
            # ë¡œê·¸ ì¶œë ¥
            if batch_idx % 100 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                logger.info(f"ë°°ì¹˜ {batch_idx:,}/{num_batches:,}: ì†ì‹¤={loss:.4f}, í‰ê· ={avg_loss:.4f}")
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if self.train_stats["step"] % 1000 == 0:
                self.save_checkpoint()
            
            # ìµœëŒ€ ìŠ¤í… ì²´í¬
            if self.train_stats["step"] >= self.config.max_steps:
                logger.info(f"âœ… ìµœëŒ€ ìŠ¤í… ({self.config.max_steps:,}) ë„ë‹¬")
                break
        
        avg_epoch_loss = total_loss / num_batches
        self.train_stats["epoch"] += 1
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if avg_epoch_loss < self.train_stats["best_loss"]:
            self.train_stats["best_loss"] = avg_epoch_loss
            self.save_best_model()
        
        logger.info(f"âœ… ì—í¬í¬ ì™„ë£Œ: í‰ê·  ì†ì‹¤={avg_epoch_loss:.4f}")
        
        return avg_epoch_loss
    
    def save_checkpoint(self, checkpoint_path: str = None):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if checkpoint_path is None:
            checkpoint_path = f"neural_autoci_checkpoint_step_{self.train_stats['step']}.pt"
        
        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "config": self.config,
            "train_stats": self.train_stats,
            "step": self.train_stats["step"]
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def save_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_model_path = f"neural_autoci_best_model.pt"
        self.save_checkpoint(best_model_path)
        logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_model_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not os.path.exists(checkpoint_path):
            logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
            return False
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            self.train_stats = checkpoint["train_stats"]
            
            logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            logger.info(f"   ìŠ¤í…: {self.train_stats['step']:,}")
            logger.info(f"   ìµœê³  ì†ì‹¤: {self.train_stats['best_loss']:.4f}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

class PureNeuralAutoCI:
    """ì™„ì „í•œ ìˆœìˆ˜ ì‹ ê²½ë§ AutoCI (ê·œì¹™ ì™„ì „ ì œê±°)"""
    
    def __init__(self, model_path: str = None):
        self.config = ModelConfig()
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = self._create_tokenizer()
        
        # ì‹ ê²½ë§ ëª¨ë¸ ì´ˆê¸°í™”
        self.model = NeuralGPTAutoCI(self.config)
        
        # í•™ìŠµê¸° ì´ˆê¸°í™”
        self.trainer = NeuralTrainer(self.model, self.config, self.tokenizer)
        
        # ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if model_path and os.path.exists(model_path):
            self.trainer.load_checkpoint(model_path)
        
        # ëŒ€í™” ê¸°ë¡
        self.conversation_history = []
        
        logger.info("ğŸ¤– ìˆœìˆ˜ ì‹ ê²½ë§ AutoCI ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“Š ëª¨ë¸ í¬ê¸°: {self.config.total_parameters:,} íŒŒë¼ë¯¸í„°")
    
    def _create_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ìƒì„±"""
        try:
            # GPT-2 í† í¬ë‚˜ì´ì € ì‚¬ìš© (í•œêµ­ì–´ ì§€ì›)
            tokenizer = AutoTokenizer.from_pretrained("gpt2")
            
            # íŠ¹ìˆ˜ í† í° ì¶”ê°€
            special_tokens = {
                "pad_token": "<|pad|>",
                "eos_token": "<|endoftext|>",
                "unk_token": "<|unk|>",
                "additional_special_tokens": ["<|user|>", "<|assistant|>"]
            }
            
            tokenizer.add_special_tokens(special_tokens)
            
            # ì–´íœ˜ í¬ê¸° ì—…ë°ì´íŠ¸
            self.config.vocab_size = len(tokenizer)
            
            logger.info(f"âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”: ì–´íœ˜ í¬ê¸° {len(tokenizer):,}")
            
            return tokenizer
            
        except Exception as e:
            logger.error(f"âŒ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê°„ë‹¨í•œ í´ë°± í† í¬ë‚˜ì´ì €
            return self._create_simple_tokenizer()
    
    def _create_simple_tokenizer(self):
        """ê°„ë‹¨í•œ í´ë°± í† í¬ë‚˜ì´ì €"""
        class SimpleTokenizer:
            def __init__(self):
                self.vocab = {"<|pad|>": 0, "<|unk|>": 1, "<|endoftext|>": 2, "<|user|>": 3, "<|assistant|>": 4}
                self.pad_token_id = 0
                self.eos_token_id = 2
                
            def encode(self, text, return_tensors=None):
                # ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜ ì¸ì½”ë”©
                tokens = [ord(c) % 1000 + 5 for c in text[:100]]  # ìµœëŒ€ 100ì
                if return_tensors == "pt":
                    return torch.tensor([tokens])
                return tokens
            
            def decode(self, tokens, skip_special_tokens=False):
                if isinstance(tokens, torch.Tensor):
                    tokens = tokens.tolist()
                # ê°„ë‹¨í•œ ë””ì½”ë”©
                try:
                    text = ''.join(chr(max(32, t-5)) for t in tokens if t >= 5)
                    return text[:200]  # ìµœëŒ€ 200ì
                except:
                    return "ìƒì„±ëœ ì‘ë‹µ"
            
            def __call__(self, text, truncation=True, max_length=512, padding="max_length", return_tensors=None):
                tokens = self.encode(text)[:max_length]
                
                if padding == "max_length":
                    while len(tokens) < max_length:
                        tokens.append(self.pad_token_id)
                
                attention_mask = [1 if t != self.pad_token_id else 0 for t in tokens]
                
                result = {
                    "input_ids": torch.tensor([tokens]) if return_tensors == "pt" else tokens,
                    "attention_mask": torch.tensor([attention_mask]) if return_tensors == "pt" else attention_mask
                }
                
                return result
            
            def __len__(self):
                return 50000
        
        logger.info("âš ï¸ í´ë°± í† í¬ë‚˜ì´ì € ì‚¬ìš©")
        return SimpleTokenizer()
    
    def chat(self, user_input: str) -> str:
        """ìˆœìˆ˜ ì‹ ê²½ë§ ê¸°ë°˜ ëŒ€í™” (ê·œì¹™ ì—†ìŒ)"""
        
        # ëŒ€í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"<|user|>{user_input}<|assistant|>"
        
        try:
            # ìˆœìˆ˜ ì‹ ê²½ë§ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            response = self.model.generate(
                self.tokenizer,
                prompt,
                max_length=200,
                temperature=0.8,
                top_k=50,
                top_p=0.9
            )
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.conversation_history.append({
                "user_input": user_input,
                "ai_response": response,
                "timestamp": datetime.now().isoformat(),
                "generation_method": "pure_neural"
            })
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ ì‹ ê²½ë§ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸´ê¸‰ í´ë°± (ë§¤ìš° ê°„ë‹¨)
            return "ì‹ ê²½ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def train_on_data(self, data_files: List[str], epochs: int = 1):
        """ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‹ ê²½ë§ í•™ìŠµ"""
        logger.info(f"ğŸš€ ëŒ€ê·œëª¨ ì‹ ê²½ë§ í•™ìŠµ ì‹œì‘: {epochs} ì—í¬í¬")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = LargeScaleDataset(data_files, self.tokenizer)
        
        if len(dataset) == 0:
            logger.error("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        dataloader = self.trainer.create_dataloader(dataset)
        
        # í•™ìŠµ ì‹¤í–‰
        for epoch in range(epochs):
            logger.info(f"ğŸ“š ì—í¬í¬ {epoch+1}/{epochs} ì‹œì‘")
            
            avg_loss = self.trainer.train_epoch(dataloader)
            
            logger.info(f"âœ… ì—í¬í¬ {epoch+1} ì™„ë£Œ: í‰ê·  ì†ì‹¤ {avg_loss:.4f}")
            
            # ì¤‘ê°„ ì €ì¥
            if (epoch + 1) % 5 == 0:
                self.trainer.save_checkpoint(f"neural_autoci_epoch_{epoch+1}.pt")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self.trainer.save_best_model()
        logger.info("ğŸ‰ ì‹ ê²½ë§ í•™ìŠµ ì™„ë£Œ!")
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("\nğŸ¤– ìˆœìˆ˜ ì‹ ê²½ë§ AutoCI (ê·œì¹™ ì—†ìŒ)")
        print("=" * 60)
        print("ì™„ì „í•œ ì‹ ê²½ë§ ê¸°ë°˜ AIì™€ ëŒ€í™”í•˜ì„¸ìš”!")
        print("ì¢…ë£Œ: 'quit', í•™ìŠµ: 'train', ìƒíƒœ: 'status'")
        print("=" * 60)
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ë‹¹ì‹ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                
                elif user_input.lower() == 'status':
                    print(f"\nğŸ“Š ëª¨ë¸ ìƒíƒœ:")
                    print(f"  íŒŒë¼ë¯¸í„°: {self.config.total_parameters:,}")
                    print(f"  ë ˆì´ì–´: {self.config.num_layers}")
                    print(f"  í—¤ë“œ: {self.config.num_heads}")
                    print(f"  ì€ë‹‰ í¬ê¸°: {self.config.hidden_size}")
                    print(f"  ëŒ€í™” ê¸°ë¡: {len(self.conversation_history)}")
                    continue
                
                elif user_input.lower() == 'train':
                    print("ğŸš€ ì‹ ê²½ë§ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ë°ì´í„° íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    print("   ì˜ˆ: data_files = ['conversations.jsonl', 'korean_qa.jsonl']")
                    continue
                
                # ìˆœìˆ˜ ì‹ ê²½ë§ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
                print("ğŸ§  ì‹ ê²½ë§ì´ ì‘ë‹µì„ ìƒì„± ì¤‘...")
                response = self.chat(user_input)
                print(f"\nğŸ¤– Neural AutoCI: {response}")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print(f"\nğŸ‘‹ ìˆœìˆ˜ ì‹ ê²½ë§ AutoCIë¥¼ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!")

def create_training_data():
    """í•™ìŠµìš© ë°ì´í„° ìƒì„± ì˜ˆì‹œ"""
    sample_data = [
        {
            "user_message": "Unityì—ì„œ GameObjectë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ai_response": "Unityì—ì„œ GameObjectë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ Instantiate ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. GameObject prefabì´ ìˆë‹¤ë©´ Instantiate(prefab)ë¡œ ìƒì„±í•  ìˆ˜ ìˆê³ , ìƒˆë¡œìš´ ë¹ˆ GameObjectë¥¼ ë§Œë“¤ë ¤ë©´ new GameObject(\"ì´ë¦„\")ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."
        },
        {
            "user_message": "C#ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ai_response": "C#ì—ì„œ List<T>ëŠ” ë™ì  ë°°ì—´ì…ë‹ˆë‹¤. List<int> numbers = new List<int>();ë¡œ ìƒì„±í•˜ê³ , Add()ë¡œ ìš”ì†Œ ì¶”ê°€, Remove()ë¡œ ì œê±°, Countë¡œ ê°œìˆ˜ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. foreachë‚˜ for ë£¨í”„ë¡œ ìˆœíšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
    ]
    
    with open("sample_training_data.jsonl", "w", encoding="utf-8") as f:
        for data in sample_data:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    logger.info("ğŸ“ ìƒ˜í”Œ í•™ìŠµ ë°ì´í„° ìƒì„±ë¨: sample_training_data.jsonl")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìˆœìˆ˜ ì‹ ê²½ë§ AutoCI (ê·œì¹™ ì™„ì „ ì œê±°)")
    print("=" * 60)
    
    if not TORCH_AVAILABLE:
        print("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        print("ì„¤ì¹˜: pip install torch transformers")
        return 1
    
    try:
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        create_training_data()
        
        # ìˆœìˆ˜ ì‹ ê²½ë§ AutoCI ì´ˆê¸°í™”
        autoci = PureNeuralAutoCI()
        
        # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì‹œì‘
        autoci.interactive_chat()
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())