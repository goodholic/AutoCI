#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ì‹ ê²½ë§ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (PyTorch ì—†ì´)
"""

import sys
import os
import time
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

@dataclass
class SimpleConversationData:
    """ê°„ë‹¨í•œ ëŒ€í™” ë°ì´í„° êµ¬ì¡°"""
    user_input: str
    ai_response: str
    feedback_score: float  # -1.0 to 1.0
    timestamp: str

class SimpleNeuralNetwork:
    """ê°„ë‹¨í•œ ì‹ ê²½ë§ (NumPy ê¸°ë°˜)"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 5, output_size: int = 1):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier ì´ˆê¸°í™”)
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros((1, output_size))
        
        self.learning_rate = 0.01
    
    def sigmoid(self, x):
        """ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def sigmoid_derivative(self, x):
        """ì‹œê·¸ëª¨ì´ë“œ ë¯¸ë¶„"""
        return x * (1 - x)
    
    def forward(self, X):
        """ìˆœì „íŒŒ"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def backward(self, X, y, output):
        """ì—­ì „íŒŒ"""
        m = X.shape[0]
        
        # ì¶œë ¥ì¸µ ì˜¤ì°¨
        dz2 = output - y
        dW2 = (1/m) * np.dot(self.a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)
        
        # ì€ë‹‰ì¸µ ì˜¤ì°¨
        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def train(self, X, y, epochs=100):
        """í•™ìŠµ"""
        losses = []
        for epoch in range(epochs):
            output = self.forward(X)
            loss = np.mean((output - y) ** 2)
            losses.append(loss)
            self.backward(X, y, output)
        return losses
    
    def predict(self, X):
        """ì˜ˆì¸¡"""
        return self.forward(X)

class SimpleLearningAutoCI:
    """ê°„ë‹¨í•œ í•™ìŠµ AutoCI ì‹œìŠ¤í…œ (PyTorch ì—†ì´)"""
    
    def __init__(self):
        self.neural_network = SimpleNeuralNetwork()
        self.conversations: List[SimpleConversationData] = []
        self.learning_enabled = True
        
        # í†µê³„
        self.stats = {
            "total_conversations": 0,
            "total_training_epochs": 0,
            "average_quality_score": 0.0,
            "last_learning_time": None,
            "model_accuracy": 0.0
        }
        
        # ê°„ë‹¨í•œ ë‹¨ì–´ ì„ë² ë”© (í•´ì‹œ ê¸°ë°˜)
        self.vocab = {}
        self.vocab_size = 0
        
        print("ğŸ§  ê°„ë‹¨í•œ í•™ìŠµ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def text_to_vector(self, text: str, vector_size: int = 10) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜)"""
        words = text.lower().split()
        vector = np.zeros(vector_size)
        
        for i, word in enumerate(words[:vector_size]):
            # ë‹¨ì–´ì˜ í•´ì‹œë¥¼ ë²¡í„° ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©
            word_hash = hash(word) % vector_size
            vector[word_hash] = 1.0
            
            # ë‹¨ì–´ ê¸¸ì´ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
            vector[word_hash] += len(word) * 0.1
            vector[word_hash] += (i + 1) * 0.01
        
        # ì •ê·œí™”
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            
        return vector
    
    def estimate_feedback_score(self, user_input: str, ai_response: str) -> float:
        """í”¼ë“œë°± ì ìˆ˜ ìë™ ì¶”ì •"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¶”ì •
        score = 0.0
        
        # ê¸ì •ì  í‚¤ì›Œë“œ
        positive_keywords = ["ì¢‹ì•„", "ë§ì•„", "ì •í™•", "ë„ì›€", "ê³ ë§ˆì›Œ", "ì˜í–ˆì–´", "í›Œë¥­"]
        negative_keywords = ["í‹€ë ¤", "ì•„ë‹ˆì•¼", "ì´ìƒí•´", "ë³„ë¡œ", "ë‹¤ì‹œ", "ì˜ëª»", "ë‚˜ë¹ "]
        
        user_lower = user_input.lower()
        response_lower = ai_response.lower()
        
        # ì‚¬ìš©ì ì…ë ¥ ë¶„ì„
        for keyword in positive_keywords:
            if keyword in user_lower:
                score += 0.2
        
        for keyword in negative_keywords:
            if keyword in user_lower:
                score -= 0.2
        
        # ì‘ë‹µ ê¸¸ì´ ë¶„ì„ (ì ì ˆí•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
        response_length = len(ai_response)
        if 10 <= response_length <= 100:
            score += 0.1
        elif response_length < 5 or response_length > 200:
            score -= 0.1
        
        # ìœ ë‹ˆí‹°/C# ê´€ë ¨ í‚¤ì›Œë“œ
        unity_keywords = ["unity", "ìœ ë‹ˆí‹°", "c#", "script", "ìŠ¤í¬ë¦½íŠ¸", "ê²Œì„", "ì˜¤ë¸Œì íŠ¸"]
        if any(keyword in user_lower for keyword in unity_keywords):
            if any(keyword in response_lower for keyword in unity_keywords):
                score += 0.1
        
        return np.clip(score, -1.0, 1.0)
    
    def process_conversation(self, user_input: str, ai_response: str, 
                           feedback_score: Optional[float] = None) -> Dict:
        """ëŒ€í™” ì²˜ë¦¬ ë° í•™ìŠµ"""
        
        # í”¼ë“œë°± ì ìˆ˜ ê³„ì‚°
        if feedback_score is None:
            feedback_score = self.estimate_feedback_score(user_input, ai_response)
        
        # ëŒ€í™” ë°ì´í„° ìƒì„±
        conversation = SimpleConversationData(
            user_input=user_input,
            ai_response=ai_response,
            feedback_score=feedback_score,
            timestamp=datetime.now().isoformat()
        )
        
        self.conversations.append(conversation)
        self.stats["total_conversations"] += 1
        
        # í•™ìŠµ íŠ¸ë¦¬ê±° (ê°•í•œ í”¼ë“œë°±ì´ ìˆì„ ë•Œ)
        if self.learning_enabled and abs(feedback_score) > 0.3:
            self.trigger_learning()
        
        return {
            "conversation_id": len(self.conversations),
            "feedback_score": feedback_score,
            "learning_triggered": abs(feedback_score) > 0.3
        }
    
    def trigger_learning(self):
        """í•™ìŠµ íŠ¸ë¦¬ê±°"""
        if len(self.conversations) < 5:
            return
        
        print("ğŸ”„ ì‹ ê²½ë§ í•™ìŠµ ì‹œì‘...")
        
        # ìµœê·¼ ëŒ€í™”ë“¤ë¡œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        recent_conversations = self.conversations[-10:]
        
        X = []
        y = []
        
        for conv in recent_conversations:
            input_vector = self.text_to_vector(conv.user_input)
            X.append(input_vector)
            
            # í”¼ë“œë°± ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
            normalized_score = (conv.feedback_score + 1.0) / 2.0
            y.append([normalized_score])
        
        if len(X) > 0:
            X = np.array(X)
            y = np.array(y)
            
            # ì‹ ê²½ë§ í•™ìŠµ
            losses = self.neural_network.train(X, y, epochs=50)
            
            self.stats["total_training_epochs"] += 50
            self.stats["last_learning_time"] = datetime.now().isoformat()
            
            final_loss = losses[-1] if losses else 0.0
            print(f"âœ… í•™ìŠµ ì™„ë£Œ - ìµœì¢… ì†ì‹¤: {final_loss:.4f}")
    
    def generate_response(self, user_input: str) -> Tuple[str, float]:
        """ì‘ë‹µ ìƒì„±"""
        
        # ì‹ ê²½ë§ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ì˜ˆì¸¡
        try:
            input_vector = self.text_to_vector(user_input)
            quality_score = self.neural_network.predict(input_vector.reshape(1, -1))[0][0]
            
            # 0-1 ë²”ìœ„ë¥¼ -1~1 ë²”ìœ„ë¡œ ë³€í™˜
            confidence = (quality_score - 0.5) * 2.0
            
        except Exception as e:
            quality_score = 0.5
            confidence = 0.0
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        user_lower = user_input.lower()
        
        if any(word in user_lower for word in ["ì•ˆë…•", "hello", "hi"]):
            response = "ì•ˆë…•í•˜ì„¸ìš”! AutoCIì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
        elif any(word in user_lower for word in ["unity", "ìœ ë‹ˆí‹°"]):
            response = "Unity ê°œë°œì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
        elif any(word in user_lower for word in ["ì½”ë“œ", "code", "ìŠ¤í¬ë¦½íŠ¸"]):
            response = "ì½”ë“œë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”."
        elif any(word in user_lower for word in ["ì˜¤ë¥˜", "ì—ëŸ¬", "error"]):
            response = "ì˜¤ë¥˜ë¥¼ í•´ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ì˜¤ë¥˜ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”."
        elif any(word in user_lower for word in ["ê³ ë§ˆì›Œ", "ê°ì‚¬"]):
            response = "ì²œë§Œì—ìš”! ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
        else:
            response = "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return response, abs(confidence)
    
    def get_learning_status(self) -> Dict:
        """í•™ìŠµ ìƒíƒœ ë°˜í™˜"""
        return {
            "stats": self.stats,
            "total_conversations": len(self.conversations),
            "neural_network_weights": {
                "W1_shape": self.neural_network.W1.shape,
                "W2_shape": self.neural_network.W2.shape
            }
        }

def test_simple_neural_learning():
    """ê°„ë‹¨í•œ ì‹ ê²½ë§ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê°„ë‹¨í•œ ì‹ ê²½ë§ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ai = SimpleLearningAutoCI()
    print("âœ… AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
    
    # í…ŒìŠ¤íŠ¸ ëŒ€í™”ë“¤
    test_conversations = [
        ("ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì„¸ìš”! AutoCIì…ë‹ˆë‹¤.", 1.0),
        ("Unity ë„ì›€ì´ í•„ìš”í•´ìš”", "Unity ê°œë°œì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!", 0.8),
        ("ì½”ë“œ ë¶„ì„í•´ì£¼ì„¸ìš”", "ì½”ë“œë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", 0.6),
        ("ì´ìƒí•œ ì‘ë‹µì´ë„¤ìš”", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤.", -0.5),
        ("ê³ ë§ˆì›Œìš”", "ì²œë§Œì—ìš”! ë„ì›€ì´ ë˜ì–´ì„œ ê¸°ë»ìš”!", 0.9),
        ("Unityì—ì„œ ì˜¤ë¸Œì íŠ¸ ìƒì„±í•˜ëŠ” ë°©ë²•", "Instantiate í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.", 0.7),
        ("ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜ê°€ ìˆì–´ìš”", "ì–´ë–¤ ì˜¤ë¥˜ì¸ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", 0.5),
        ("ì˜ëª»ëœ ë‹µë³€", "ì£„ì†¡í•©ë‹ˆë‹¤. ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤.", -0.3)
    ]
    
    print(f"\nğŸ—£ï¸ {len(test_conversations)}ê°œ í…ŒìŠ¤íŠ¸ ëŒ€í™” ì²˜ë¦¬ ì¤‘...")
    
    # ëŒ€í™” ì²˜ë¦¬ ë° í•™ìŠµ
    for i, (user_input, ai_response, feedback) in enumerate(test_conversations, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}/{len(test_conversations)}:")
        print(f"ì‚¬ìš©ì: {user_input}")
        print(f"AI: {ai_response}")
        print(f"í”¼ë“œë°±: {feedback}")
        
        result = ai.process_conversation(user_input, ai_response, feedback)
        
        print(f"âœ… ëŒ€í™” ID: {result['conversation_id']}")
        print(f"âœ… í•™ìŠµ íŠ¸ë¦¬ê±°: {result['learning_triggered']}")
        
        # ì ì‹œ ëŒ€ê¸°
        time.sleep(0.5)
    
    print(f"\nğŸ“Š ìµœì¢… í•™ìŠµ ìƒíƒœ:")
    status = ai.get_learning_status()
    for key, value in status["stats"].items():
        print(f"  {key}: {value}")
    
    print(f"\nğŸ¤– ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸:")
    test_inputs = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "Unity ìŠ¤í¬ë¦½íŠ¸ ë„ì›€",
        "ì½”ë“œ ì˜¤ë¥˜ í•´ê²°",
        "ê°ì‚¬í•©ë‹ˆë‹¤"
    ]
    
    for test_input in test_inputs:
        response, confidence = ai.generate_response(test_input)
        print(f"ì…ë ¥: {test_input}")
        print(f"ì‘ë‹µ: {response}")
        print(f"ì‹ ë¢°ë„: {confidence:.2f}\n")
    
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("âœ… ê°„ë‹¨í•œ ì‹ ê²½ë§ ê¸°ë°˜ í•™ìŠµ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
    
    return ai

if __name__ == "__main__":
    test_simple_neural_learning()