#!/usr/bin/env python3
"""
autoci learn low ëª…ë ¹ì–´ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""

import sys
import json
import asyncio
from pathlib import Path

# continuous_learning_system ì„í¬íŠ¸
try:
    from continuous_learning_system import ContinuousLearningSystem
    print("âœ… continuous_learning_system ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    sys.exit(1)

async def test_autoci_learn_low():
    """autoci learn low ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª autoci learn low í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    try:
        # ContinuousLearningSystem ì´ˆê¸°í™” (RTX 2080 ì„¤ì •)
        system = ContinuousLearningSystem(
            models_dir="./models",
            learning_dir="./continuous_learning", 
            max_memory_gb=24.0  # RTX 2080 + 32GB RAM ìµœì í™”
        )
        
        print("ğŸ” ëª¨ë¸ ì •ë³´ ë¡œë“œ ì¤‘...")
        system.load_model_info()
        
        # DeepSeek-coder ìƒíƒœ ìš°ì„  í™•ì¸
        deepseek_status = system.available_models.get("deepseek-coder-7b", {}).get('status', 'not_found')
        if deepseek_status == 'installed':
            print("ğŸ”¥ DeepSeek-coder-v2 6.7B: ì„¤ì¹˜ë¨ âœ…")
            print("   â†’ 5ê°€ì§€ í•µì‹¬ ì£¼ì œ ìµœì í™” í•™ìŠµ ê°€ëŠ¥!")
        else:
            print("âš ï¸  DeepSeek-coder-v2 6.7B: ë¯¸ì„¤ì¹˜ âŒ")
            print("   â†’ python download_deepseek_coder.pyë¡œ ì„¤ì¹˜í•˜ì„¸ìš”")
        
        print("\nğŸ“‹ ì „ì²´ ëª¨ë¸ ëª©ë¡:")
        for model_name, info in system.available_models.items():
            status = info.get('status', 'unknown')
            rtx_opt = info.get('rtx_2080_optimized', False)
            
            # DeepSeek-coder ê°•ì¡°
            if model_name == "deepseek-coder-7b":
                prefix = "ğŸ”¥ [5ê°€ì§€ í•µì‹¬ ì£¼ì œ íŠ¹í™”]"
            elif rtx_opt:
                prefix = "ğŸ¯ [RTX 2080 ìµœì í™”]"
            else:
                prefix = "âš ï¸"
            
            print(f"  - {prefix} {model_name}: {status}")
        
        # ì„¤ì¹˜ëœ ëª¨ë¸ ì°¾ê¸°
        installed_models = [
            name for name, info in system.available_models.items() 
            if info.get('status') == 'installed'
        ]
        
        if not installed_models:
            print("âŒ ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            print("ğŸ’¡ ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   python install_llm_models_rtx2080.py")
            return False
        
        print(f"âœ… ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(installed_models)}")
        
        # ì²« ë²ˆì§¸ ì„¤ì¹˜ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        test_model = installed_models[0]
        print(f"ğŸ§ª {test_model} ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸...")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        if system.load_model(test_model):
            print(f"âœ… {test_model} ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        else:
            print(f"âŒ {test_model} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # ê°„ë‹¨í•œ ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ í…ŒìŠ¤íŠ¸
        print("ğŸ’­ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        
        test_question = {
            "id": "test_001",
            "topic": "C# ê¸°ì´ˆ",
            "question": "C#ì—ì„œ async/awaitë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "language": "korean",
            "type": "explain",
            "difficulty": 2
        }
        
        print(f"â“ ì§ˆë¬¸: {test_question['question']}")
        
        # ëª¨ë¸ì—ê²Œ ì§ˆë¬¸
        answer = await system.ask_model(test_model, test_question)
        
        if answer and answer.get('success', False):
            response = answer.get('response', 'No response')
            print("âœ… ë‹µë³€ ìƒì„± ì„±ê³µ!")
            print("ğŸ“ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
            print("-" * 40)
            print(response[:300] + "..." if len(response) > 300 else response)
            print("-" * 40)
        else:
            print("âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
            print(f"ì˜¤ë¥˜: {answer.get('error', 'Unknown error')}")
            return False
        
        print("\nğŸ‰ autoci learn low ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("âœ… í˜„ì¬ ì‹œìŠ¤í…œì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_deepseek_availability():
    """DeepSeek-coder ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
    print("\nğŸ” DeepSeek-coder ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸...")
    
    models_file = Path("models/installed_models.json")
    if not models_file.exists():
        print("âŒ models/installed_models.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        with open(models_file, 'r', encoding='utf-8') as f:
            models = json.load(f)
        
        deepseek_info = models.get('deepseek-coder-7b', {})
        status = deepseek_info.get('status', 'unknown')
        
        print(f"ğŸ“‹ DeepSeek-coder ìƒíƒœ: {status}")
        
        if status == 'installed':
            print("âœ… DeepSeek-coderê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            return True
        elif status == 'not_downloaded':
            print("â³ DeepSeek-coderê°€ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("\nğŸ’¡ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
            print("1. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥):")
            print("   python download_deepseek_coder.py")
            print("\n2. ê°€ìƒí™˜ê²½ì—ì„œ ì§ì ‘:")
            print("   source autoci_env/bin/activate  # Linux/WSL")
            print("   autoci_env\\Scripts\\activate     # Windows")
            print("   pip install transformers torch")
            print("   python download_deepseek_coder.py")
            return False
        else:
            print("â“ DeepSeek-coder ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ AutoCI Learn Low ìƒíƒœ í™•ì¸")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("1ï¸âƒ£ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    success = asyncio.run(test_autoci_learn_low())
    
    if success:
        print("\nâœ… autoci learn lowëŠ” í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ë¡œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    else:
        print("\nâŒ autoci learn lowì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # 2. DeepSeek-coder í™•ì¸
    print("\n2ï¸âƒ£ DeepSeek-coder í™•ì¸")
    deepseek_ready = check_deepseek_availability()
    
    # 3. ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    if success and deepseek_ready:
        print("ğŸ‰ ëª¨ë“  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ‘‰ autoci learn low ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    elif success and not deepseek_ready:
        print("âœ… ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‘ë™í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ DeepSeek-coderë¥¼ ì„¤ì¹˜í•˜ë©´ ë” ë‚˜ì€ ì½”ë”© ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("ğŸ‘‰ python download_deepseek_coder.py")
    else:
        print("âŒ ë¨¼ì € ëª¨ë¸ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ğŸ‘‰ python install_llm_models_rtx2080.py")
    
    return success

if __name__ == "__main__":
    main() 