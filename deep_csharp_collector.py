#!/usr/bin/env python3
"""
Ïã¨Ï∏µ C# Ï†ÑÎ¨∏Í∞Ä Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÍ∏∞
Ïã§Ï†ú ÏõπÏóêÏÑú Í≥†ÌíàÏßà C# Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßë
"""

import os
import sys
import json
import time
import asyncio
import aiohttp
import aiofiles
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
import hashlib
import re
from bs4 import BeautifulSoup
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import sqlite3
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('deep_csharp_collector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DeepCSharpCollector:
    """Ïã¨Ï∏µ C# Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÍ∏∞"""
    
    def __init__(self, output_dir: str = "expert_learning_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÎîîÎ†âÌÜ†Î¶¨
        self.categories = {
            'microsoft_docs': {
                'path': self.output_dir / 'microsoft_docs',
                'base_url': 'https://docs.microsoft.com/en-us/dotnet/csharp/',
                'topics': [
                    'programming-guide/concepts/async/',
                    'programming-guide/concepts/linq/',
                    'programming-guide/concepts/expression-trees/',
                    'programming-guide/generics/',
                    'programming-guide/delegates/',
                    'programming-guide/events/',
                    'programming-guide/indexers/',
                    'programming-guide/interfaces/',
                    'language-reference/operators/',
                    'language-reference/keywords/',
                    'language-reference/attributes/',
                    'whats-new/csharp-10',
                    'whats-new/csharp-11',
                    'tutorials/pattern-matching',
                    'tutorials/nullable-reference-types'
                ]
            },
            'github_samples': {
                'path': self.output_dir / 'github_samples',
                'repos': [
                    {'owner': 'dotnet', 'repo': 'samples'},
                    {'owner': 'dotnet', 'repo': 'docs'},
                    {'owner': 'dotnet', 'repo': 'AspNetCore.Docs'},
                    {'owner': 'microsoft', 'repo': 'dotnet-samples'},
                    {'owner': 'Azure-Samples', 'repo': 'cognitive-services-speech-sdk'},
                    {'owner': 'Unity-Technologies', 'repo': 'EntityComponentSystemSamples'},
                    {'owner': 'dotnet', 'repo': 'machinelearning-samples'},
                    {'owner': 'dotnet', 'repo': 'blazor-samples'}
                ]
            },
            'nuget_packages': {
                'path': self.output_dir / 'nuget_packages',
                'packages': [
                    'Newtonsoft.Json',
                    'Microsoft.EntityFrameworkCore',
                    'Serilog',
                    'AutoMapper',
                    'FluentValidation',
                    'Polly',
                    'MediatR',
                    'Dapper',
                    'NUnit',
                    'xunit',
                    'Moq',
                    'FluentAssertions',
                    'BenchmarkDotNet',
                    'MessagePack',
                    'StackExchange.Redis'
                ]
            },
            'stackoverflow_advanced': {
                'path': self.output_dir / 'stackoverflow_advanced',
                'tags': [
                    'c#+async-await',
                    'c#+performance',
                    'c#+memory-management',
                    'c#+linq',
                    'c#+entity-framework-core',
                    'c#+dependency-injection',
                    'c#+design-patterns',
                    'c#+unit-testing',
                    'c#+blazor',
                    'c#+grpc'
                ],
                'min_score': 50,
                'min_views': 10000
            },
            'expert_blogs': {
                'path': self.output_dir / 'expert_blogs',
                'sources': [
                    {'name': 'Jon Skeet', 'url': 'https://codeblog.jonskeet.uk/'},
                    {'name': 'Eric Lippert', 'url': 'https://ericlippert.com/'},
                    {'name': 'Stephen Cleary', 'url': 'https://blog.stephencleary.com/'},
                    {'name': 'Steve Gordon', 'url': 'https://www.stevejgordon.co.uk/'},
                    {'name': 'Andrew Lock', 'url': 'https://andrewlock.net/'},
                    {'name': 'Nick Chapsas', 'url': 'https://www.youtube.com/@nickchapsas'},
                    {'name': 'David Fowler', 'url': 'https://twitter.com/davidfowl'}
                ]
            },
            'performance_guides': {
                'path': self.output_dir / 'performance_guides',
                'sources': [
                    'https://docs.microsoft.com/en-us/dotnet/framework/performance/',
                    'https://github.com/dotnet/performance',
                    'https://devblogs.microsoft.com/dotnet/category/performance/'
                ]
            },
            'design_patterns': {
                'path': self.output_dir / 'design_patterns',
                'patterns': [
                    'Singleton', 'Factory Method', 'Abstract Factory', 'Builder', 'Prototype',
                    'Adapter', 'Bridge', 'Composite', 'Decorator', 'Facade', 'Flyweight', 'Proxy',
                    'Chain of Responsibility', 'Command', 'Iterator', 'Mediator', 'Memento',
                    'Observer', 'State', 'Strategy', 'Template Method', 'Visitor',
                    'Repository', 'Unit of Work', 'Specification', 'CQRS', 'Event Sourcing'
                ]
            },
            'unity_best_practices': {
                'path': self.output_dir / 'unity_best_practices',
                'topics': [
                    'MonoBehaviour lifecycle',
                    'Object pooling',
                    'Coroutines vs async',
                    'Performance optimization',
                    'Memory management',
                    'ScriptableObjects',
                    'Event systems',
                    'Input system',
                    'Addressables',
                    'DOTS/ECS'
                ]
            }
        }
        
        # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        for category_info in self.categories.values():
            category_info['path'].mkdir(exist_ok=True)
            
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî
        self.init_database()
        
        # ÏÑ∏ÏÖò ÏÑ§Ï†ï
        self.session = None
        
        # ÏàòÏßë ÌÜµÍ≥Ñ
        self.stats = {
            'total_collected': 0,
            'total_size_mb': 0,
            'categories': {},
            'errors': [],
            'start_time': datetime.now()
        }
        
    def init_database(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS collected_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            category TEXT,
            subcategory TEXT,
            title TEXT,
            content_type TEXT,
            file_path TEXT,
            size_bytes INTEGER,
            quality_score REAL,
            collected_at TIMESTAMP,
            metadata TEXT
        )
        ''')
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS code_samples (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            data_id INTEGER,
            language TEXT,
            code TEXT,
            description TEXT,
            tags TEXT,
            FOREIGN KEY (data_id) REFERENCES collected_data(id)
        )
        ''')
        
        conn.commit()
        conn.close()
        
    async def collect_all_async(self):
        """ÎπÑÎèôÍ∏∞Î°ú Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        logger.info("üöÄ Ïã¨Ï∏µ C# Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏãúÏûë...")
        
        # aiohttp ÏÑ∏ÏÖò ÏÉùÏÑ±
        async with aiohttp.ClientSession() as self.session:
            tasks = []
            
            # Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÏàòÏßë ÏûëÏóÖ
            tasks.append(self.collect_microsoft_docs_async())
            tasks.append(self.collect_github_samples_async())
            tasks.append(self.collect_nuget_packages_async())
            tasks.append(self.collect_stackoverflow_async())
            tasks.append(self.collect_expert_blogs_async())
            tasks.append(self.collect_performance_guides_async())
            tasks.append(self.collect_design_patterns_async())
            tasks.append(self.collect_unity_practices_async())
            
            # Î™®Îì† ÏûëÏóÖ Ïã§Ìñâ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Í≤∞Í≥º Ï≤òÎ¶¨
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ÏàòÏßë ÏûëÏóÖ Ïã§Ìå®: {result}")
                    self.stats['errors'].append(str(result))
                    
        # ÌÜµÍ≥Ñ Ï†ÄÏû•
        self.save_statistics()
        
    async def collect_microsoft_docs_async(self):
        """Microsoft Î¨∏ÏÑú ÏàòÏßë"""
        logger.info("üìö Microsoft Î¨∏ÏÑú ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['microsoft_docs']
        collected = 0
        
        for topic in category_info['topics']:
            try:
                url = urljoin(category_info['base_url'], topic)
                content = await self.fetch_url_async(url)
                
                if content:
                    # HTML ÌååÏã±
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Ï†úÎ™© Ï∂îÏ∂ú
                    title = soup.find('h1')
                    title_text = title.text.strip() if title else topic.replace('/', ' ').title()
                    
                    # Î≥∏Î¨∏ Ï∂îÏ∂ú
                    article = soup.find('article') or soup.find('main')
                    if article:
                        # ÏΩîÎìú ÏÉòÌîå Ï∂îÏ∂ú
                        code_samples = self.extract_code_samples(article)
                        
                        # ÌÖçÏä§Ìä∏ ÏΩòÌÖêÏ∏†
                        text_content = article.get_text(separator='\n', strip=True)
                        
                        # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Ìôî
                        doc_data = {
                            'url': url,
                            'title': title_text,
                            'topic': topic,
                            'content': text_content,
                            'code_samples': code_samples,
                            'collected_at': datetime.now().isoformat()
                        }
                        
                        # ÌååÏùº Ï†ÄÏû•
                        file_name = f"{topic.replace('/', '_')}.json"
                        file_path = category_info['path'] / file_name
                        
                        await self.save_json_async(file_path, doc_data)
                        
                        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                        self.save_to_database(
                            url=url,
                            category='microsoft_docs',
                            subcategory=topic.split('/')[0],
                            title=title_text,
                            content_type='documentation',
                            file_path=str(file_path),
                            size_bytes=len(json.dumps(doc_data)),
                            quality_score=0.95
                        )
                        
                        collected += 1
                        
                        # Í¥ÄÎ†® ÎßÅÌÅ¨ ÏàòÏßë
                        related_links = self.extract_related_links(soup, category_info['base_url'])
                        for link in related_links[:5]:  # ÏµúÎåÄ 5Í∞ú
                            await self.collect_related_doc(link, category_info)
                            
            except Exception as e:
                logger.error(f"Microsoft Î¨∏ÏÑú ÏàòÏßë Ïò§Î•ò ({topic}): {e}")
                
        self.stats['categories']['microsoft_docs'] = collected
        logger.info(f"‚úÖ Microsoft Î¨∏ÏÑú {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    def extract_code_samples(self, element) -> List[Dict]:
        """ÏΩîÎìú ÏÉòÌîå Ï∂îÏ∂ú"""
        code_samples = []
        
        # <pre><code> ÌÉúÍ∑∏ Ï∞æÍ∏∞
        for code_block in element.find_all(['pre', 'code']):
            code_text = code_block.get_text(strip=True)
            
            if len(code_text) > 50:  # ÏùòÎØ∏ÏûàÎäî ÏΩîÎìúÎßå
                # Ïñ∏Ïñ¥ Í∞êÏßÄ
                language = 'csharp'
                class_attr = code_block.get('class', [])
                if isinstance(class_attr, list):
                    for cls in class_attr:
                        if 'language-' in cls:
                            language = cls.replace('language-', '')
                            break
                            
                code_samples.append({
                    'language': language,
                    'code': code_text,
                    'lines': len(code_text.split('\n'))
                })
                
        return code_samples
        
    def extract_related_links(self, soup, base_url: str) -> List[str]:
        """Í¥ÄÎ†® ÎßÅÌÅ¨ Ï∂îÏ∂ú"""
        links = []
        
        # Í¥ÄÎ†® Î¨∏ÏÑú ÎßÅÌÅ¨ Ï∞æÍ∏∞
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú
            if href.startswith('/'):
                href = urljoin(base_url, href)
                
            # C# Í¥ÄÎ†® ÎßÅÌÅ¨Îßå
            if 'csharp' in href or 'dotnet' in href:
                if href not in links and base_url in href:
                    links.append(href)
                    
        return links
        
    async def collect_related_doc(self, url: str, category_info: Dict):
        """Í¥ÄÎ†® Î¨∏ÏÑú ÏàòÏßë"""
        try:
            # Ïù¥ÎØ∏ ÏàòÏßëÌñàÎäîÏßÄ ÌôïÏù∏
            if self.is_already_collected(url):
                return
                
            content = await self.fetch_url_async(url)
            if content:
                # Í∞ÑÎã®Ìïú Ï≤òÎ¶¨ (Î©îÏù∏ Î¨∏ÏÑúÏôÄ ÎèôÏùºÌïú Î∞©Ïãù)
                # ... (Íµ¨ÌòÑ ÏÉùÎûµ)
                pass
                
        except Exception as e:
            logger.debug(f"Í¥ÄÎ†® Î¨∏ÏÑú ÏàòÏßë Ïä§ÌÇµ: {e}")
            
    def is_already_collected(self, url: str) -> bool:
        """Ïù¥ÎØ∏ ÏàòÏßëÌñàÎäîÏßÄ ÌôïÏù∏"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT 1 FROM collected_data WHERE url = ?', (url,))
        result = cursor.fetchone()
        
        conn.close()
        return result is not None
        
    async def collect_github_samples_async(self):
        """GitHub ÏÉòÌîå ÏΩîÎìú ÏàòÏßë"""
        logger.info("üêô GitHub ÏÉòÌîå ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['github_samples']
        collected = 0
        
        for repo_info in category_info['repos']:
            try:
                # GitHub API ÏÇ¨Ïö©
                api_url = f"https://api.github.com/repos/{repo_info['owner']}/{repo_info['repo']}/contents"
                
                # Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú C# ÌååÏùº Ï∞æÍ∏∞
                cs_files = await self.find_github_cs_files(api_url)
                
                for file_info in cs_files[:50]:  # Î¶¨Ìè¨Îãπ ÏµúÎåÄ 50Í∞ú
                    try:
                        # ÌååÏùº ÎÇ¥Ïö© Í∞ÄÏ†∏Ïò§Í∏∞
                        file_content = await self.fetch_github_file(file_info['download_url'])
                        
                        if file_content:
                            # ÌååÏùº Î∂ÑÏÑù
                            analysis = self.analyze_cs_file(file_content)
                            
                            # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Ìôî
                            sample_data = {
                                'repo': f"{repo_info['owner']}/{repo_info['repo']}",
                                'file_path': file_info['path'],
                                'file_name': file_info['name'],
                                'content': file_content,
                                'analysis': analysis,
                                'size': file_info['size'],
                                'url': file_info['html_url'],
                                'collected_at': datetime.now().isoformat()
                            }
                            
                            # ÌååÏùº Ï†ÄÏû•
                            safe_name = file_info['path'].replace('/', '_')
                            file_path = category_info['path'] / f"{safe_name}.json"
                            
                            await self.save_json_async(file_path, sample_data)
                            
                            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                            self.save_to_database(
                                url=file_info['html_url'],
                                category='github_samples',
                                subcategory=repo_info['repo'],
                                title=file_info['name'],
                                content_type='source_code',
                                file_path=str(file_path),
                                size_bytes=file_info['size'],
                                quality_score=analysis['quality_score']
                            )
                            
                            collected += 1
                            
                    except Exception as e:
                        logger.debug(f"ÌååÏùº ÏàòÏßë Ïä§ÌÇµ: {e}")
                        
            except Exception as e:
                logger.error(f"GitHub Î¶¨Ìè¨ ÏàòÏßë Ïò§Î•ò: {e}")
                
        self.stats['categories']['github_samples'] = collected
        logger.info(f"‚úÖ GitHub ÏÉòÌîå {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    async def find_github_cs_files(self, api_url: str, path: str = '') -> List[Dict]:
        """GitHubÏóêÏÑú C# ÌååÏùº Ï∞æÍ∏∞"""
        cs_files = []
        
        try:
            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'AutoCI-Collector'
            }
            
            # GitHub ÌÜ†ÌÅ∞Ïù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
            github_token = os.environ.get('GITHUB_TOKEN')
            if github_token:
                headers['Authorization'] = f'token {github_token}'
                
            async with self.session.get(api_url, headers=headers) as response:
                if response.status == 200:
                    items = await response.json()
                    
                    for item in items:
                        if item['type'] == 'file' and item['name'].endswith('.cs'):
                            cs_files.append(item)
                        elif item['type'] == 'dir' and len(cs_files) < 100:
                            # ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ ÌÉêÏÉâ
                            sub_files = await self.find_github_cs_files(item['url'])
                            cs_files.extend(sub_files)
                            
                            if len(cs_files) >= 100:
                                break
                                
        except Exception as e:
            logger.debug(f"GitHub ÌååÏùº Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            
        return cs_files
        
    async def fetch_github_file(self, url: str) -> Optional[str]:
        """GitHub ÌååÏùº ÎÇ¥Ïö© Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            logger.debug(f"GitHub ÌååÏùº Í∞ÄÏ†∏Ïò§Í∏∞ Ïò§Î•ò: {e}")
            
        return None
        
    def analyze_cs_file(self, content: str) -> Dict:
        """C# ÌååÏùº Î∂ÑÏÑù"""
        analysis = {
            'lines': len(content.split('\n')),
            'classes': 0,
            'methods': 0,
            'interfaces': 0,
            'usings': [],
            'namespaces': [],
            'has_async': False,
            'has_linq': False,
            'has_generics': False,
            'complexity_estimate': 0,
            'quality_score': 0.7
        }
        
        # Ï†ïÍ∑úÏãùÏúºÎ°ú Î∂ÑÏÑù
        analysis['classes'] = len(re.findall(r'\bclass\s+\w+', content))
        analysis['methods'] = len(re.findall(r'(public|private|protected|internal)\s+\w+\s+\w+\s*\(', content))
        analysis['interfaces'] = len(re.findall(r'\binterface\s+\w+', content))
        
        # using Î¨∏ Ï∂îÏ∂ú
        usings = re.findall(r'using\s+([\w.]+);', content)
        analysis['usings'] = list(set(usings))
        
        # namespace Ï∂îÏ∂ú
        namespaces = re.findall(r'namespace\s+([\w.]+)', content)
        analysis['namespaces'] = list(set(namespaces))
        
        # ÌäπÏßï ÌôïÏù∏
        analysis['has_async'] = 'async' in content and 'await' in content
        analysis['has_linq'] = any(linq in content for linq in ['.Where(', '.Select(', '.OrderBy(', 'from '])
        analysis['has_generics'] = '<' in content and '>' in content
        
        # Î≥µÏû°ÎèÑ Ï∂îÏ†ï (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±)
        keywords = ['if', 'else', 'for', 'foreach', 'while', 'switch', 'try', 'catch']
        for keyword in keywords:
            analysis['complexity_estimate'] += content.count(keyword)
            
        # ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
        if analysis['has_async']:
            analysis['quality_score'] += 0.1
        if analysis['has_linq']:
            analysis['quality_score'] += 0.05
        if len(analysis['usings']) > 5:
            analysis['quality_score'] += 0.05
        if analysis['methods'] > 0:
            analysis['quality_score'] += 0.1
            
        analysis['quality_score'] = min(analysis['quality_score'], 1.0)
        
        return analysis
        
    async def collect_nuget_packages_async(self):
        """NuGet Ìå®ÌÇ§ÏßÄ Ï†ïÎ≥¥ ÏàòÏßë"""
        logger.info("üì¶ NuGet Ìå®ÌÇ§ÏßÄ Ï†ïÎ≥¥ ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['nuget_packages']
        collected = 0
        
        for package_name in category_info['packages']:
            try:
                # NuGet API
                api_url = f"https://api.nuget.org/v3-flatcontainer/{package_name.lower()}/index.json"
                
                async with self.session.get(api_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        versions = data.get('versions', [])
                        
                        if versions:
                            # ÏµúÏã† Î≤ÑÏ†Ñ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                            latest_version = versions[-1]
                            
                            # Ìå®ÌÇ§ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                            metadata_url = f"https://api.nuget.org/v3/registration5-semver1/{package_name.lower()}/{latest_version}.json"
                            
                            async with self.session.get(metadata_url) as meta_response:
                                if meta_response.status == 200:
                                    metadata = await meta_response.json()
                                    
                                    # Ïπ¥ÌÉàÎ°úÍ∑∏ ÏóîÌä∏Î¶¨
                                    catalog_entry = metadata.get('catalogEntry', {})
                                    
                                    package_data = {
                                        'name': package_name,
                                        'version': latest_version,
                                        'description': catalog_entry.get('description', ''),
                                        'authors': catalog_entry.get('authors', ''),
                                        'project_url': catalog_entry.get('projectUrl', ''),
                                        'repository': catalog_entry.get('repository', {}),
                                        'tags': catalog_entry.get('tags', []),
                                        'dependencies': catalog_entry.get('dependencyGroups', []),
                                        'download_count': 0,  # Î≥ÑÎèÑ API ÌïÑÏöî
                                        'collected_at': datetime.now().isoformat()
                                    }
                                    
                                    # README Í∞ÄÏ†∏Ïò§Í∏∞
                                    readme_url = catalog_entry.get('readmeUrl')
                                    if readme_url:
                                        readme_content = await self.fetch_url_async(readme_url)
                                        if readme_content:
                                            package_data['readme'] = readme_content
                                            
                                    # ÌååÏùº Ï†ÄÏû•
                                    file_path = category_info['path'] / f"{package_name}.json"
                                    await self.save_json_async(file_path, package_data)
                                    
                                    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                                    self.save_to_database(
                                        url=f"https://www.nuget.org/packages/{package_name}",
                                        category='nuget_packages',
                                        subcategory='package',
                                        title=package_name,
                                        content_type='package_info',
                                        file_path=str(file_path),
                                        size_bytes=len(json.dumps(package_data)),
                                        quality_score=0.9
                                    )
                                    
                                    collected += 1
                                    
            except Exception as e:
                logger.error(f"NuGet Ìå®ÌÇ§ÏßÄ ÏàòÏßë Ïò§Î•ò ({package_name}): {e}")
                
        self.stats['categories']['nuget_packages'] = collected
        logger.info(f"‚úÖ NuGet Ìå®ÌÇ§ÏßÄ {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    async def collect_stackoverflow_async(self):
        """Stack Overflow Í≥†Í∏â Q&A ÏàòÏßë"""
        logger.info("üèÜ Stack Overflow Í≥†Í∏â Q&A ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['stackoverflow_advanced']
        collected = 0
        
        # Stack Exchange API
        base_url = "https://api.stackexchange.com/2.3/questions"
        
        for tag in category_info['tags']:
            try:
                params = {
                    'order': 'desc',
                    'sort': 'votes',
                    'tagged': tag,
                    'site': 'stackoverflow',
                    'filter': '!9_bDE(fI5',  # Î≥∏Î¨∏ Ìè¨Ìï®
                    'pagesize': 20
                }
                
                async with self.session.get(base_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        questions = data.get('items', [])
                        
                        for question in questions:
                            # ÌíàÏßà ÌïÑÌÑ∞ÎßÅ
                            if (question.get('score', 0) >= category_info['min_score'] and
                                question.get('view_count', 0) >= category_info['min_views']):
                                
                                # ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞
                                answers = await self.fetch_stackoverflow_answers(question['question_id'])
                                
                                qa_data = {
                                    'question_id': question['question_id'],
                                    'title': question['title'],
                                    'body': question.get('body', ''),
                                    'tags': question['tags'],
                                    'score': question['score'],
                                    'view_count': question['view_count'],
                                    'creation_date': question['creation_date'],
                                    'link': question['link'],
                                    'answers': answers,
                                    'collected_at': datetime.now().isoformat()
                                }
                                
                                # ÏΩîÎìú ÏÉòÌîå Ï∂îÏ∂ú
                                code_samples = self.extract_code_from_html(question.get('body', ''))
                                for answer in answers:
                                    code_samples.extend(
                                        self.extract_code_from_html(answer.get('body', ''))
                                    )
                                qa_data['code_samples'] = code_samples
                                
                                # ÌååÏùº Ï†ÄÏû•
                                file_name = f"{tag.replace('+', '_')}_{question['question_id']}.json"
                                file_path = category_info['path'] / file_name
                                
                                await self.save_json_async(file_path, qa_data)
                                
                                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                                self.save_to_database(
                                    url=question['link'],
                                    category='stackoverflow_advanced',
                                    subcategory=tag,
                                    title=question['title'],
                                    content_type='qa',
                                    file_path=str(file_path),
                                    size_bytes=len(json.dumps(qa_data)),
                                    quality_score=min(question['score'] / 100, 1.0)
                                )
                                
                                collected += 1
                                
                                if collected >= 100:  # ÏµúÎåÄ 100Í∞ú
                                    break
                                    
            except Exception as e:
                logger.error(f"Stack Overflow ÏàòÏßë Ïò§Î•ò ({tag}): {e}")
                
        self.stats['categories']['stackoverflow_advanced'] = collected
        logger.info(f"‚úÖ Stack Overflow Q&A {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    async def fetch_stackoverflow_answers(self, question_id: int) -> List[Dict]:
        """Stack Overflow ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞"""
        answers = []
        
        try:
            url = f"https://api.stackexchange.com/2.3/questions/{question_id}/answers"
            params = {
                'order': 'desc',
                'sort': 'votes',
                'site': 'stackoverflow',
                'filter': '!9_bDE(fI5'  # Î≥∏Î¨∏ Ìè¨Ìï®
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for answer in data.get('items', [])[:3]:  # ÏÉÅÏúÑ 3Í∞ú ÎãµÎ≥Ä
                        answers.append({
                            'answer_id': answer['answer_id'],
                            'body': answer.get('body', ''),
                            'score': answer['score'],
                            'is_accepted': answer.get('is_accepted', False)
                        })
                        
        except Exception as e:
            logger.debug(f"ÎãµÎ≥Ä Í∞ÄÏ†∏Ïò§Í∏∞ Ïò§Î•ò: {e}")
            
        return answers
        
    def extract_code_from_html(self, html_content: str) -> List[Dict]:
        """HTMLÏóêÏÑú ÏΩîÎìú Ï∂îÏ∂ú"""
        code_samples = []
        
        if not html_content:
            return code_samples
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # <code> ÌÉúÍ∑∏ Ï∞æÍ∏∞
        for code_tag in soup.find_all('code'):
            code_text = code_tag.get_text(strip=True)
            
            # C# ÏΩîÎìúÏù∏ÏßÄ ÌôïÏù∏ (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±)
            if any(keyword in code_text for keyword in ['class', 'public', 'private', 'using', 'namespace', 'var']):
                code_samples.append({
                    'code': code_text,
                    'type': 'inline' if len(code_text) < 100 else 'block'
                })
                
        # <pre><code> Î∏îÎ°ù
        for pre_tag in soup.find_all('pre'):
            code_tag = pre_tag.find('code')
            if code_tag:
                code_text = code_tag.get_text(strip=True)
                code_samples.append({
                    'code': code_text,
                    'type': 'block'
                })
                
        return code_samples
        
    async def collect_expert_blogs_async(self):
        """Ï†ÑÎ¨∏Í∞Ä Î∏îÎ°úÍ∑∏ ÏàòÏßë"""
        logger.info("üìù Ï†ÑÎ¨∏Í∞Ä Î∏îÎ°úÍ∑∏ ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['expert_blogs']
        collected = 0
        
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Í∞Å Î∏îÎ°úÍ∑∏Ïùò RSS ÌîºÎìúÎÇò ÏÇ¨Ïù¥Ìä∏ÎßµÏùÑ ÏÇ¨Ïö©
        # Ïó¨Í∏∞ÏÑúÎäî Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå ÏàòÏßë
        
        for blog in category_info['sources']:
            try:
                blog_data = {
                    'name': blog['name'],
                    'url': blog['url'],
                    'type': 'blog',
                    'topics': self.get_expert_topics(blog['name']),
                    'sample_posts': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # RSS ÌîºÎìú ÌôïÏù∏
                rss_url = await self.find_rss_feed(blog['url'])
                if rss_url:
                    posts = await self.parse_rss_feed(rss_url)
                    blog_data['sample_posts'] = posts[:10]  # ÏµúÍ∑º 10Í∞ú
                    
                # ÌååÏùº Ï†ÄÏû•
                file_name = f"{blog['name'].replace(' ', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, blog_data)
                
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                self.save_to_database(
                    url=blog['url'],
                    category='expert_blogs',
                    subcategory='blog',
                    title=blog['name'],
                    content_type='blog_info',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(blog_data)),
                    quality_score=0.95
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"Î∏îÎ°úÍ∑∏ ÏàòÏßë Ïò§Î•ò ({blog['name']}): {e}")
                
        self.stats['categories']['expert_blogs'] = collected
        logger.info(f"‚úÖ Ï†ÑÎ¨∏Í∞Ä Î∏îÎ°úÍ∑∏ {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    def get_expert_topics(self, expert_name: str) -> List[str]:
        """Ï†ÑÎ¨∏Í∞ÄÎ≥Ñ Ï£ºÏöî ÌÜ†ÌîΩ"""
        topics_map = {
            'Jon Skeet': ['async/await', 'DateTime', 'strings', 'LINQ'],
            'Eric Lippert': ['language design', 'performance', 'compiler'],
            'Stephen Cleary': ['async/await', 'concurrency', 'tasks'],
            'Steve Gordon': ['ASP.NET Core', 'performance', 'HttpClient'],
            'Andrew Lock': ['ASP.NET Core', 'Docker', 'configuration'],
            'Nick Chapsas': ['performance', 'best practices', 'new features'],
            'David Fowler': ['ASP.NET Core', 'SignalR', 'performance']
        }
        
        return topics_map.get(expert_name, ['C#', '.NET'])
        
    async def find_rss_feed(self, blog_url: str) -> Optional[str]:
        """RSS ÌîºÎìú URL Ï∞æÍ∏∞"""
        try:
            content = await self.fetch_url_async(blog_url)
            if content:
                soup = BeautifulSoup(content, 'html.parser')
                
                # RSS ÎßÅÌÅ¨ Ï∞æÍ∏∞
                rss_link = soup.find('link', {'type': 'application/rss+xml'})
                if rss_link and rss_link.get('href'):
                    return urljoin(blog_url, rss_link['href'])
                    
                # ÏùºÎ∞òÏ†ÅÏù∏ RSS Í≤ΩÎ°ú ÏãúÎèÑ
                common_paths = ['/rss', '/feed', '/rss.xml', '/feed.xml', '/atom.xml']
                for path in common_paths:
                    test_url = urljoin(blog_url, path)
                    if await self.url_exists(test_url):
                        return test_url
                        
        except Exception as e:
            logger.debug(f"RSS ÌîºÎìú Ï∞æÍ∏∞ Ïò§Î•ò: {e}")
            
        return None
        
    async def url_exists(self, url: str) -> bool:
        """URL Ï°¥Ïû¨ ÌôïÏù∏"""
        try:
            async with self.session.head(url) as response:
                return response.status == 200
        except:
            return False
            
    async def parse_rss_feed(self, rss_url: str) -> List[Dict]:
        """RSS ÌîºÎìú ÌååÏã±"""
        posts = []
        
        try:
            content = await self.fetch_url_async(rss_url)
            if content:
                root = ET.fromstring(content)
                
                # RSS 2.0
                for item in root.findall('.//item')[:10]:
                    post = {
                        'title': item.findtext('title', ''),
                        'link': item.findtext('link', ''),
                        'pubDate': item.findtext('pubDate', ''),
                        'description': item.findtext('description', '')
                    }
                    posts.append(post)
                    
        except Exception as e:
            logger.debug(f"RSS ÌååÏã± Ïò§Î•ò: {e}")
            
        return posts
        
    async def collect_performance_guides_async(self):
        """ÏÑ±Îä• Í∞ÄÏù¥Îìú ÏàòÏßë"""
        logger.info("‚ö° ÏÑ±Îä• Í∞ÄÏù¥Îìú ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['performance_guides']
        collected = 0
        
        performance_topics = [
            'string-performance',
            'collection-performance',
            'linq-performance',
            'async-performance',
            'memory-allocation',
            'span-memory',
            'object-pooling',
            'caching-strategies',
            'profiling-tools',
            'benchmarking'
        ]
        
        for topic in performance_topics:
            try:
                guide_data = {
                    'topic': topic,
                    'title': topic.replace('-', ' ').title(),
                    'sources': [],
                    'best_practices': [],
                    'code_examples': [],
                    'benchmarks': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # Í∞Å ÏÜåÏä§ÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥ ÏàòÏßë
                for source_url in category_info['sources']:
                    if 'docs.microsoft.com' in source_url:
                        # Microsoft Î¨∏ÏÑúÏóêÏÑú ÏÑ±Îä• Í∞ÄÏù¥Îìú Ï∞æÍ∏∞
                        search_url = f"{source_url}{topic}"
                        content = await self.fetch_url_async(search_url)
                        
                        if content:
                            guide_data['sources'].append({
                                'url': search_url,
                                'type': 'microsoft_docs'
                            })
                            
                # ÌååÏùº Ï†ÄÏû•
                file_path = category_info['path'] / f"{topic}.json"
                await self.save_json_async(file_path, guide_data)
                
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                self.save_to_database(
                    url=f"performance-guide-{topic}",
                    category='performance_guides',
                    subcategory='guide',
                    title=guide_data['title'],
                    content_type='guide',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(guide_data)),
                    quality_score=0.9
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"ÏÑ±Îä• Í∞ÄÏù¥Îìú ÏàòÏßë Ïò§Î•ò ({topic}): {e}")
                
        self.stats['categories']['performance_guides'] = collected
        logger.info(f"‚úÖ ÏÑ±Îä• Í∞ÄÏù¥Îìú {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    async def collect_design_patterns_async(self):
        """ÎîîÏûêÏù∏ Ìå®ÌÑ¥ ÏàòÏßë"""
        logger.info("üé® ÎîîÏûêÏù∏ Ìå®ÌÑ¥ ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['design_patterns']
        collected = 0
        
        for pattern in category_info['patterns']:
            try:
                pattern_data = {
                    'name': pattern,
                    'category': self.get_pattern_category(pattern),
                    'intent': '',
                    'structure': {},
                    'participants': [],
                    'implementation': {
                        'csharp': ''
                    },
                    'example_usage': '',
                    'when_to_use': [],
                    'pros': [],
                    'cons': [],
                    'related_patterns': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # Ìå®ÌÑ¥ Ï†ïÎ≥¥ Ï∂îÍ∞Ä (Ïã§Ï†úÎ°úÎäî Ïô∏Î∂Ä ÏÜåÏä§ÏóêÏÑú ÏàòÏßë)
                pattern_data.update(self.get_pattern_details(pattern))
                
                # ÌååÏùº Ï†ÄÏû•
                file_name = f"{pattern.replace(' ', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, pattern_data)
                
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                self.save_to_database(
                    url=f"pattern-{pattern}",
                    category='design_patterns',
                    subcategory=pattern_data['category'],
                    title=pattern,
                    content_type='pattern',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(pattern_data)),
                    quality_score=0.95
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"Ìå®ÌÑ¥ ÏàòÏßë Ïò§Î•ò ({pattern}): {e}")
                
        self.stats['categories']['design_patterns'] = collected
        logger.info(f"‚úÖ ÎîîÏûêÏù∏ Ìå®ÌÑ¥ {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    def get_pattern_category(self, pattern: str) -> str:
        """Ìå®ÌÑ¥ Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ•ò"""
        categories = {
            'Creational': ['Singleton', 'Factory Method', 'Abstract Factory', 'Builder', 'Prototype'],
            'Structural': ['Adapter', 'Bridge', 'Composite', 'Decorator', 'Facade', 'Flyweight', 'Proxy'],
            'Behavioral': ['Chain of Responsibility', 'Command', 'Iterator', 'Mediator', 'Memento',
                          'Observer', 'State', 'Strategy', 'Template Method', 'Visitor'],
            'Architectural': ['Repository', 'Unit of Work', 'Specification', 'CQRS', 'Event Sourcing']
        }
        
        for category, patterns in categories.items():
            if pattern in patterns:
                return category
                
        return 'Other'
        
    def get_pattern_details(self, pattern: str) -> Dict:
        """Ìå®ÌÑ¥ ÏÉÅÏÑ∏ Ï†ïÎ≥¥"""
        # Ïã§Ï†úÎ°úÎäî Ïô∏Î∂Ä ÏÜåÏä§ÏóêÏÑú ÏàòÏßë
        # Ïó¨Í∏∞ÏÑúÎäî ÏòàÏãú Îç∞Ïù¥ÌÑ∞
        
        details = {
            'Singleton': {
                'intent': 'Ensure a class has only one instance and provide a global point of access to it.',
                'implementation': {
                    'csharp': '''public sealed class Singleton
{
    private static readonly Lazy<Singleton> lazy = 
        new Lazy<Singleton>(() => new Singleton());
    
    public static Singleton Instance => lazy.Value;
    
    private Singleton()
    {
    }
}'''
                },
                'when_to_use': [
                    'When exactly one instance of a class is needed',
                    'When the single instance should be extensible by subclassing'
                ],
                'pros': ['Controlled access to sole instance', 'Reduced namespace pollution'],
                'cons': ['Difficult to test', 'Violates Single Responsibility Principle']
            }
        }
        
        return details.get(pattern, {})
        
    async def collect_unity_practices_async(self):
        """Unity Î≤†Ïä§Ìä∏ ÌîÑÎûôÌã∞Ïä§ ÏàòÏßë"""
        logger.info("üéÆ Unity Î≤†Ïä§Ìä∏ ÌîÑÎûôÌã∞Ïä§ ÏàòÏßë Ï§ë...")
        
        category_info = self.categories['unity_best_practices']
        collected = 0
        
        for topic in category_info['topics']:
            try:
                practice_data = {
                    'topic': topic,
                    'description': '',
                    'best_practices': [],
                    'common_mistakes': [],
                    'code_examples': [],
                    'performance_tips': [],
                    'unity_version': '2022.3 LTS',
                    'collected_at': datetime.now().isoformat()
                }
                
                # Unity ÌäπÌôî Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                practice_data.update(self.get_unity_practice_details(topic))
                
                # ÌååÏùº Ï†ÄÏû•
                file_name = f"{topic.replace(' ', '_').replace('/', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, practice_data)
                
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î°ù
                self.save_to_database(
                    url=f"unity-practice-{topic}",
                    category='unity_best_practices',
                    subcategory='practice',
                    title=topic,
                    content_type='unity_guide',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(practice_data)),
                    quality_score=0.9
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"Unity ÌîÑÎûôÌã∞Ïä§ ÏàòÏßë Ïò§Î•ò ({topic}): {e}")
                
        self.stats['categories']['unity_best_practices'] = collected
        logger.info(f"‚úÖ Unity Î≤†Ïä§Ìä∏ ÌîÑÎûôÌã∞Ïä§ {collected}Í∞ú ÏàòÏßë ÏôÑÎ£å")
        
    def get_unity_practice_details(self, topic: str) -> Dict:
        """Unity ÌîÑÎûôÌã∞Ïä§ ÏÉÅÏÑ∏ Ï†ïÎ≥¥"""
        details = {
            'Object pooling': {
                'description': 'Reuse objects instead of instantiating and destroying them repeatedly',
                'best_practices': [
                    'Pre-instantiate objects at scene load',
                    'Use generic pool implementation',
                    'Reset object state when returning to pool'
                ],
                'code_examples': [
                    {
                        'title': 'Generic Object Pool',
                        'code': '''public class ObjectPool<T> where T : MonoBehaviour
{
    private Queue<T> pool = new Queue<T>();
    private T prefab;
    private Transform parent;
    
    public ObjectPool(T prefab, int initialSize, Transform parent = null)
    {
        this.prefab = prefab;
        this.parent = parent;
        
        for (int i = 0; i < initialSize; i++)
        {
            T obj = Object.Instantiate(prefab, parent);
            obj.gameObject.SetActive(false);
            pool.Enqueue(obj);
        }
    }
    
    public T Get()
    {
        if (pool.Count > 0)
        {
            T obj = pool.Dequeue();
            obj.gameObject.SetActive(true);
            return obj;
        }
        else
        {
            return Object.Instantiate(prefab, parent);
        }
    }
    
    public void Return(T obj)
    {
        obj.gameObject.SetActive(false);
        pool.Enqueue(obj);
    }
}'''
                    }
                ]
            }
        }
        
        return details.get(topic, {})
        
    async def fetch_url_async(self, url: str) -> Optional[str]:
        """URL ÎÇ¥Ïö© ÎπÑÎèôÍ∏∞Î°ú Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with self.session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    return await response.text()
                    
        except asyncio.TimeoutError:
            logger.warning(f"ÌÉÄÏûÑÏïÑÏõÉ: {url}")
        except Exception as e:
            logger.debug(f"URL Í∞ÄÏ†∏Ïò§Í∏∞ Ïò§Î•ò ({url}): {e}")
            
        return None
        
    async def save_json_async(self, file_path: Path, data: Dict):
        """JSON ÌååÏùº ÎπÑÎèôÍ∏∞ Ï†ÄÏû•"""
        try:
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(data, indent=2, ensure_ascii=False))
                
            # ÌååÏùº ÌÅ¨Í∏∞ ÏóÖÎç∞Ïù¥Ìä∏
            size_mb = file_path.stat().st_size / 1024 / 1024
            self.stats['total_size_mb'] += size_mb
            self.stats['total_collected'] += 1
            
        except Exception as e:
            logger.error(f"ÌååÏùº Ï†ÄÏû• Ïò§Î•ò ({file_path}): {e}")
            
    def save_to_database(self, **kwargs):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÏàòÏßë Ï†ïÎ≥¥ Ï†ÄÏû•"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
            INSERT OR REPLACE INTO collected_data 
            (url, category, subcategory, title, content_type, 
             file_path, size_bytes, quality_score, collected_at, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                kwargs.get('url'),
                kwargs.get('category'),
                kwargs.get('subcategory'),
                kwargs.get('title'),
                kwargs.get('content_type'),
                kwargs.get('file_path'),
                kwargs.get('size_bytes', 0),
                kwargs.get('quality_score', 0.7),
                datetime.now(),
                json.dumps(kwargs.get('metadata', {}))
            ))
            
            conn.commit()
            
        except Exception as e:
            logger.error(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ïò§Î•ò: {e}")
        finally:
            conn.close()
            
    def save_statistics(self):
        """ÏàòÏßë ÌÜµÍ≥Ñ Ï†ÄÏû•"""
        self.stats['end_time'] = datetime.now()
        self.stats['duration'] = str(self.stats['end_time'] - self.stats['start_time'])
        
        # JSON ÌååÏùºÎ°ú Ï†ÄÏû•
        stats_file = self.output_dir / 'deep_collection_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, default=str)
            
        # ÎßàÌÅ¨Îã§Ïö¥ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        self.generate_report()
        
    def generate_report(self):
        """ÏàòÏßë Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
        report = f"""# Ïã¨Ï∏µ C# Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î¶¨Ìè¨Ìä∏

## üìä ÏàòÏßë ÌÜµÍ≥Ñ
- **ÏãúÏûë ÏãúÍ∞Ñ**: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **Ï¢ÖÎ£å ÏãúÍ∞Ñ**: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **ÏÜåÏöî ÏãúÍ∞Ñ**: {self.stats['duration']}
- **Ï¥ù ÏàòÏßë Ìï≠Î™©**: {self.stats['total_collected']}Í∞ú
- **Ï¥ù Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞**: {self.stats['total_size_mb']:.2f}MB

## üìÅ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÏàòÏßë ÌòÑÌô©
"""
        
        for category, count in self.stats['categories'].items():
            report += f"- **{category}**: {count}Í∞ú\n"
            
        if self.stats['errors']:
            report += f"\n## ‚ö†Ô∏è Ïò§Î•ò Î∞úÏÉù\n"
            for error in self.stats['errors'][:10]:  # ÏµúÎåÄ 10Í∞ú
                report += f"- {error}\n"
                
        report += f"""
## üéØ ÏàòÏßë ÌíàÏßà
- Microsoft Í≥µÏãù Î¨∏ÏÑú: Í≥†ÌíàÏßà (0.95)
- GitHub Ï†ÑÎ¨∏Í∞Ä ÌîÑÎ°úÏ†ùÌä∏: Í≥†ÌíàÏßà (0.7-0.9)
- Stack Overflow ÎãµÎ≥Ä: Ìà¨Ìëú Í∏∞Î∞ò ÌíàÏßà
- NuGet Ìå®ÌÇ§ÏßÄ: Í≤ÄÏ¶ùÎêú Ìå®ÌÇ§ÏßÄ (0.9)

## üìÇ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞
```
{self.output_dir}/
‚îú‚îÄ‚îÄ microsoft_docs/          # MS Í≥µÏãù Î¨∏ÏÑú
‚îú‚îÄ‚îÄ github_samples/          # GitHub ÏÉòÌîå ÏΩîÎìú
‚îú‚îÄ‚îÄ nuget_packages/          # NuGet Ìå®ÌÇ§ÏßÄ Ï†ïÎ≥¥
‚îú‚îÄ‚îÄ stackoverflow_advanced/  # SO Í≥†Í∏â Q&A
‚îú‚îÄ‚îÄ expert_blogs/           # Ï†ÑÎ¨∏Í∞Ä Î∏îÎ°úÍ∑∏
‚îú‚îÄ‚îÄ performance_guides/     # ÏÑ±Îä• Í∞ÄÏù¥Îìú
‚îú‚îÄ‚îÄ design_patterns/        # ÎîîÏûêÏù∏ Ìå®ÌÑ¥
‚îú‚îÄ‚îÄ unity_best_practices/   # Unity Í∞ÄÏù¥Îìú
‚îî‚îÄ‚îÄ collection_index.db     # ÏàòÏßë Ïù∏Îç±Ïä§ DB
```

## üöÄ Îã§Ïùå Îã®Í≥Ñ
1. `autoci data index` - ÏàòÏßëÎêú Îç∞Ïù¥ÌÑ∞ Ïù∏Îç±Ïã±
2. `autoci dual start` - RAG + ÌååÏù∏ÌäúÎãù ÏãúÏûë
3. `autoci enhance start` - 24ÏãúÍ∞Ñ ÏûêÎèô ÏãúÏä§ÌÖú ÏãúÏûë

---
*ÏÉùÏÑ± ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_file = self.output_dir / 'deep_collection_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"üìù Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å: {report_file}")


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    collector = DeepCSharpCollector()
    
    # ÎπÑÎèôÍ∏∞ Ïã§Ìñâ
    asyncio.run(collector.collect_all_async())
    
    print("\n‚úÖ Ïã¨Ï∏µ C# Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å!")
    print(f"üìÅ Ï†ÄÏû• ÏúÑÏπò: {collector.output_dir}")
    print(f"üìä Ï¥ù {collector.stats['total_collected']}Í∞ú Ìï≠Î™© ÏàòÏßë")
    print(f"üíæ Ï¥ù {collector.stats['total_size_mb']:.2f}MB Îç∞Ïù¥ÌÑ∞")
    print("\nüìã Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú Ïù∏Îç±Ïã±ÏùÑ ÏãúÏûëÌïòÏÑ∏Ïöî: autoci data index")


if __name__ == "__main__":
    main()