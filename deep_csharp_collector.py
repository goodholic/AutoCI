#!/usr/bin/env python3
"""
ì‹¬ì¸µ C# ì „ë¬¸ê°€ ë°ì´í„° ìˆ˜ì§‘ê¸°
ì‹¤ì œ ì›¹ì—ì„œ ê³ í’ˆì§ˆ C# ë°ì´í„°ë¥¼ ìˆ˜ì§‘
"""

import os
import sys
import json
import time
import asyncio
import aiohttp
import aiofiles
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
import hashlib
import re
from bs4 import BeautifulSoup
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import sqlite3
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('deep_csharp_collector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DeepCSharpCollector:
    """ì‹¬ì¸µ C# ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, output_dir: str = "expert_learning_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬
        self.categories = {
            'microsoft_docs': {
                'path': self.output_dir / 'microsoft_docs',
                'base_url': 'https://docs.microsoft.com/en-us/dotnet/csharp/',
                'topics': [
                    'programming-guide/concepts/async/',
                    'programming-guide/concepts/linq/',
                    'programming-guide/concepts/expression-trees/',
                    'programming-guide/generics/',
                    'programming-guide/delegates/',
                    'programming-guide/events/',
                    'programming-guide/indexers/',
                    'programming-guide/interfaces/',
                    'language-reference/operators/',
                    'language-reference/keywords/',
                    'language-reference/attributes/',
                    'whats-new/csharp-10',
                    'whats-new/csharp-11',
                    'tutorials/pattern-matching',
                    'tutorials/nullable-reference-types'
                ]
            },
            'github_samples': {
                'path': self.output_dir / 'github_samples',
                'repos': [
                    {'owner': 'dotnet', 'repo': 'samples'},
                    {'owner': 'dotnet', 'repo': 'docs'},
                    {'owner': 'dotnet', 'repo': 'AspNetCore.Docs'},
                    {'owner': 'microsoft', 'repo': 'dotnet-samples'},
                    {'owner': 'Azure-Samples', 'repo': 'cognitive-services-speech-sdk'},
                    {'owner': 'Unity-Technologies', 'repo': 'EntityComponentSystemSamples'},
                    {'owner': 'dotnet', 'repo': 'machinelearning-samples'},
                    {'owner': 'dotnet', 'repo': 'blazor-samples'}
                ]
            },
            'nuget_packages': {
                'path': self.output_dir / 'nuget_packages',
                'packages': [
                    'Newtonsoft.Json',
                    'Microsoft.EntityFrameworkCore',
                    'Serilog',
                    'AutoMapper',
                    'FluentValidation',
                    'Polly',
                    'MediatR',
                    'Dapper',
                    'NUnit',
                    'xunit',
                    'Moq',
                    'FluentAssertions',
                    'BenchmarkDotNet',
                    'MessagePack',
                    'StackExchange.Redis'
                ]
            },
            'stackoverflow_advanced': {
                'path': self.output_dir / 'stackoverflow_advanced',
                'tags': [
                    'c#+async-await',
                    'c#+performance',
                    'c#+memory-management',
                    'c#+linq',
                    'c#+entity-framework-core',
                    'c#+dependency-injection',
                    'c#+design-patterns',
                    'c#+unit-testing',
                    'c#+blazor',
                    'c#+grpc'
                ],
                'min_score': 50,
                'min_views': 10000
            },
            'expert_blogs': {
                'path': self.output_dir / 'expert_blogs',
                'sources': [
                    {'name': 'Jon Skeet', 'url': 'https://codeblog.jonskeet.uk/'},
                    {'name': 'Eric Lippert', 'url': 'https://ericlippert.com/'},
                    {'name': 'Stephen Cleary', 'url': 'https://blog.stephencleary.com/'},
                    {'name': 'Steve Gordon', 'url': 'https://www.stevejgordon.co.uk/'},
                    {'name': 'Andrew Lock', 'url': 'https://andrewlock.net/'},
                    {'name': 'Nick Chapsas', 'url': 'https://www.youtube.com/@nickchapsas'},
                    {'name': 'David Fowler', 'url': 'https://twitter.com/davidfowl'}
                ]
            },
            'performance_guides': {
                'path': self.output_dir / 'performance_guides',
                'sources': [
                    'https://docs.microsoft.com/en-us/dotnet/framework/performance/',
                    'https://github.com/dotnet/performance',
                    'https://devblogs.microsoft.com/dotnet/category/performance/'
                ]
            },
            'design_patterns': {
                'path': self.output_dir / 'design_patterns',
                'patterns': [
                    'Singleton', 'Factory Method', 'Abstract Factory', 'Builder', 'Prototype',
                    'Adapter', 'Bridge', 'Composite', 'Decorator', 'Facade', 'Flyweight', 'Proxy',
                    'Chain of Responsibility', 'Command', 'Iterator', 'Mediator', 'Memento',
                    'Observer', 'State', 'Strategy', 'Template Method', 'Visitor',
                    'Repository', 'Unit of Work', 'Specification', 'CQRS', 'Event Sourcing'
                ]
            },
            'unity_best_practices': {
                'path': self.output_dir / 'unity_best_practices',
                'topics': [
                    'MonoBehaviour lifecycle',
                    'Object pooling',
                    'Coroutines vs async',
                    'Performance optimization',
                    'Memory management',
                    'ScriptableObjects',
                    'Event systems',
                    'Input system',
                    'Addressables',
                    'DOTS/ECS'
                ]
            }
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for category_info in self.categories.values():
            category_info['path'].mkdir(exist_ok=True)
            
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.init_database()
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = {
            'total_collected': 0,
            'total_size_mb': 0,
            'categories': {},
            'errors': [],
            'start_time': datetime.now()
        }
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS collected_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            category TEXT,
            subcategory TEXT,
            title TEXT,
            content_type TEXT,
            file_path TEXT,
            size_bytes INTEGER,
            quality_score REAL,
            collected_at TIMESTAMP,
            metadata TEXT
        )
        ''')
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS code_samples (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            data_id INTEGER,
            language TEXT,
            code TEXT,
            description TEXT,
            tags TEXT,
            FOREIGN KEY (data_id) REFERENCES collected_data(id)
        )
        ''')
        
        conn.commit()
        conn.close()
        
    async def collect_all_async(self):
        """ë¹„ë™ê¸°ë¡œ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("ğŸš€ ì‹¬ì¸µ C# ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # aiohttp ì„¸ì…˜ ìƒì„±
        async with aiohttp.ClientSession() as self.session:
            tasks = []
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ì‘ì—…
            tasks.append(self.collect_microsoft_docs_async())
            tasks.append(self.collect_github_samples_async())
            tasks.append(self.collect_nuget_packages_async())
            tasks.append(self.collect_stackoverflow_async())
            tasks.append(self.collect_expert_blogs_async())
            tasks.append(self.collect_performance_guides_async())
            tasks.append(self.collect_design_patterns_async())
            tasks.append(self.collect_unity_practices_async())
            
            # ëª¨ë“  ì‘ì—… ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ìˆ˜ì§‘ ì‘ì—… ì‹¤íŒ¨: {result}")
                    self.stats['errors'].append(str(result))
                    
        # í†µê³„ ì €ì¥
        self.save_statistics()
        
    async def collect_microsoft_docs_async(self):
        """Microsoft ë¬¸ì„œ ìˆ˜ì§‘"""
        logger.info("ğŸ“š Microsoft ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['microsoft_docs']
        collected = 0
        
        for topic in category_info['topics']:
            try:
                url = urljoin(category_info['base_url'], topic)
                content = await self.fetch_url_async(url)
                
                if content:
                    # HTML íŒŒì‹±
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # ì œëª© ì¶”ì¶œ
                    title = soup.find('h1')
                    title_text = title.text.strip() if title else topic.replace('/', ' ').title()
                    
                    # ë³¸ë¬¸ ì¶”ì¶œ
                    article = soup.find('article') or soup.find('main')
                    if article:
                        # ì½”ë“œ ìƒ˜í”Œ ì¶”ì¶œ
                        code_samples = self.extract_code_samples(article)
                        
                        # í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
                        text_content = article.get_text(separator='\n', strip=True)
                        
                        # ë°ì´í„° êµ¬ì¡°í™”
                        doc_data = {
                            'url': url,
                            'title': title_text,
                            'topic': topic,
                            'content': text_content,
                            'code_samples': code_samples,
                            'collected_at': datetime.now().isoformat()
                        }
                        
                        # íŒŒì¼ ì €ì¥
                        file_name = f"{topic.replace('/', '_')}.json"
                        file_path = category_info['path'] / file_name
                        
                        await self.save_json_async(file_path, doc_data)
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                        self.save_to_database(
                            url=url,
                            category='microsoft_docs',
                            subcategory=topic.split('/')[0],
                            title=title_text,
                            content_type='documentation',
                            file_path=str(file_path),
                            size_bytes=len(json.dumps(doc_data)),
                            quality_score=0.95
                        )
                        
                        collected += 1
                        
                        # ê´€ë ¨ ë§í¬ ìˆ˜ì§‘
                        related_links = self.extract_related_links(soup, category_info['base_url'])
                        for link in related_links[:5]:  # ìµœëŒ€ 5ê°œ
                            await self.collect_related_doc(link, category_info)
                            
            except Exception as e:
                logger.error(f"Microsoft ë¬¸ì„œ ìˆ˜ì§‘ ì˜¤ë¥˜ ({topic}): {e}")
                
        self.stats['categories']['microsoft_docs'] = collected
        logger.info(f"âœ… Microsoft ë¬¸ì„œ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    def extract_code_samples(self, element) -> List[Dict]:
        """ì½”ë“œ ìƒ˜í”Œ ì¶”ì¶œ"""
        code_samples = []
        
        # <pre><code> íƒœê·¸ ì°¾ê¸°
        for code_block in element.find_all(['pre', 'code']):
            code_text = code_block.get_text(strip=True)
            
            if len(code_text) > 50:  # ì˜ë¯¸ìˆëŠ” ì½”ë“œë§Œ
                # ì–¸ì–´ ê°ì§€
                language = 'csharp'
                class_attr = code_block.get('class', [])
                if isinstance(class_attr, list):
                    for cls in class_attr:
                        if 'language-' in cls:
                            language = cls.replace('language-', '')
                            break
                            
                code_samples.append({
                    'language': language,
                    'code': code_text,
                    'lines': len(code_text.split('\n'))
                })
                
        return code_samples
        
    def extract_related_links(self, soup, base_url: str) -> List[str]:
        """ê´€ë ¨ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        # ê´€ë ¨ ë¬¸ì„œ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ
            if href.startswith('/'):
                href = urljoin(base_url, href)
                
            # C# ê´€ë ¨ ë§í¬ë§Œ
            if 'csharp' in href or 'dotnet' in href:
                if href not in links and base_url in href:
                    links.append(href)
                    
        return links
        
    async def collect_related_doc(self, url: str, category_info: Dict):
        """ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘"""
        try:
            # ì´ë¯¸ ìˆ˜ì§‘í–ˆëŠ”ì§€ í™•ì¸
            if self.is_already_collected(url):
                return
                
            content = await self.fetch_url_async(url)
            if content:
                # ê°„ë‹¨í•œ ì²˜ë¦¬ (ë©”ì¸ ë¬¸ì„œì™€ ë™ì¼í•œ ë°©ì‹)
                # ... (êµ¬í˜„ ìƒëµ)
                pass
                
        except Exception as e:
            logger.debug(f"ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")
            
    def is_already_collected(self, url: str) -> bool:
        """ì´ë¯¸ ìˆ˜ì§‘í–ˆëŠ”ì§€ í™•ì¸"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT 1 FROM collected_data WHERE url = ?', (url,))
        result = cursor.fetchone()
        
        conn.close()
        return result is not None
        
    async def collect_github_samples_async(self):
        """GitHub ìƒ˜í”Œ ì½”ë“œ ìˆ˜ì§‘"""
        logger.info("ğŸ™ GitHub ìƒ˜í”Œ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['github_samples']
        collected = 0
        
        for repo_info in category_info['repos']:
            try:
                # GitHub API ì‚¬ìš©
                api_url = f"https://api.github.com/repos/{repo_info['owner']}/{repo_info['repo']}/contents"
                
                # ì¬ê·€ì ìœ¼ë¡œ C# íŒŒì¼ ì°¾ê¸°
                cs_files = await self.find_github_cs_files(api_url)
                
                for file_info in cs_files[:50]:  # ë¦¬í¬ë‹¹ ìµœëŒ€ 50ê°œ
                    try:
                        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        file_content = await self.fetch_github_file(file_info['download_url'])
                        
                        if file_content:
                            # íŒŒì¼ ë¶„ì„
                            analysis = self.analyze_cs_file(file_content)
                            
                            # ë°ì´í„° êµ¬ì¡°í™”
                            sample_data = {
                                'repo': f"{repo_info['owner']}/{repo_info['repo']}",
                                'file_path': file_info['path'],
                                'file_name': file_info['name'],
                                'content': file_content,
                                'analysis': analysis,
                                'size': file_info['size'],
                                'url': file_info['html_url'],
                                'collected_at': datetime.now().isoformat()
                            }
                            
                            # íŒŒì¼ ì €ì¥
                            safe_name = file_info['path'].replace('/', '_')
                            file_path = category_info['path'] / f"{safe_name}.json"
                            
                            await self.save_json_async(file_path, sample_data)
                            
                            # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                            self.save_to_database(
                                url=file_info['html_url'],
                                category='github_samples',
                                subcategory=repo_info['repo'],
                                title=file_info['name'],
                                content_type='source_code',
                                file_path=str(file_path),
                                size_bytes=file_info['size'],
                                quality_score=analysis['quality_score']
                            )
                            
                            collected += 1
                            
                    except Exception as e:
                        logger.debug(f"íŒŒì¼ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")
                        
            except Exception as e:
                logger.error(f"GitHub ë¦¬í¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                
        self.stats['categories']['github_samples'] = collected
        logger.info(f"âœ… GitHub ìƒ˜í”Œ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    async def find_github_cs_files(self, api_url: str, path: str = '') -> List[Dict]:
        """GitHubì—ì„œ C# íŒŒì¼ ì°¾ê¸°"""
        cs_files = []
        
        try:
            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'AutoCI-Collector'
            }
            
            # GitHub í† í°ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            github_token = os.environ.get('GITHUB_TOKEN')
            if github_token:
                headers['Authorization'] = f'token {github_token}'
                
            async with self.session.get(api_url, headers=headers) as response:
                if response.status == 200:
                    items = await response.json()
                    
                    for item in items:
                        if item['type'] == 'file' and item['name'].endswith('.cs'):
                            cs_files.append(item)
                        elif item['type'] == 'dir' and len(cs_files) < 100:
                            # í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰
                            sub_files = await self.find_github_cs_files(item['url'])
                            cs_files.extend(sub_files)
                            
                            if len(cs_files) >= 100:
                                break
                                
        except Exception as e:
            logger.debug(f"GitHub íŒŒì¼ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
        return cs_files
        
    async def fetch_github_file(self, url: str) -> Optional[str]:
        """GitHub íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            logger.debug(f"GitHub íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            
        return None
        
    def analyze_cs_file(self, content: str) -> Dict:
        """C# íŒŒì¼ ë¶„ì„"""
        analysis = {
            'lines': len(content.split('\n')),
            'classes': 0,
            'methods': 0,
            'interfaces': 0,
            'usings': [],
            'namespaces': [],
            'has_async': False,
            'has_linq': False,
            'has_generics': False,
            'complexity_estimate': 0,
            'quality_score': 0.7
        }
        
        # ì •ê·œì‹ìœ¼ë¡œ ë¶„ì„
        analysis['classes'] = len(re.findall(r'\bclass\s+\w+', content))
        analysis['methods'] = len(re.findall(r'(public|private|protected|internal)\s+\w+\s+\w+\s*\(', content))
        analysis['interfaces'] = len(re.findall(r'\binterface\s+\w+', content))
        
        # using ë¬¸ ì¶”ì¶œ
        usings = re.findall(r'using\s+([\w.]+);', content)
        analysis['usings'] = list(set(usings))
        
        # namespace ì¶”ì¶œ
        namespaces = re.findall(r'namespace\s+([\w.]+)', content)
        analysis['namespaces'] = list(set(namespaces))
        
        # íŠ¹ì§• í™•ì¸
        analysis['has_async'] = 'async' in content and 'await' in content
        analysis['has_linq'] = any(linq in content for linq in ['.Where(', '.Select(', '.OrderBy(', 'from '])
        analysis['has_generics'] = '<' in content and '>' in content
        
        # ë³µì¡ë„ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        keywords = ['if', 'else', 'for', 'foreach', 'while', 'switch', 'try', 'catch']
        for keyword in keywords:
            analysis['complexity_estimate'] += content.count(keyword)
            
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        if analysis['has_async']:
            analysis['quality_score'] += 0.1
        if analysis['has_linq']:
            analysis['quality_score'] += 0.05
        if len(analysis['usings']) > 5:
            analysis['quality_score'] += 0.05
        if analysis['methods'] > 0:
            analysis['quality_score'] += 0.1
            
        analysis['quality_score'] = min(analysis['quality_score'], 1.0)
        
        return analysis
        
    async def collect_nuget_packages_async(self):
        """NuGet íŒ¨í‚¤ì§€ ì •ë³´ ìˆ˜ì§‘"""
        logger.info("ğŸ“¦ NuGet íŒ¨í‚¤ì§€ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['nuget_packages']
        collected = 0
        
        for package_name in category_info['packages']:
            try:
                # NuGet API
                api_url = f"https://api.nuget.org/v3-flatcontainer/{package_name.lower()}/index.json"
                
                async with self.session.get(api_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        versions = data.get('versions', [])
                        
                        if versions:
                            # ìµœì‹  ë²„ì „ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            latest_version = versions[-1]
                            
                            # íŒ¨í‚¤ì§€ ë©”íƒ€ë°ì´í„°
                            metadata_url = f"https://api.nuget.org/v3/registration5-semver1/{package_name.lower()}/{latest_version}.json"
                            
                            async with self.session.get(metadata_url) as meta_response:
                                if meta_response.status == 200:
                                    metadata = await meta_response.json()
                                    
                                    # ì¹´íƒˆë¡œê·¸ ì—”íŠ¸ë¦¬
                                    catalog_entry = metadata.get('catalogEntry', {})
                                    
                                    package_data = {
                                        'name': package_name,
                                        'version': latest_version,
                                        'description': catalog_entry.get('description', ''),
                                        'authors': catalog_entry.get('authors', ''),
                                        'project_url': catalog_entry.get('projectUrl', ''),
                                        'repository': catalog_entry.get('repository', {}),
                                        'tags': catalog_entry.get('tags', []),
                                        'dependencies': catalog_entry.get('dependencyGroups', []),
                                        'download_count': 0,  # ë³„ë„ API í•„ìš”
                                        'collected_at': datetime.now().isoformat()
                                    }
                                    
                                    # README ê°€ì ¸ì˜¤ê¸°
                                    readme_url = catalog_entry.get('readmeUrl')
                                    if readme_url:
                                        readme_content = await self.fetch_url_async(readme_url)
                                        if readme_content:
                                            package_data['readme'] = readme_content
                                            
                                    # íŒŒì¼ ì €ì¥
                                    file_path = category_info['path'] / f"{package_name}.json"
                                    await self.save_json_async(file_path, package_data)
                                    
                                    # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                                    self.save_to_database(
                                        url=f"https://www.nuget.org/packages/{package_name}",
                                        category='nuget_packages',
                                        subcategory='package',
                                        title=package_name,
                                        content_type='package_info',
                                        file_path=str(file_path),
                                        size_bytes=len(json.dumps(package_data)),
                                        quality_score=0.9
                                    )
                                    
                                    collected += 1
                                    
            except Exception as e:
                logger.error(f"NuGet íŒ¨í‚¤ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜ ({package_name}): {e}")
                
        self.stats['categories']['nuget_packages'] = collected
        logger.info(f"âœ… NuGet íŒ¨í‚¤ì§€ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    async def collect_stackoverflow_async(self):
        """Stack Overflow ê³ ê¸‰ Q&A ìˆ˜ì§‘"""
        logger.info("ğŸ† Stack Overflow ê³ ê¸‰ Q&A ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['stackoverflow_advanced']
        collected = 0
        
        # Stack Exchange API
        base_url = "https://api.stackexchange.com/2.3/questions"
        
        for tag in category_info['tags']:
            try:
                params = {
                    'order': 'desc',
                    'sort': 'votes',
                    'tagged': tag,
                    'site': 'stackoverflow',
                    'filter': '!9_bDE(fI5',  # ë³¸ë¬¸ í¬í•¨
                    'pagesize': 20
                }
                
                async with self.session.get(base_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        questions = data.get('items', [])
                        
                        for question in questions:
                            # í’ˆì§ˆ í•„í„°ë§
                            if (question.get('score', 0) >= category_info['min_score'] and
                                question.get('view_count', 0) >= category_info['min_views']):
                                
                                # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
                                answers = await self.fetch_stackoverflow_answers(question['question_id'])
                                
                                qa_data = {
                                    'question_id': question['question_id'],
                                    'title': question['title'],
                                    'body': question.get('body', ''),
                                    'tags': question['tags'],
                                    'score': question['score'],
                                    'view_count': question['view_count'],
                                    'creation_date': question['creation_date'],
                                    'link': question['link'],
                                    'answers': answers,
                                    'collected_at': datetime.now().isoformat()
                                }
                                
                                # ì½”ë“œ ìƒ˜í”Œ ì¶”ì¶œ
                                code_samples = self.extract_code_from_html(question.get('body', ''))
                                for answer in answers:
                                    code_samples.extend(
                                        self.extract_code_from_html(answer.get('body', ''))
                                    )
                                qa_data['code_samples'] = code_samples
                                
                                # íŒŒì¼ ì €ì¥
                                file_name = f"{tag.replace('+', '_')}_{question['question_id']}.json"
                                file_path = category_info['path'] / file_name
                                
                                await self.save_json_async(file_path, qa_data)
                                
                                # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                                self.save_to_database(
                                    url=question['link'],
                                    category='stackoverflow_advanced',
                                    subcategory=tag,
                                    title=question['title'],
                                    content_type='qa',
                                    file_path=str(file_path),
                                    size_bytes=len(json.dumps(qa_data)),
                                    quality_score=min(question['score'] / 100, 1.0)
                                )
                                
                                collected += 1
                                
                                if collected >= 100:  # ìµœëŒ€ 100ê°œ
                                    break
                                    
            except Exception as e:
                logger.error(f"Stack Overflow ìˆ˜ì§‘ ì˜¤ë¥˜ ({tag}): {e}")
                
        self.stats['categories']['stackoverflow_advanced'] = collected
        logger.info(f"âœ… Stack Overflow Q&A {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    async def fetch_stackoverflow_answers(self, question_id: int) -> List[Dict]:
        """Stack Overflow ë‹µë³€ ê°€ì ¸ì˜¤ê¸°"""
        answers = []
        
        try:
            url = f"https://api.stackexchange.com/2.3/questions/{question_id}/answers"
            params = {
                'order': 'desc',
                'sort': 'votes',
                'site': 'stackoverflow',
                'filter': '!9_bDE(fI5'  # ë³¸ë¬¸ í¬í•¨
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for answer in data.get('items', [])[:3]:  # ìƒìœ„ 3ê°œ ë‹µë³€
                        answers.append({
                            'answer_id': answer['answer_id'],
                            'body': answer.get('body', ''),
                            'score': answer['score'],
                            'is_accepted': answer.get('is_accepted', False)
                        })
                        
        except Exception as e:
            logger.debug(f"ë‹µë³€ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            
        return answers
        
    def extract_code_from_html(self, html_content: str) -> List[Dict]:
        """HTMLì—ì„œ ì½”ë“œ ì¶”ì¶œ"""
        code_samples = []
        
        if not html_content:
            return code_samples
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # <code> íƒœê·¸ ì°¾ê¸°
        for code_tag in soup.find_all('code'):
            code_text = code_tag.get_text(strip=True)
            
            # C# ì½”ë“œì¸ì§€ í™•ì¸ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            if any(keyword in code_text for keyword in ['class', 'public', 'private', 'using', 'namespace', 'var']):
                code_samples.append({
                    'code': code_text,
                    'type': 'inline' if len(code_text) < 100 else 'block'
                })
                
        # <pre><code> ë¸”ë¡
        for pre_tag in soup.find_all('pre'):
            code_tag = pre_tag.find('code')
            if code_tag:
                code_text = code_tag.get_text(strip=True)
                code_samples.append({
                    'code': code_text,
                    'type': 'block'
                })
                
        return code_samples
        
    async def collect_expert_blogs_async(self):
        """ì „ë¬¸ê°€ ë¸”ë¡œê·¸ ìˆ˜ì§‘"""
        logger.info("ğŸ“ ì „ë¬¸ê°€ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['expert_blogs']
        collected = 0
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ë¸”ë¡œê·¸ì˜ RSS í”¼ë“œë‚˜ ì‚¬ì´íŠ¸ë§µì„ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ìˆ˜ì§‘
        
        for blog in category_info['sources']:
            try:
                blog_data = {
                    'name': blog['name'],
                    'url': blog['url'],
                    'type': 'blog',
                    'topics': self.get_expert_topics(blog['name']),
                    'sample_posts': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # RSS í”¼ë“œ í™•ì¸
                rss_url = await self.find_rss_feed(blog['url'])
                if rss_url:
                    posts = await self.parse_rss_feed(rss_url)
                    blog_data['sample_posts'] = posts[:10]  # ìµœê·¼ 10ê°œ
                    
                # íŒŒì¼ ì €ì¥
                file_name = f"{blog['name'].replace(' ', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, blog_data)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                self.save_to_database(
                    url=blog['url'],
                    category='expert_blogs',
                    subcategory='blog',
                    title=blog['name'],
                    content_type='blog_info',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(blog_data)),
                    quality_score=0.95
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì˜¤ë¥˜ ({blog['name']}): {e}")
                
        self.stats['categories']['expert_blogs'] = collected
        logger.info(f"âœ… ì „ë¬¸ê°€ ë¸”ë¡œê·¸ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    def get_expert_topics(self, expert_name: str) -> List[str]:
        """ì „ë¬¸ê°€ë³„ ì£¼ìš” í† í”½"""
        topics_map = {
            'Jon Skeet': ['async/await', 'DateTime', 'strings', 'LINQ'],
            'Eric Lippert': ['language design', 'performance', 'compiler'],
            'Stephen Cleary': ['async/await', 'concurrency', 'tasks'],
            'Steve Gordon': ['ASP.NET Core', 'performance', 'HttpClient'],
            'Andrew Lock': ['ASP.NET Core', 'Docker', 'configuration'],
            'Nick Chapsas': ['performance', 'best practices', 'new features'],
            'David Fowler': ['ASP.NET Core', 'SignalR', 'performance']
        }
        
        return topics_map.get(expert_name, ['C#', '.NET'])
        
    async def find_rss_feed(self, blog_url: str) -> Optional[str]:
        """RSS í”¼ë“œ URL ì°¾ê¸°"""
        try:
            content = await self.fetch_url_async(blog_url)
            if content:
                soup = BeautifulSoup(content, 'html.parser')
                
                # RSS ë§í¬ ì°¾ê¸°
                rss_link = soup.find('link', {'type': 'application/rss+xml'})
                if rss_link and rss_link.get('href'):
                    return urljoin(blog_url, rss_link['href'])
                    
                # ì¼ë°˜ì ì¸ RSS ê²½ë¡œ ì‹œë„
                common_paths = ['/rss', '/feed', '/rss.xml', '/feed.xml', '/atom.xml']
                for path in common_paths:
                    test_url = urljoin(blog_url, path)
                    if await self.url_exists(test_url):
                        return test_url
                        
        except Exception as e:
            logger.debug(f"RSS í”¼ë“œ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            
        return None
        
    async def url_exists(self, url: str) -> bool:
        """URL ì¡´ì¬ í™•ì¸"""
        try:
            async with self.session.head(url) as response:
                return response.status == 200
        except:
            return False
            
    async def parse_rss_feed(self, rss_url: str) -> List[Dict]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        posts = []
        
        try:
            content = await self.fetch_url_async(rss_url)
            if content:
                root = ET.fromstring(content)
                
                # RSS 2.0
                for item in root.findall('.//item')[:10]:
                    post = {
                        'title': item.findtext('title', ''),
                        'link': item.findtext('link', ''),
                        'pubDate': item.findtext('pubDate', ''),
                        'description': item.findtext('description', '')
                    }
                    posts.append(post)
                    
        except Exception as e:
            logger.debug(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            
        return posts
        
    async def collect_performance_guides_async(self):
        """ì„±ëŠ¥ ê°€ì´ë“œ ìˆ˜ì§‘"""
        logger.info("âš¡ ì„±ëŠ¥ ê°€ì´ë“œ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['performance_guides']
        collected = 0
        
        performance_topics = [
            'string-performance',
            'collection-performance',
            'linq-performance',
            'async-performance',
            'memory-allocation',
            'span-memory',
            'object-pooling',
            'caching-strategies',
            'profiling-tools',
            'benchmarking'
        ]
        
        for topic in performance_topics:
            try:
                guide_data = {
                    'topic': topic,
                    'title': topic.replace('-', ' ').title(),
                    'sources': [],
                    'best_practices': [],
                    'code_examples': [],
                    'benchmarks': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # ê° ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
                for source_url in category_info['sources']:
                    if 'docs.microsoft.com' in source_url:
                        # Microsoft ë¬¸ì„œì—ì„œ ì„±ëŠ¥ ê°€ì´ë“œ ì°¾ê¸°
                        search_url = f"{source_url}{topic}"
                        content = await self.fetch_url_async(search_url)
                        
                        if content:
                            guide_data['sources'].append({
                                'url': search_url,
                                'type': 'microsoft_docs'
                            })
                            
                # íŒŒì¼ ì €ì¥
                file_path = category_info['path'] / f"{topic}.json"
                await self.save_json_async(file_path, guide_data)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                self.save_to_database(
                    url=f"performance-guide-{topic}",
                    category='performance_guides',
                    subcategory='guide',
                    title=guide_data['title'],
                    content_type='guide',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(guide_data)),
                    quality_score=0.9
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"ì„±ëŠ¥ ê°€ì´ë“œ ìˆ˜ì§‘ ì˜¤ë¥˜ ({topic}): {e}")
                
        self.stats['categories']['performance_guides'] = collected
        logger.info(f"âœ… ì„±ëŠ¥ ê°€ì´ë“œ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    async def collect_design_patterns_async(self):
        """ë””ìì¸ íŒ¨í„´ ìˆ˜ì§‘"""
        logger.info("ğŸ¨ ë””ìì¸ íŒ¨í„´ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['design_patterns']
        collected = 0
        
        for pattern in category_info['patterns']:
            try:
                pattern_data = {
                    'name': pattern,
                    'category': self.get_pattern_category(pattern),
                    'intent': '',
                    'structure': {},
                    'participants': [],
                    'implementation': {
                        'csharp': ''
                    },
                    'example_usage': '',
                    'when_to_use': [],
                    'pros': [],
                    'cons': [],
                    'related_patterns': [],
                    'collected_at': datetime.now().isoformat()
                }
                
                # íŒ¨í„´ ì •ë³´ ì¶”ê°€ (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘)
                pattern_data.update(self.get_pattern_details(pattern))
                
                # íŒŒì¼ ì €ì¥
                file_name = f"{pattern.replace(' ', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, pattern_data)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                self.save_to_database(
                    url=f"pattern-{pattern}",
                    category='design_patterns',
                    subcategory=pattern_data['category'],
                    title=pattern,
                    content_type='pattern',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(pattern_data)),
                    quality_score=0.95
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"íŒ¨í„´ ìˆ˜ì§‘ ì˜¤ë¥˜ ({pattern}): {e}")
                
        self.stats['categories']['design_patterns'] = collected
        logger.info(f"âœ… ë””ìì¸ íŒ¨í„´ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    def get_pattern_category(self, pattern: str) -> str:
        """íŒ¨í„´ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        categories = {
            'Creational': ['Singleton', 'Factory Method', 'Abstract Factory', 'Builder', 'Prototype'],
            'Structural': ['Adapter', 'Bridge', 'Composite', 'Decorator', 'Facade', 'Flyweight', 'Proxy'],
            'Behavioral': ['Chain of Responsibility', 'Command', 'Iterator', 'Mediator', 'Memento',
                          'Observer', 'State', 'Strategy', 'Template Method', 'Visitor'],
            'Architectural': ['Repository', 'Unit of Work', 'Specification', 'CQRS', 'Event Sourcing']
        }
        
        for category, patterns in categories.items():
            if pattern in patterns:
                return category
                
        return 'Other'
        
    def get_pattern_details(self, pattern: str) -> Dict:
        """íŒ¨í„´ ìƒì„¸ ì •ë³´"""
        # ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„°
        
        details = {
            'Singleton': {
                'intent': 'Ensure a class has only one instance and provide a global point of access to it.',
                'implementation': {
                    'csharp': '''public sealed class Singleton
{
    private static readonly Lazy<Singleton> lazy = 
        new Lazy<Singleton>(() => new Singleton());
    
    public static Singleton Instance => lazy.Value;
    
    private Singleton()
    {
    }
}'''
                },
                'when_to_use': [
                    'When exactly one instance of a class is needed',
                    'When the single instance should be extensible by subclassing'
                ],
                'pros': ['Controlled access to sole instance', 'Reduced namespace pollution'],
                'cons': ['Difficult to test', 'Violates Single Responsibility Principle']
            }
        }
        
        return details.get(pattern, {})
        
    async def collect_unity_practices_async(self):
        """Unity ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ìˆ˜ì§‘"""
        logger.info("ğŸ® Unity ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        category_info = self.categories['unity_best_practices']
        collected = 0
        
        for topic in category_info['topics']:
            try:
                practice_data = {
                    'topic': topic,
                    'description': '',
                    'best_practices': [],
                    'common_mistakes': [],
                    'code_examples': [],
                    'performance_tips': [],
                    'unity_version': '2022.3 LTS',
                    'collected_at': datetime.now().isoformat()
                }
                
                # Unity íŠ¹í™” ì •ë³´ ì¶”ê°€
                practice_data.update(self.get_unity_practice_details(topic))
                
                # íŒŒì¼ ì €ì¥
                file_name = f"{topic.replace(' ', '_').replace('/', '_')}.json"
                file_path = category_info['path'] / file_name
                
                await self.save_json_async(file_path, practice_data)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
                self.save_to_database(
                    url=f"unity-practice-{topic}",
                    category='unity_best_practices',
                    subcategory='practice',
                    title=topic,
                    content_type='unity_guide',
                    file_path=str(file_path),
                    size_bytes=len(json.dumps(practice_data)),
                    quality_score=0.9
                )
                
                collected += 1
                
            except Exception as e:
                logger.error(f"Unity í”„ë™í‹°ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({topic}): {e}")
                
        self.stats['categories']['unity_best_practices'] = collected
        logger.info(f"âœ… Unity ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ {collected}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
    def get_unity_practice_details(self, topic: str) -> Dict:
        """Unity í”„ë™í‹°ìŠ¤ ìƒì„¸ ì •ë³´"""
        details = {
            'Object pooling': {
                'description': 'Reuse objects instead of instantiating and destroying them repeatedly',
                'best_practices': [
                    'Pre-instantiate objects at scene load',
                    'Use generic pool implementation',
                    'Reset object state when returning to pool'
                ],
                'code_examples': [
                    {
                        'title': 'Generic Object Pool',
                        'code': '''public class ObjectPool<T> where T : MonoBehaviour
{
    private Queue<T> pool = new Queue<T>();
    private T prefab;
    private Transform parent;
    
    public ObjectPool(T prefab, int initialSize, Transform parent = null)
    {
        this.prefab = prefab;
        this.parent = parent;
        
        for (int i = 0; i < initialSize; i++)
        {
            T obj = Object.Instantiate(prefab, parent);
            obj.gameObject.SetActive(false);
            pool.Enqueue(obj);
        }
    }
    
    public T Get()
    {
        if (pool.Count > 0)
        {
            T obj = pool.Dequeue();
            obj.gameObject.SetActive(true);
            return obj;
        }
        else
        {
            return Object.Instantiate(prefab, parent);
        }
    }
    
    public void Return(T obj)
    {
        obj.gameObject.SetActive(false);
        pool.Enqueue(obj);
    }
}'''
                    }
                ]
            }
        }
        
        return details.get(topic, {})
        
    async def fetch_url_async(self, url: str) -> Optional[str]:
        """URL ë‚´ìš© ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with self.session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    return await response.text()
                    
        except asyncio.TimeoutError:
            logger.warning(f"íƒ€ì„ì•„ì›ƒ: {url}")
        except Exception as e:
            logger.debug(f"URL ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ ({url}): {e}")
            
        return None
        
    async def save_json_async(self, file_path: Path, data: Dict):
        """JSON íŒŒì¼ ë¹„ë™ê¸° ì €ì¥"""
        try:
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(data, indent=2, ensure_ascii=False))
                
            # íŒŒì¼ í¬ê¸° ì—…ë°ì´íŠ¸
            size_mb = file_path.stat().st_size / 1024 / 1024
            self.stats['total_size_mb'] += size_mb
            self.stats['total_collected'] += 1
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜ ({file_path}): {e}")
            
    def save_to_database(self, **kwargs):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ìˆ˜ì§‘ ì •ë³´ ì €ì¥"""
        db_path = self.output_dir / 'collection_index.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
            INSERT OR REPLACE INTO collected_data 
            (url, category, subcategory, title, content_type, 
             file_path, size_bytes, quality_score, collected_at, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                kwargs.get('url'),
                kwargs.get('category'),
                kwargs.get('subcategory'),
                kwargs.get('title'),
                kwargs.get('content_type'),
                kwargs.get('file_path'),
                kwargs.get('size_bytes', 0),
                kwargs.get('quality_score', 0.7),
                datetime.now(),
                json.dumps(kwargs.get('metadata', {}))
            ))
            
            conn.commit()
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
        finally:
            conn.close()
            
    def save_statistics(self):
        """ìˆ˜ì§‘ í†µê³„ ì €ì¥"""
        self.stats['end_time'] = datetime.now()
        self.stats['duration'] = str(self.stats['end_time'] - self.stats['start_time'])
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        stats_file = self.output_dir / 'deep_collection_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, default=str)
            
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()
        
    def generate_report(self):
        """ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""# ì‹¬ì¸µ C# ë°ì´í„° ìˆ˜ì§‘ ë¦¬í¬íŠ¸

## ğŸ“Š ìˆ˜ì§‘ í†µê³„
- **ì‹œì‘ ì‹œê°„**: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **ì¢…ë£Œ ì‹œê°„**: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
- **ì†Œìš” ì‹œê°„**: {self.stats['duration']}
- **ì´ ìˆ˜ì§‘ í•­ëª©**: {self.stats['total_collected']}ê°œ
- **ì´ ë°ì´í„° í¬ê¸°**: {self.stats['total_size_mb']:.2f}MB

## ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©
"""
        
        for category, count in self.stats['categories'].items():
            report += f"- **{category}**: {count}ê°œ\n"
            
        if self.stats['errors']:
            report += f"\n## âš ï¸ ì˜¤ë¥˜ ë°œìƒ\n"
            for error in self.stats['errors'][:10]:  # ìµœëŒ€ 10ê°œ
                report += f"- {error}\n"
                
        report += f"""
## ğŸ¯ ìˆ˜ì§‘ í’ˆì§ˆ
- Microsoft ê³µì‹ ë¬¸ì„œ: ê³ í’ˆì§ˆ (0.95)
- GitHub ì „ë¬¸ê°€ í”„ë¡œì íŠ¸: ê³ í’ˆì§ˆ (0.7-0.9)
- Stack Overflow ë‹µë³€: íˆ¬í‘œ ê¸°ë°˜ í’ˆì§ˆ
- NuGet íŒ¨í‚¤ì§€: ê²€ì¦ëœ íŒ¨í‚¤ì§€ (0.9)

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
{self.output_dir}/
â”œâ”€â”€ microsoft_docs/          # MS ê³µì‹ ë¬¸ì„œ
â”œâ”€â”€ github_samples/          # GitHub ìƒ˜í”Œ ì½”ë“œ
â”œâ”€â”€ nuget_packages/          # NuGet íŒ¨í‚¤ì§€ ì •ë³´
â”œâ”€â”€ stackoverflow_advanced/  # SO ê³ ê¸‰ Q&A
â”œâ”€â”€ expert_blogs/           # ì „ë¬¸ê°€ ë¸”ë¡œê·¸
â”œâ”€â”€ performance_guides/     # ì„±ëŠ¥ ê°€ì´ë“œ
â”œâ”€â”€ design_patterns/        # ë””ìì¸ íŒ¨í„´
â”œâ”€â”€ unity_best_practices/   # Unity ê°€ì´ë“œ
â””â”€â”€ collection_index.db     # ìˆ˜ì§‘ ì¸ë±ìŠ¤ DB
```

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„
1. `autoci data index` - ìˆ˜ì§‘ëœ ë°ì´í„° ì¸ë±ì‹±
2. `autoci dual start` - RAG + íŒŒì¸íŠœë‹ ì‹œì‘
3. `autoci enhance start` - 24ì‹œê°„ ìë™ ì‹œìŠ¤í…œ ì‹œì‘

---
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_file = self.output_dir / 'deep_collection_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"ğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = DeepCSharpCollector()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(collector.collect_all_async())
    
    print("\nâœ… ì‹¬ì¸µ C# ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {collector.output_dir}")
    print(f"ğŸ“Š ì´ {collector.stats['total_collected']}ê°œ í•­ëª© ìˆ˜ì§‘")
    print(f"ğŸ’¾ ì´ {collector.stats['total_size_mb']:.2f}MB ë°ì´í„°")
    print("\nğŸ“‹ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¸ë±ì‹±ì„ ì‹œì‘í•˜ì„¸ìš”: autoci data index")


if __name__ == "__main__":
    main()