#!/usr/bin/env python3
"""
AutoCI í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ë° ìµœì í™” ì‹œìŠ¤í…œ
ì ì‘í˜• í•™ìŠµìœ¨, ë™ì  ë°°ì¹˜ í¬ê¸°, ì„±ëŠ¥ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
"""

import os
import sys
import time
import json
import sqlite3
import threading
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import torch
    import torch.optim as optim
    from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdaptiveLearningScheduler:
    """ì ì‘í˜• í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, initial_lr: float = 0.001, min_lr: float = 1e-6, max_lr: float = 0.01):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = []
        self.loss_history = []
        self.lr_history = []
        
        # ì ì‘í˜• íŒŒë¼ë¯¸í„°
        self.patience = 5  # ì„±ëŠ¥ ê°œì„  ì—†ì„ ë•Œ ëŒ€ê¸° íšŸìˆ˜
        self.factor = 0.5  # í•™ìŠµìœ¨ ê°ì†Œ ë¹„ìœ¨
        self.improvement_threshold = 0.01  # ê°œì„  ì„ê³„ê°’
        self.no_improvement_count = 0
        
        # í•™ìŠµìœ¨ ì¦ê°€/ê°ì†Œ ì „ëµ
        self.lr_strategy = "adaptive"  # adaptive, cosine, plateau
        
        logger.info(f"ğŸ¯ ì ì‘í˜• í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”: LR={initial_lr}")
    
    def update_performance(self, loss: float, accuracy: float, epoch: int):
        """ì„±ëŠ¥ ì—…ë°ì´íŠ¸ ë° í•™ìŠµìœ¨ ì¡°ì •"""
        
        # ì„±ëŠ¥ ê¸°ë¡
        performance_score = accuracy - (loss * 0.1)  # ì •í™•ë„ ìš°ì„ , ì†ì‹¤ íŒ¨ë„í‹°
        self.performance_history.append(performance_score)
        self.loss_history.append(loss)
        self.lr_history.append(self.current_lr)
        
        # ìµœê·¼ ì„±ëŠ¥ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(self.performance_history) > 50:
            self.performance_history = self.performance_history[-50:]
            self.loss_history = self.loss_history[-50:]
            self.lr_history = self.lr_history[-50:]
        
        # í•™ìŠµìœ¨ ì¡°ì •
        self._adjust_learning_rate(performance_score, epoch)
        
        logger.info(f"ğŸ“ˆ ì„±ëŠ¥ ì—…ë°ì´íŠ¸: ì†ì‹¤={loss:.4f}, ì •í™•ë„={accuracy:.4f}, LR={self.current_lr:.6f}")
    
    def _adjust_learning_rate(self, current_performance: float, epoch: int):
        """í•™ìŠµìœ¨ ì¡°ì • ë¡œì§"""
        
        if len(self.performance_history) < 2:
            return
        
        if self.lr_strategy == "adaptive":
            self._adaptive_lr_adjustment(current_performance)
        elif self.lr_strategy == "cosine":
            self._cosine_lr_adjustment(epoch)
        elif self.lr_strategy == "plateau":
            self._plateau_lr_adjustment(current_performance)
    
    def _adaptive_lr_adjustment(self, current_performance: float):
        """ì ì‘í˜• í•™ìŠµìœ¨ ì¡°ì •"""
        
        if len(self.performance_history) < 2:
            return
        
        recent_avg = sum(self.performance_history[-3:]) / min(3, len(self.performance_history))
        older_avg = sum(self.performance_history[-6:-3]) / max(1, min(3, len(self.performance_history) - 3))
        
        improvement = recent_avg - older_avg
        
        if improvement > self.improvement_threshold:
            # ì„±ëŠ¥ ê°œì„  ì¤‘ - í•™ìŠµìœ¨ ì•½ê°„ ì¦ê°€
            self.current_lr = min(self.current_lr * 1.05, self.max_lr)
            self.no_improvement_count = 0
            logger.info(f"ğŸ”¼ ì„±ëŠ¥ ê°œì„  ê°ì§€, í•™ìŠµìœ¨ ì¦ê°€: {self.current_lr:.6f}")
            
        elif improvement < -self.improvement_threshold:
            # ì„±ëŠ¥ ì•…í™” - í•™ìŠµìœ¨ ê°ì†Œ
            self.no_improvement_count += 1
            if self.no_improvement_count >= self.patience:
                self.current_lr = max(self.current_lr * self.factor, self.min_lr)
                self.no_improvement_count = 0
                logger.info(f"ğŸ”½ ì„±ëŠ¥ ì•…í™” ê°ì§€, í•™ìŠµìœ¨ ê°ì†Œ: {self.current_lr:.6f}")
        
        else:
            # ì„±ëŠ¥ ì •ì²´
            self.no_improvement_count += 1
            if self.no_improvement_count >= self.patience * 2:
                # í•™ìŠµìœ¨ì„ ë¦¬ì…‹í•˜ì—¬ local minimum íƒˆì¶œ ì‹œë„
                self.current_lr = self.initial_lr
                self.no_improvement_count = 0
                logger.info(f"ğŸ”„ ì„±ëŠ¥ ì •ì²´, í•™ìŠµìœ¨ ë¦¬ì…‹: {self.current_lr:.6f}")
    
    def _cosine_lr_adjustment(self, epoch: int):
        """ì½”ì‚¬ì¸ ì–´ë‹ë§ í•™ìŠµìœ¨ ì¡°ì •"""
        T_max = 100  # ì£¼ê¸°
        eta_min = self.min_lr
        
        self.current_lr = eta_min + (self.initial_lr - eta_min) * (1 + math.cos(math.pi * epoch / T_max)) / 2
        self.current_lr = max(self.current_lr, self.min_lr)
    
    def _plateau_lr_adjustment(self, current_performance: float):
        """í”Œë˜í†  ê¸°ë°˜ í•™ìŠµìœ¨ ì¡°ì •"""
        if len(self.performance_history) < 5:
            return
        
        # ìµœê·¼ 5íšŒ ì„±ëŠ¥ì˜ ìµœëŒ€ê°’ê³¼ ë¹„êµ
        recent_max = max(self.performance_history[-5:])
        
        if current_performance < recent_max - self.improvement_threshold:
            self.no_improvement_count += 1
        else:
            self.no_improvement_count = 0
        
        if self.no_improvement_count >= self.patience:
            self.current_lr = max(self.current_lr * self.factor, self.min_lr)
            self.no_improvement_count = 0
            logger.info(f"ğŸ¯ í”Œë˜í†  ê°ì§€, í•™ìŠµìœ¨ ê°ì†Œ: {self.current_lr:.6f}")
    
    def get_current_lr(self) -> float:
        """í˜„ì¬ í•™ìŠµìœ¨ ë°˜í™˜"""
        return self.current_lr
    
    def get_lr_stats(self) -> Dict:
        """í•™ìŠµìœ¨ í†µê³„ ë°˜í™˜"""
        return {
            "current_lr": self.current_lr,
            "initial_lr": self.initial_lr,
            "min_lr": self.min_lr,
            "max_lr": self.max_lr,
            "no_improvement_count": self.no_improvement_count,
            "performance_trend": self._calculate_trend(),
            "lr_history": self.lr_history[-10:],  # ìµœê·¼ 10ê°œ
            "performance_history": self.performance_history[-10:]
        }
    
    def _calculate_trend(self) -> str:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(self.performance_history) < 3:
            return "insufficient_data"
        
        recent = self.performance_history[-3:]
        if len(set(recent)) == 1:
            return "stable"
        
        if recent[-1] > recent[0]:
            return "improving"
        elif recent[-1] < recent[0]:
            return "declining"
        else:
            return "stable"

class DynamicBatchSizer:
    """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê¸°"""
    
    def __init__(self, initial_batch_size: int = 16, min_batch_size: int = 4, max_batch_size: int = 128):
        self.initial_batch_size = initial_batch_size
        self.current_batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        
        # ì„±ëŠ¥ ì¶”ì 
        self.throughput_history = []  # ì²˜ë¦¬ëŸ‰ (samples/sec)
        self.memory_usage_history = []
        self.loss_convergence_history = []
        
        # ì¡°ì • íŒŒë¼ë¯¸í„°
        self.adjustment_factor = 1.2
        self.memory_threshold = 0.8  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì„ê³„ê°’
        
        logger.info(f"ğŸ“¦ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê¸° ì´ˆê¸°í™”: {initial_batch_size}")
    
    def update_metrics(self, processing_time: float, samples_processed: int, 
                      memory_usage: float, loss_improvement: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
        throughput = samples_processed / max(processing_time, 0.001)
        self.throughput_history.append(throughput)
        self.memory_usage_history.append(memory_usage)
        self.loss_convergence_history.append(loss_improvement)
        
        # ìµœê·¼ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.throughput_history) > 20:
            self.throughput_history = self.throughput_history[-20:]
            self.memory_usage_history = self.memory_usage_history[-20:]
            self.loss_convergence_history = self.loss_convergence_history[-20:]
        
        # ë°°ì¹˜ í¬ê¸° ì¡°ì •
        self._adjust_batch_size(throughput, memory_usage, loss_improvement)
    
    def _adjust_batch_size(self, current_throughput: float, memory_usage: float, loss_improvement: float):
        """ë°°ì¹˜ í¬ê¸° ì¡°ì • ë¡œì§"""
        
        if len(self.throughput_history) < 3:
            return
        
        # ìµœê·¼ ì²˜ë¦¬ëŸ‰ íŠ¸ë Œë“œ
        recent_throughput = sum(self.throughput_history[-3:]) / 3
        older_throughput = sum(self.throughput_history[-6:-3]) / max(1, min(3, len(self.throughput_history) - 3))
        
        throughput_improvement = recent_throughput - older_throughput
        
        # ì¡°ì • ê²°ì •
        should_increase = (
            throughput_improvement > 0 and 
            memory_usage < self.memory_threshold and 
            loss_improvement > 0 and
            self.current_batch_size < self.max_batch_size
        )
        
        should_decrease = (
            memory_usage > self.memory_threshold or 
            throughput_improvement < -0.1 or
            loss_improvement < -0.01
        ) and self.current_batch_size > self.min_batch_size
        
        if should_increase:
            new_batch_size = min(int(self.current_batch_size * self.adjustment_factor), self.max_batch_size)
            if new_batch_size != self.current_batch_size:
                self.current_batch_size = new_batch_size
                logger.info(f"ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {self.current_batch_size}")
        
        elif should_decrease:
            new_batch_size = max(int(self.current_batch_size / self.adjustment_factor), self.min_batch_size)
            if new_batch_size != self.current_batch_size:
                self.current_batch_size = new_batch_size
                logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {self.current_batch_size}")
    
    def get_current_batch_size(self) -> int:
        """í˜„ì¬ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.current_batch_size
    
    def get_batch_stats(self) -> Dict:
        """ë°°ì¹˜ í¬ê¸° í†µê³„ ë°˜í™˜"""
        return {
            "current_batch_size": self.current_batch_size,
            "initial_batch_size": self.initial_batch_size,
            "min_batch_size": self.min_batch_size,
            "max_batch_size": self.max_batch_size,
            "avg_throughput": sum(self.throughput_history) / len(self.throughput_history) if self.throughput_history else 0,
            "avg_memory_usage": sum(self.memory_usage_history) / len(self.memory_usage_history) if self.memory_usage_history else 0,
            "throughput_history": self.throughput_history[-5:],
            "memory_usage_history": self.memory_usage_history[-5:]
        }

class PerformanceBasedScheduler:
    """ì„±ëŠ¥ ê¸°ë°˜ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        self.priority_queue = []  # (priority, task_type, task_data)
        self.task_performance = {}  # task_type -> performance_score
        self.execution_history = {}  # task_type -> [execution_times]
        
        # ì‘ì—… ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜
        self.task_weights = {
            "high_feedback_learning": 1.0,    # ë†’ì€ í”¼ë“œë°± ì ìˆ˜ ë°ì´í„°
            "error_correction": 0.9,          # ì˜¤ë¥˜ ìˆ˜ì • í•™ìŠµ
            "new_pattern_learning": 0.8,      # ìƒˆë¡œìš´ íŒ¨í„´ í•™ìŠµ
            "reinforcement_learning": 0.7,    # ê°•í™” í•™ìŠµ
            "general_learning": 0.5,          # ì¼ë°˜ í•™ìŠµ
            "maintenance": 0.3                # ìœ ì§€ë³´ìˆ˜ ì‘ì—…
        }
        
        logger.info("ğŸ¯ ì„±ëŠ¥ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”")
    
    def add_task(self, task_type: str, task_data: Dict, urgency: float = 0.5):
        """ì‘ì—… ì¶”ê°€"""
        
        # ìš°ì„ ìˆœìœ„ ê³„ì‚°
        base_priority = self.task_weights.get(task_type, 0.5)
        performance_bonus = self.task_performance.get(task_type, 0.5)
        urgency_factor = urgency
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜ (ì˜¤ë˜ëœ ì‘ì—…ì¼ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ì¦ê°€)
        time_weight = min(1.0, (time.time() - task_data.get('created_time', time.time())) / 3600)
        
        final_priority = (base_priority + performance_bonus + urgency_factor + time_weight) / 4
        
        self.priority_queue.append((final_priority, task_type, task_data))
        self.priority_queue.sort(key=lambda x: x[0], reverse=True)
        
        logger.info(f"ğŸ“‹ ì‘ì—… ì¶”ê°€: {task_type} (ìš°ì„ ìˆœìœ„: {final_priority:.3f})")
    
    def get_next_task(self) -> Optional[Tuple[str, Dict]]:
        """ë‹¤ìŒ ì‹¤í–‰í•  ì‘ì—… ë°˜í™˜"""
        if not self.priority_queue:
            return None
        
        priority, task_type, task_data = self.priority_queue.pop(0)
        return task_type, task_data
    
    def update_task_performance(self, task_type: str, performance_score: float, execution_time: float):
        """ì‘ì—… ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        
        # ì„±ëŠ¥ ì ìˆ˜ ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ì´ë™ í‰ê· )
        if task_type in self.task_performance:
            self.task_performance[task_type] = 0.7 * self.task_performance[task_type] + 0.3 * performance_score
        else:
            self.task_performance[task_type] = performance_score
        
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
        if task_type not in self.execution_history:
            self.execution_history[task_type] = []
        
        self.execution_history[task_type].append(execution_time)
        if len(self.execution_history[task_type]) > 10:
            self.execution_history[task_type] = self.execution_history[task_type][-10:]
        
        logger.info(f"ğŸ“Š ì„±ëŠ¥ ì—…ë°ì´íŠ¸: {task_type} -> {performance_score:.3f} (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}s)")
    
    def get_scheduler_stats(self) -> Dict:
        """ìŠ¤ì¼€ì¤„ëŸ¬ í†µê³„ ë°˜í™˜"""
        return {
            "queue_length": len(self.priority_queue),
            "task_performance": self.task_performance.copy(),
            "avg_execution_times": {
                task_type: sum(times) / len(times) 
                for task_type, times in self.execution_history.items()
            },
            "pending_tasks": [
                {"type": task_type, "priority": priority}
                for priority, task_type, _ in self.priority_queue[:5]
            ]
        }

class LearningOptimizerManager:
    """í•™ìŠµ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, db_path: str = "continuous_learning.db"):
        self.db_path = db_path
        self.lr_scheduler = AdaptiveLearningScheduler()
        self.batch_sizer = DynamicBatchSizer()
        self.task_scheduler = PerformanceBasedScheduler()
        
        # ìµœì í™” ìƒíƒœ
        self.optimization_enabled = True
        self.auto_tune_mode = True
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.global_performance_score = 0.5
        self.optimization_history = []
        
        logger.info("ğŸ›ï¸ í•™ìŠµ ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
    
    def optimize_learning_session(self, learning_data: List[Dict], model=None, optimizer=None) -> Dict:
        """í•™ìŠµ ì„¸ì…˜ ìµœì í™”"""
        
        start_time = time.time()
        
        # í˜„ì¬ ì„¤ì •
        current_lr = self.lr_scheduler.get_current_lr()
        current_batch_size = self.batch_sizer.get_current_batch_size()
        
        # í•™ìŠµ ì‹¤í–‰
        if TORCH_AVAILABLE and model and optimizer:
            results = self._pytorch_optimized_training(learning_data, model, optimizer, current_lr, current_batch_size)
        else:
            results = self._simulated_optimized_training(learning_data, current_batch_size)
        
        # ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        execution_time = time.time() - start_time
        self._update_optimizers(results, execution_time, len(learning_data))
        
        return {
            "optimization_applied": True,
            "learning_rate": current_lr,
            "batch_size": current_batch_size,
            "execution_time": execution_time,
            "performance_score": results.get("accuracy", 0.5),
            "loss": results.get("loss", 0.0),
            "samples_processed": len(learning_data)
        }
    
    def _pytorch_optimized_training(self, learning_data: List[Dict], model, optimizer, lr: float, batch_size: int) -> Dict:
        """PyTorch ìµœì í™” í•™ìŠµ"""
        
        # í•™ìŠµìœ¨ ì ìš©
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        total_loss = 0.0
        total_accuracy = 0.0
        batch_count = 0
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ
        for i in range(0, len(learning_data), batch_size):
            batch = learning_data[i:i + batch_size]
            
            # ì—¬ê¸°ì„œ ì‹¤ì œ PyTorch í•™ìŠµ ë¡œì§ êµ¬í˜„
            # (ê°„ì†Œí™”ëœ ë²„ì „)
            batch_loss = 0.5 + (len(batch) * 0.01)  # ì‹œë®¬ë ˆì´ì…˜
            batch_accuracy = 0.7 + (batch_size * 0.001)  # ì‹œë®¬ë ˆì´ì…˜
            
            total_loss += batch_loss
            total_accuracy += batch_accuracy
            batch_count += 1
        
        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0
        avg_accuracy = total_accuracy / batch_count if batch_count > 0 else 0.0
        
        return {
            "loss": avg_loss,
            "accuracy": avg_accuracy,
            "batches_processed": batch_count
        }
    
    def _simulated_optimized_training(self, learning_data: List[Dict], batch_size: int) -> Dict:
        """ì‹œë®¬ë ˆì´ì…˜ ìµœì í™” í•™ìŠµ"""
        
        # ì‹œë®¬ë ˆì´ì…˜ ë¡œì§
        data_quality = sum(d.get("quality", 0.5) for d in learning_data) / len(learning_data)
        batch_efficiency = min(1.0, batch_size / 32)
        
        simulated_loss = max(0.1, 1.0 - data_quality * batch_efficiency)
        simulated_accuracy = min(0.95, data_quality * batch_efficiency + 0.3)
        
        return {
            "loss": simulated_loss,
            "accuracy": simulated_accuracy,
            "batches_processed": len(learning_data) // batch_size + 1
        }
    
    def _update_optimizers(self, results: Dict, execution_time: float, data_size: int):
        """ìµœì í™”ê¸°ë“¤ ì—…ë°ì´íŠ¸"""
        
        loss = results.get("loss", 0.0)
        accuracy = results.get("accuracy", 0.5)
        
        # í•™ìŠµìœ¨ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        self.lr_scheduler.update_performance(loss, accuracy, len(self.optimization_history))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¶”ì • (ì‹œë®¬ë ˆì´ì…˜)
        estimated_memory_usage = min(0.9, data_size / 1000 * 0.1)
        loss_improvement = self.global_performance_score - loss
        
        # ë°°ì¹˜ í¬ê¸° ì¡°ì •ê¸° ì—…ë°ì´íŠ¸
        self.batch_sizer.update_metrics(
            processing_time=execution_time,
            samples_processed=data_size,
            memory_usage=estimated_memory_usage,
            loss_improvement=loss_improvement
        )
        
        # ì „ì—­ ì„±ëŠ¥ ì ìˆ˜ ì—…ë°ì´íŠ¸
        self.global_performance_score = 0.8 * self.global_performance_score + 0.2 * accuracy
        
        # ìµœì í™” ê¸°ë¡
        optimization_record = {
            "timestamp": datetime.now().isoformat(),
            "learning_rate": self.lr_scheduler.get_current_lr(),
            "batch_size": self.batch_sizer.get_current_batch_size(),
            "performance_score": self.global_performance_score,
            "loss": loss,
            "accuracy": accuracy,
            "execution_time": execution_time
        }
        
        self.optimization_history.append(optimization_record)
        if len(self.optimization_history) > 100:
            self.optimization_history = self.optimization_history[-100:]
    
    def schedule_learning_task(self, task_type: str, task_data: Dict, urgency: float = 0.5):
        """í•™ìŠµ ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
        task_data["created_time"] = time.time()
        self.task_scheduler.add_task(task_type, task_data, urgency)
    
    def get_next_scheduled_task(self) -> Optional[Tuple[str, Dict]]:
        """ë‹¤ìŒ ì˜ˆì •ëœ ì‘ì—… ê°€ì ¸ì˜¤ê¸°"""
        return self.task_scheduler.get_next_task()
    
    def report_task_completion(self, task_type: str, performance_score: float, execution_time: float):
        """ì‘ì—… ì™„ë£Œ ë³´ê³ """
        self.task_scheduler.update_task_performance(task_type, performance_score, execution_time)
    
    def get_optimization_status(self) -> Dict:
        """ìµœì í™” ìƒíƒœ ë³´ê³ """
        return {
            "global_performance_score": self.global_performance_score,
            "optimization_enabled": self.optimization_enabled,
            "auto_tune_mode": self.auto_tune_mode,
            "learning_rate_stats": self.lr_scheduler.get_lr_stats(),
            "batch_size_stats": self.batch_sizer.get_batch_stats(),
            "scheduler_stats": self.task_scheduler.get_scheduler_stats(),
            "recent_optimizations": self.optimization_history[-5:] if self.optimization_history else []
        }
    
    def save_optimization_config(self, config_path: str = "optimization_config.json"):
        """ìµœì í™” ì„¤ì • ì €ì¥"""
        config = {
            "learning_rate": {
                "current": self.lr_scheduler.get_current_lr(),
                "initial": self.lr_scheduler.initial_lr,
                "min": self.lr_scheduler.min_lr,
                "max": self.lr_scheduler.max_lr,
                "strategy": self.lr_scheduler.lr_strategy
            },
            "batch_size": {
                "current": self.batch_sizer.get_current_batch_size(),
                "initial": self.batch_sizer.initial_batch_size,
                "min": self.batch_sizer.min_batch_size,
                "max": self.batch_sizer.max_batch_size
            },
            "global_performance": self.global_performance_score,
            "optimization_enabled": self.optimization_enabled
        }
        
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info(f"ğŸ’¾ ìµœì í™” ì„¤ì • ì €ì¥: {config_path}")

def test_learning_optimizer():
    """í•™ìŠµ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ë° ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™”
    optimizer_manager = LearningOptimizerManager()
    
    # í…ŒìŠ¤íŠ¸ í•™ìŠµ ë°ì´í„°
    test_learning_data = [
        {"input": "Unity GameObject ìƒì„±", "quality": 0.8},
        {"input": "C# async/await ì‚¬ìš©ë²•", "quality": 0.9},
        {"input": "ì½”ë£¨í‹´ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜", "quality": 0.7},
        {"input": "Physics2D ì¶©ëŒ ê°ì§€", "quality": 0.6}
    ] * 10  # 40ê°œ ë°ì´í„°
    
    print(f"ğŸ“š {len(test_learning_data)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì í™” í…ŒìŠ¤íŠ¸")
    
    # ì—¬ëŸ¬ ë²ˆì˜ ìµœì í™” ì„¸ì…˜ ì‹¤í–‰
    for session in range(5):
        print(f"\nğŸ”„ ìµœì í™” ì„¸ì…˜ {session + 1}/5")
        
        # í•™ìŠµ ì‘ì—… ìŠ¤ì¼€ì¤„ë§
        optimizer_manager.schedule_learning_task(
            task_type="general_learning",
            task_data={"data": test_learning_data, "session": session},
            urgency=0.5 + session * 0.1
        )
        
        # ì˜ˆì •ëœ ì‘ì—… ì‹¤í–‰
        task_info = optimizer_manager.get_next_scheduled_task()
        if task_info:
            task_type, task_data = task_info
            
            # ìµœì í™”ëœ í•™ìŠµ ì‹¤í–‰
            start_time = time.time()
            results = optimizer_manager.optimize_learning_session(test_learning_data)
            execution_time = time.time() - start_time
            
            # ì‘ì—… ì™„ë£Œ ë³´ê³ 
            optimizer_manager.report_task_completion(
                task_type, 
                results["performance_score"], 
                execution_time
            )
            
            print(f"âœ… ì„¸ì…˜ ì™„ë£Œ: LR={results['learning_rate']:.6f}, "
                  f"ë°°ì¹˜={results['batch_size']}, ì„±ëŠ¥={results['performance_score']:.3f}")
        
        time.sleep(1)  # ì‹œë®¬ë ˆì´ì…˜ ëŒ€ê¸°
    
    # ìµœì¢… ìƒíƒœ ë³´ê³ 
    print(f"\nğŸ“Š ìµœì í™” ì‹œìŠ¤í…œ ìµœì¢… ìƒíƒœ:")
    status = optimizer_manager.get_optimization_status()
    
    print(f"ì „ì—­ ì„±ëŠ¥ ì ìˆ˜: {status['global_performance_score']:.3f}")
    print(f"í˜„ì¬ í•™ìŠµìœ¨: {status['learning_rate_stats']['current_lr']:.6f}")
    print(f"í˜„ì¬ ë°°ì¹˜ í¬ê¸°: {status['batch_size_stats']['current_batch_size']}")
    print(f"ì„±ëŠ¥ íŠ¸ë Œë“œ: {status['learning_rate_stats']['performance_trend']}")
    
    # ì„¤ì • ì €ì¥
    optimizer_manager.save_optimization_config()
    
    print("\nğŸ‰ í•™ìŠµ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_learning_optimizer()